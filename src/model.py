# %% 0.IMPORTS
import torch
import torch.nn as nn
import torch.nn.functional as F
import math

# %% 1. POSITIONAL ENCODING
# V√¨ Transformer kh√¥ng c√≥ RNN n√™n c·∫ßn c·ªông vector n√†y ƒë·ªÉ bi·∫øt th·ª© t·ª± t·ª´.
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super(PositionalEncoding, self).__init__()
        # T·∫°o ma tr·∫≠n [max_len, d_model] ch·ª©a to√†n s·ªë 0
        pe = torch.zeros(max_len, d_model)
        
        # T·∫°o vector v·ªã tr√≠ [0, 1, 2, ..., max_len-1]
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        
        # T√≠nh m·∫´u s·ªë (div_term) cho h√†m sin/cos
        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))
        
        # √Åp d·ª•ng c√¥ng th·ª©c: ch·∫µn d√πng Sin, l·∫ª d√πng Cos
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        
        # Th√™m chi·ªÅu batch: [1, max_len, d_model]
        pe = pe.unsqueeze(0)
        
        # L∆∞u v√†o buffer (kh√¥ng train, nh∆∞ng v·∫´n ƒë∆∞·ª£c l∆∞u c√πng model)
        self.register_buffer('pe', pe)

    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        # C·ªông PE v√†o x (c·∫Øt ƒë√∫ng ƒë·ªô d√†i c√¢u hi·ªán t·∫°i)
        return x + self.pe[:, :x.size(1), :]

# %% 2. MULTI-HEAD ATTENTION
# C∆° ch·∫ø gi√∫p m√¥ h√¨nh t·∫≠p trung v√†o c√°c ph·∫ßn kh√°c nhau c·ªßa c√¢u c√πng l√∫c.
class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, n_heads, dropout=0.1):
        super(MultiHeadAttention, self).__init__()
        assert d_model % n_heads == 0, "d_model ph·∫£i chia h·∫øt cho n_heads"
        self.d_model = d_model
        self.d_k = d_model // n_heads  # K√≠ch th∆∞·ªõc m·ªói head
        self.n_heads = n_heads
        
        # C√°c l·ªõp Linear ƒë·ªÉ chi·∫øu Q, K, V
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
        # L·ªõp Linear cu·ªëi c√πng sau khi g·ªôp c√°c heads
        self.out = nn.Linear(d_model, d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, q, k, v, mask=None):
        bs = q.size(0) # Batch size
        
        # 1. Chi·∫øu Linear v√† t√°ch th√†nh n_heads
        # Shape: [batch_size, seq_len, n_heads, d_k] -> transpose -> [batch_size, n_heads, seq_len, d_k]
        k = self.k_linear(k).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)
        q = self.q_linear(q).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)
        v = self.v_linear(v).view(bs, -1, self.n_heads, self.d_k).transpose(1, 2)

        # 2. T√≠nh Scaled Dot-Product Attention
        # scores = (Q * K^T) / sqrt(d_k)
        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)
        
        # 3. √Åp d·ª•ng Mask (n·∫øu c√≥) - Che ƒëi c√°c v·ªã tr√≠ padding ho·∫∑c t∆∞∆°ng lai
        if mask is not None:
            # mask == 0 nghƒ©a l√† v·ªã tr√≠ ƒë√≥ c·∫ßn che, g√°n gi√° tr·ªã r·∫•t nh·ªè (-1 t·ª∑)
            scores = scores.masked_fill(mask == 0, -1e9)
        
        # 4. Softmax ƒë·ªÉ l·∫•y tr·ªçng s·ªë attention
        attn_weights = F.softmax(scores, dim=-1)
        attn_weights = self.dropout(attn_weights)
        
        # 5. Nh√¢n v·ªõi V
        output = torch.matmul(attn_weights, v)
        
        # 6. G·ªôp (Concatenate) c√°c heads l·∫°i
        # Shape: [batch_size, seq_len, n_heads * d_k] = [batch_size, seq_len, d_model]
        output = output.transpose(1, 2).contiguous().view(bs, -1, self.d_model)
        
        # 7. ƒêi qua l·ªõp Linear cu·ªëi
        return self.out(output)

# %% 3. FEED FORWARD NETWORK
# M·∫°ng n∆°-ron ƒë∆°n gi·∫£n x·ª≠ l√Ω t·ª´ng v·ªã tr√≠ ri√™ng bi·ªát
class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff=2048, dropout=0.1):
        super(FeedForward, self).__init__()
        self.linear_1 = nn.Linear(d_model, d_ff)
        self.dropout = nn.Dropout(dropout)
        self.linear_2 = nn.Linear(d_ff, d_model)

    def forward(self, x):
        # Linear -> ReLU -> Dropout -> Linear
        return self.linear_2(self.dropout(F.relu(self.linear_1(x))))
    
# %% 4. ENCODER LAYER
class EncoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        self.ffn = FeedForward(d_model, d_ff, dropout)
        
        # Layer Norm gi√∫p ·ªïn ƒë·ªãnh qu√° tr√¨nh hu·∫•n luy·ªán
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, mask):
        # --- Kh·ªëi 1: Self-Attention ---
        # Input x ƒëi v√†o Attention
        # 3 tham s·ªë ƒë·∫ßu v√†o ƒë·ªÅu l√† x v√¨ ƒë√¢y l√† Self-Attention (t·ª± nh√¨n ch√≠nh m√¨nh)
        attn_output = self.self_attn(x, x, x, mask)
        
        # Residual Connection (C·ªông x c≈©) + Layer Norm
        # C√¥ng th·ª©c: Norm(x + Dropout(Sublayer(x)))
        x = self.norm1(x + self.dropout(attn_output))
        
        # --- Kh·ªëi 2: Feed Forward ---
        # Output tr√™n ƒëi v√†o FFN
        ffn_output = self.ffn(x)
        
        # Residual Connection + Layer Norm
        x = self.norm2(x + self.dropout(ffn_output))
        
        return x

# %% 5. ENCODER (CONTAINER)
class Encoder(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len):
        """
        Args:
            vocab_size: K√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn ngu·ªìn (v√≠ d·ª• 8000)
            d_model: K√≠ch th∆∞·ªõc vector (v√≠ d·ª• 512)
            n_layers: S·ªë l∆∞·ª£ng l·ªõp Encoder ch·ªìng l√™n nhau (v√≠ d·ª• 6)
        """
        super(Encoder, self).__init__()
        self.d_model = d_model
        
        # 1. Embedding: Chuy·ªÉn ID t·ª´ th√†nh Vector
        self.embed = nn.Embedding(vocab_size, d_model)
        
        # 2. Positional Encoding: Th√™m th√¥ng tin v·ªã tr√≠
        self.pe = PositionalEncoding(d_model, max_len)
        
        # 3. Stack N l·ªõp EncoderLayer b·∫±ng ModuleList
        self.layers = nn.ModuleList([
            EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)
        ])
        
        # 4. Norm cu·ªëi c√πng tr∆∞·ªõc khi xu·∫•t sang Decoder
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, src, src_mask):
        # src shape: [batch_size, seq_len]
        
        # B∆∞·ªõc 1: Embedding
        x = self.embed(src)
        
        # B∆∞·ªõc 2: Scaling (M·∫πo quan tr·ªçng trong paper Transformer)
        # Nh√¢n vector v·ªõi cƒÉn b·∫≠c 2 c·ªßa d_model ƒë·ªÉ gi√° tr·ªã kh√¥ng b·ªã qu√° nh·ªè so v·ªõi PE
        x = x * math.sqrt(self.d_model)
        
        # B∆∞·ªõc 3: C·ªông PE
        x = self.pe(x)
        x = self.dropout(x)
        
        # B∆∞·ªõc 4: Ch·∫°y qua t·ª´ng l·ªõp Encoder
        for layer in self.layers:
            x = layer(x, src_mask)
            
        # Tr·∫£ v·ªÅ output ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
        return self.norm(x)
    
# %% 6. DECODER LAYER
class DecoderLayer(nn.Module):
    def __init__(self, d_model, n_heads, d_ff, dropout):
        super(DecoderLayer, self).__init__()
        
        # 1. Masked Self-Attention (Cho ch√≠nh c√¢u ƒë√≠ch)
        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)
        
        # 2. Cross-Attention (Quan tr·ªçng: Nh√¨n sang Encoder Output)
        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)
        
        # 3. Feed Forward
        self.ffn = FeedForward(d_model, d_ff, dropout)
        
        # 3 l·ªõp Norm cho 3 kh·ªëi tr√™n
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, enc_output, src_mask, tgt_mask):
        # x: Input c·ªßa Decoder (ho·∫∑c output c·ªßa layer tr∆∞·ªõc)
        # enc_output: Output t·ª´ Encoder (d√πng cho Cross-Attention)
        # src_mask: Che padding c·ªßa Encoder
        # tgt_mask: Che t∆∞∆°ng lai c·ªßa Decoder (Look-ahead mask)
        
        # --- Kh·ªëi 1: Masked Self-Attention ---
        # tgt_mask ·ªü ƒë√¢y r·∫•t quan tr·ªçng (d·∫°ng tam gi√°c) ƒë·ªÉ che c√°c t·ª´ t∆∞∆°ng lai
        attn_output = self.self_attn(x, x, x, tgt_mask)
        x = self.norm1(x + self.dropout(attn_output))
        
        # --- Kh·ªëi 2: Cross-Attention ---
        # Query l√† x (t·ª´ decoder)
        # Key/Value l√† enc_output (t·ª´ encoder) -> ƒê√¢y l√† ch·ªó Decoder "ƒë·ªçc hi·ªÉu" c√¢u ngu·ªìn
        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)
        x = self.norm2(x + self.dropout(attn_output))
        
        # --- Kh·ªëi 3: Feed Forward ---
        ffn_output = self.ffn(x)
        x = self.norm3(x + self.dropout(ffn_output))
        
        return x

# %% 7. DECODER (CONTAINER)
class Decoder(nn.Module):
    def __init__(self, vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len):
        super(Decoder, self).__init__()
        self.d_model = d_model
        
        # 1. Embedding & PE
        self.embed = nn.Embedding(vocab_size, d_model)
        self.pe = PositionalEncoding(d_model, max_len)
        
        # 2. Stack N l·ªõp DecoderLayer
        self.layers = nn.ModuleList([
            DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)
        ])
        
        # 3. Norm cu·ªëi c√πng
        self.norm = nn.LayerNorm(d_model)
        self.dropout = nn.Dropout(dropout)

    def forward(self, tgt, enc_output, src_mask, tgt_mask):
        # tgt: [batch_size, seq_len]
        
        # Embedding + Scaling + PE
        x = self.embed(tgt) * math.sqrt(self.d_model)
        x = self.pe(x)
        x = self.dropout(x)
        
        # Qua t·ª´ng l·ªõp Decoder
        for layer in self.layers:
            # Truy·ªÅn enc_output v√†o t·ª´ng l·ªõp ƒë·ªÉ l√†m Cross-Attention
            x = layer(x, enc_output, src_mask, tgt_mask)
            
        return self.norm(x)


# %% 8. TRANSFORMER (FULL MODEL)
class Transformer(nn.Module):
    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len=5000):
        super(Transformer, self).__init__()
        
        # Kh·ªüi t·∫°o Encoder v√† Decoder
        self.encoder = Encoder(src_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)
        self.decoder = Decoder(tgt_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)
        
        # L·ªõp chi·∫øu cu·ªëi c√πng: Chuy·ªÉn ƒë·ªïi t·ª´ d_model sang k√≠ch th∆∞·ªõc t·ª´ ƒëi·ªÉn ƒë√≠ch
        # V√≠ d·ª•: 512 -> 8000 t·ª´
        self.projection = nn.Linear(d_model, tgt_vocab_size)
        
        # Kh·ªüi t·∫°o tr·ªçng s·ªë (Xavier Initialization) gi√∫p model h·ªôi t·ª• nhanh h∆°n
        self._init_parameters()

    def forward(self, src, tgt, src_mask, tgt_mask):
        # 1. Qua Encoder
        # enc_output shape: [batch_size, src_len, d_model]
        enc_output = self.encoder(src, src_mask)
        
        # 2. Qua Decoder
        # dec_output shape: [batch_size, tgt_len, d_model]
        dec_output = self.decoder(tgt, enc_output, src_mask, tgt_mask)
        
        # 3. Qua l·ªõp chi·∫øu cu·ªëi c√πng
        # output shape: [batch_size, tgt_len, tgt_vocab_size]
        output = self.projection(dec_output)
        
        return output

    def encode(self, src, src_mask):
        # H√†m ph·ª• d√πng khi Inference (d·ªãch th·ª≠)
        return self.encoder(src, src_mask)

    def decode(self, tgt, enc_output, src_mask, tgt_mask):
        # H√†m ph·ª• d√πng khi Inference
        return self.decoder(tgt, enc_output, src_mask, tgt_mask)

    def _init_parameters(self):
        # Kh·ªüi t·∫°o tr·ªçng s·ªë Xavier Uniform cho c√°c tham s·ªë
        for p in self.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)


# %% 9. TEST BLOCK (Ch·∫°y th·ª≠ ƒë·ªÉ ki·ªÉm tra)
if __name__ == "__main__":
    print("üöÄ ƒêang ki·ªÉm tra to√†n b·ªô ki·∫øn tr√∫c Transformer...")
    
    # 1. Gi·∫£ l·∫≠p tham s·ªë
    src_vocab_size = 100
    tgt_vocab_size = 100
    d_model = 512
    n_layers = 2 # Test √≠t l·ªõp cho nhanh
    n_heads = 8
    d_ff = 2048
    dropout = 0.1
    max_len = 50
    
    # 2. Kh·ªüi t·∫°o Model
    model = Transformer(src_vocab_size, tgt_vocab_size, d_model, n_layers, n_heads, d_ff, dropout, max_len)
    print("‚úÖ Kh·ªüi t·∫°o Model th√†nh c√¥ng!")

    # 3. T·∫°o d·ªØ li·ªáu gi·∫£ (Batch size = 2, Seq len = 10)
    src = torch.randint(0, src_vocab_size, (2, 10))
    tgt = torch.randint(0, tgt_vocab_size, (2, 10))
    
    # T·∫°o Mask gi·∫£ (Test k·ªπ thu·∫≠t Masking sau, gi·ªù test k√≠ch th∆∞·ªõc tr∆∞·ªõc)
    src_mask = torch.ones(2, 1, 1, 10) # Che padding
    tgt_mask = torch.ones(2, 1, 10, 10) # Che t∆∞∆°ng lai (Look-ahead)

    # 4. Forward Pass
    try:
        out = model(src, tgt, src_mask, tgt_mask)
        print(f"‚úÖ Forward pass th√†nh c√¥ng! Output shape: {out.shape}")
        
        # Ki·ªÉm tra k√≠ch th∆∞·ªõc cu·ªëi c√πng
        expected_shape = (2, 10, tgt_vocab_size)
        if out.shape == expected_shape:
            print("üéâ CH√öC M·ª™NG! Ki·∫øn tr√∫c Transformer From Scratch ƒë√£ ho√†n thi·ªán chu·∫©n x√°c.")
        else:
            print(f"‚ùå Sai k√≠ch th∆∞·ªõc. Nh·∫≠n ƒë∆∞·ª£c {out.shape}, k·ª≥ v·ªçng {expected_shape}")
            
    except Exception as e:
        print(f"‚ùå L·ªói khi ch·∫°y Forward: {e}")
        import traceback
        traceback.print_exc()
