{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14156402,"sourceType":"datasetVersion","datasetId":9022972},{"sourceId":14172059,"sourceType":"datasetVersion","datasetId":9033493}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nos.environ[\"HF_HOME\"] = \"/kaggle/working/hf\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n\n# Fix protobuf conflict first\n!pip -q uninstall -y protobuf\n!pip -q install -q \"protobuf==4.25.3\"\n!python -c \"import google.protobuf as pb; print('protobuf =', pb.__version__)\"\n\n# Install pinned libs (with deps)\n!pip -q install -U \"transformers==4.44.2\" \"tokenizers==0.19.1\" \"peft==0.11.1\" \\\n                 \"accelerate==0.33.0\" \"datasets==2.21.0\" \"sacrebleu==2.4.2\"\n\nimport torch, transformers, peft, datasets\nprint(\"torch\", torch.__version__)\nprint(\"transformers\", transformers.__version__, \"| peft\", peft.__version__)\nprint(\"datasets\", datasets.__version__)\nprint(\"GPU count:\", torch.cuda.device_count())","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:20:36.704939Z","iopub.execute_input":"2025-12-16T06:20:36.705544Z","iopub.status.idle":"2025-12-16T06:22:20.164516Z","shell.execute_reply.started":"2025-12-16T06:20:36.705511Z","shell.execute_reply":"2025-12-16T06:22:20.163837Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\nopentelemetry-proto 1.37.0 requires protobuf<7.0,>=5.0, but you have protobuf 4.25.3 which is incompatible.\na2a-sdk 0.3.10 requires protobuf>=5.29.5, but you have protobuf 4.25.3 which is incompatible.\nray 2.51.1 requires click!=8.3.0,>=7.0, but you have click 8.3.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\npydrive2 1.21.3 requires cryptography<44, but you have cryptography 46.0.3 which is incompatible.\npydrive2 1.21.3 requires pyOpenSSL<=24.2.1,>=19.1.0, but you have pyopenssl 25.3.0 which is incompatible.\nydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\ngrpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2025.10.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mprotobuf = 4.25.3\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.0/58.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m86.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.1/315.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m527.3/527.3 kB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.7/106.7 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.6/177.6 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ns3fs 2025.3.0 requires fsspec==2025.3.0.*, but you have fsspec 2024.6.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ngradio 5.38.1 requires pydantic<2.12,>=2.0, but you have pydantic 2.12.4 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\ngcsfs 2025.3.0 requires fsspec==2025.3.0, but you have fsspec 2024.6.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mtorch 2.6.0+cu124\ntransformers 4.44.2 | peft 0.11.1\ndatasets 2.21.0\nGPU count: 2\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from pathlib import Path\nfrom datasets import Dataset, DatasetDict\n\nPROC_DIR = Path(\"/kaggle/input/data-vlsp/processed\")\nassert PROC_DIR.exists()\n\nTRAIN_EN, TRAIN_VI = PROC_DIR/\"train.en\", PROC_DIR/\"train.vi\"\nVALID_EN, VALID_VI = PROC_DIR/\"valid.en\", PROC_DIR/\"valid.vi\"\nTEST_EN,  TEST_VI  = PROC_DIR/\"test.en\",  PROC_DIR/\"test.vi\"\n\nOUT_ROOT = Path(\"/kaggle/working/vlsp_en2vi_run\")\nOUT_ROOT.mkdir(parents=True, exist_ok=True)\nDS_DIR = OUT_ROOT/\"dataset_en2vi_raw\"\n\ndef build_split(src_path: Path, tgt_path: Path):\n    def gen():\n        with src_path.open(\"r\", encoding=\"utf-8\") as fs, tgt_path.open(\"r\", encoding=\"utf-8\") as ft:\n            for s, t in zip(fs, ft):\n                s = s.strip()\n                t = t.strip()\n                if s and t:\n                    yield {\"src\": s, \"tgt\": t}\n    return Dataset.from_generator(gen)\n\nraw = DatasetDict({\n    \"train\": build_split(TRAIN_EN, TRAIN_VI),\n    \"valid\": build_split(VALID_EN, VALID_VI),\n    \"test\":  build_split(TEST_EN,  TEST_VI),\n})\nraw.save_to_disk(str(DS_DIR))\nprint(\"Saved dataset to:\", DS_DIR)\nprint(raw)\nprint(\"Sample:\", raw[\"train\"][0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:22:25.960566Z","iopub.execute_input":"2025-12-16T06:22:25.961040Z","iopub.status.idle":"2025-12-16T06:22:30.415910Z","shell.execute_reply.started":"2025-12-16T06:22:25.961012Z","shell.execute_reply":"2025-12-16T06:22:30.415354Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32c0223d973f4cebbcdd5a6c50df88ee"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abbcd6229ca6469d97f4503ee15fbb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4aa1f60f231b4722ae45f97e249e2e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/490000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3315898ce0243ebb188a43ac3cb2a8d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/10000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"522c4443db1a45998503564fb1fe7851"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/3000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f51a8b5c6bd140f8ada5b9c192f13b52"}},"metadata":{}},{"name":"stdout","text":"Saved dataset to: /kaggle/working/vlsp_en2vi_run/dataset_en2vi_raw\nDatasetDict({\n    train: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 490000\n    })\n    valid: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 10000\n    })\n    test: Dataset({\n        features: ['src', 'tgt'],\n        num_rows: 3000\n    })\n})\nSample: {'src': 'Characteristics of patients studied 60 patients with 60 soft-tisue defects in the weight-bearing area of the foot, including 46 male and 14 female.', 'tgt': 'Đặc điểm của nhóm BN nghiên cứu Tổng cộng có 60 BN với 60 KHPM, bao gồm 46 nam và 14 nữ.'}\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# from pathlib import Path\n\n# Path(\"/kaggle/working/train_qwen_en2vi_lora.py\").write_text(r\"\"\"\n# import os, argparse, glob\n# os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n# os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n# os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n\n# import torch\n# from datasets import load_from_disk\n# from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n# from peft import LoraConfig, get_peft_model\n\n# def make_prompt_en2vi(src_en: str) -> str:\n#     # Format chuẩn ChatML cho Qwen\n#     return (\n#         f\"<|im_start|>system\\nYou are a professional medical translator.<|im_end|>\\n\"\n#         f\"<|im_start|>user\\nTranslate the following medical text from English to Vietnamese:\\n{src_en}<|im_end|>\\n\"\n#         f\"<|im_start|>assistant\\n\"\n#     )\n\n# def preprocess_builder(tokenizer, max_len: int):\n#     eos_id = tokenizer.eos_token_id\n#     def _pp(ex):\n#         prompt = make_prompt_en2vi(ex[\"src\"])\n#         completion = \" \" + ex[\"tgt\"]\n\n#         prompt_ids = tokenizer(prompt, add_special_tokens=False)[\"input_ids\"]\n#         comp_ids   = tokenizer(completion, add_special_tokens=False)[\"input_ids\"]\n#         if eos_id is not None:\n#             comp_ids = comp_ids + [eos_id]\n\n#         input_ids = (prompt_ids + comp_ids)[:max_len]\n#         labels    = ([-100] * len(prompt_ids) + comp_ids)[:max_len]\n#         attn      = [1] * len(input_ids)\n#         return {\"input_ids\": input_ids, \"attention_mask\": attn, \"labels\": labels}\n#     return _pp\n\n# class CausalCollator:\n#     def __init__(self, pad_id: int):\n#         self.pad_id = pad_id\n#     def __call__(self, feats):\n#         max_len = max(len(f[\"input_ids\"]) for f in feats)\n#         def pad(x, v): return x + [v] * (max_len - len(x))\n#         return {\n#             \"input_ids\": torch.tensor([pad(f[\"input_ids\"], self.pad_id) for f in feats], dtype=torch.long),\n#             \"attention_mask\": torch.tensor([pad(f[\"attention_mask\"], 0) for f in feats], dtype=torch.long),\n#             \"labels\": torch.tensor([pad(f[\"labels\"], -100) for f in feats], dtype=torch.long),\n#         }\n\n# def last_checkpoint(output_dir: str):\n#     ckpts = sorted(glob.glob(os.path.join(output_dir, \"checkpoint-*\")), key=lambda p: int(p.split(\"-\")[-1]))\n#     return ckpts[-1] if ckpts else None\n\n# def main():\n#     ap = argparse.ArgumentParser()\n#     ap.add_argument(\"--model_id\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n#     ap.add_argument(\"--dataset_dir\", type=str, required=True)\n#     ap.add_argument(\"--output_dir\", type=str, required=True)\n\n#     ap.add_argument(\"--max_seq_length\", type=int, default=512)\n#     ap.add_argument(\"--per_device_train_batch_size\", type=int, default=1)\n#     ap.add_argument(\"--per_device_eval_batch_size\", type=int, default=1)\n#     ap.add_argument(\"--gradient_accumulation_steps\", type=int, default=32)\n\n#     ap.add_argument(\"--learning_rate\", type=float, default=2e-4)\n#     ap.add_argument(\"--max_steps\", type=int, default=20000)  # overnight\n#     ap.add_argument(\"--warmup_ratio\", type=float, default=0.03)\n\n#     ap.add_argument(\"--lora_r\", type=int, default=16)\n#     ap.add_argument(\"--lora_alpha\", type=int, default=32)\n#     ap.add_argument(\"--lora_dropout\", type=float, default=0.05)\n\n#     ap.add_argument(\"--eval_steps\", type=int, default=2000)\n#     ap.add_argument(\"--save_steps\", type=int, default=2000)\n#     ap.add_argument(\"--logging_steps\", type=int, default=50)\n#     ap.add_argument(\"--seed\", type=int, default=42)\n#     args = ap.parse_args()\n\n#     torch.manual_seed(args.seed)\n\n#     local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n#     if torch.cuda.is_available():\n#         torch.cuda.set_device(local_rank)\n\n#     dsd = load_from_disk(args.dataset_dir)\n#     train_raw = dsd[\"train\"]\n#     valid_raw = dsd[\"valid\"]\n\n#     tok = AutoTokenizer.from_pretrained(args.model_id, use_fast=True, trust_remote_code=True)\n#     if tok.pad_token is None:\n#         tok.pad_token = tok.eos_token\n#     tok.padding_side = \"right\"\n\n#     pp = preprocess_builder(tok, args.max_seq_length)\n#     train_ds = train_raw.map(pp, remove_columns=train_raw.column_names, num_proc=4)\n#     valid_ds = valid_raw.map(pp, remove_columns=valid_raw.column_names, num_proc=1)\n\n#     model = AutoModelForCausalLM.from_pretrained(\n#         args.model_id,\n#         torch_dtype=torch.float16,\n#         device_map={\"\": local_rank} if torch.cuda.is_available() else None,\n#         low_cpu_mem_usage=True,\n#         trust_remote_code=True,\n#     )\n#     model.config.use_cache = False\n\n#     lora_cfg = LoraConfig(\n#         r=args.lora_r, lora_alpha=args.lora_alpha, lora_dropout=args.lora_dropout,\n#         bias=\"none\", task_type=\"CAUSAL_LM\",\n#         target_modules=[\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n#     )\n#     model = get_peft_model(model, lora_cfg)\n\n#     # IMPORTANT: keep trainable (LoRA) params in fp32 to avoid GradScaler \"unscale FP16 gradients\" error\n#     for n, p in model.named_parameters():\n#         if p.requires_grad:\n#             p.data = p.data.float()\n\n\n#     targs = TrainingArguments(\n#         output_dir=args.output_dir,\n#         seed=args.seed,\n#         fp16=True, bf16=False,\n\n#         per_device_train_batch_size=args.per_device_train_batch_size,\n#         per_device_eval_batch_size=args.per_device_eval_batch_size,\n#         gradient_accumulation_steps=args.gradient_accumulation_steps,\n\n#         learning_rate=args.learning_rate,\n#         warmup_ratio=args.warmup_ratio,\n#         max_steps=args.max_steps,\n\n#         logging_steps=args.logging_steps,\n#         evaluation_strategy=\"steps\",\n#         eval_steps=args.eval_steps,\n\n#         save_strategy=\"steps\",\n#         save_steps=args.save_steps,\n#         save_total_limit=3,\n\n#         load_best_model_at_end=True,\n#         metric_for_best_model=\"eval_loss\",\n#         greater_is_better=False,\n\n#         report_to=\"none\",\n#         ddp_find_unused_parameters=False,\n#         remove_unused_columns=False,\n#         optim=\"adamw_torch\",\n\n#         dataloader_num_workers=4,\n#         dataloader_pin_memory=True,\n#         group_by_length=True,\n#     )\n\n#     trainer = Trainer(\n#         model=model,\n#         args=targs,\n#         train_dataset=train_ds,\n#         eval_dataset=valid_ds,\n#         data_collator=CausalCollator(tok.pad_token_id),\n#     )\n\n#     ckpt = last_checkpoint(args.output_dir)\n#     trainer.train(resume_from_checkpoint=ckpt)\n#     trainer.save_model(args.output_dir)\n#     tok.save_pretrained(args.output_dir)\n\n# if __name__ == \"__main__\":\n#     main()\n# \"\"\", encoding=\"utf-8\")\n\n# print(\"Wrote train script.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:45:12.020379Z","iopub.execute_input":"2025-12-15T19:45:12.020648Z","iopub.status.idle":"2025-12-15T19:45:12.027777Z","shell.execute_reply.started":"2025-12-15T19:45:12.020621Z","shell.execute_reply":"2025-12-15T19:45:12.027078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n# TRAIN_OUT = str(OUT_ROOT/\"lora_en2vi_qwen2.5_1.5b\")\n\n# !TOKENIZERS_PARALLELISM=false OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 /kaggle/working/train_qwen_en2vi_lora.py \\\n#   --model_id \"{MODEL_ID}\" \\\n#   --dataset_dir \"{DS_DIR}\" \\\n#   --output_dir \"{TRAIN_OUT}\" \\\n#   --max_seq_length 320 \\\n#   --per_device_train_batch_size 4 \\\n#   --per_device_eval_batch_size 4 \\\n#   --gradient_accumulation_steps 8 \\\n#   --learning_rate 2e-4 \\\n#   --lora_dropout 0.05 \\\n#   --max_steps 8000 \\\n#   --eval_steps 800 --save_steps 800 --logging_steps 50","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-15T19:45:12.028435Z","iopub.execute_input":"2025-12-15T19:45:12.028596Z","iopub.status.idle":"2025-12-15T19:59:45.570255Z","shell.execute_reply.started":"2025-12-15T19:45:12.028583Z","shell.execute_reply":"2025-12-15T19:59:45.569219Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\n\nPath(\"/kaggle/working/eval_test_bleu_en2vi_ddp.py\").write_text(r\"\"\"\nimport os, argparse\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\nos.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n\nimport torch\nimport torch.distributed as dist\nfrom datasets import load_from_disk\nfrom sacrebleu.metrics import BLEU\nfrom transformers import AutoTokenizer, AutoModelForCausalLM\nfrom peft import PeftModel\n\ndef make_prompt_en2vi(src_en: str) -> str:\n    return (\n        \"You are a professional medical translator.\\n\"\n        \"### Task: Translate English to Vietnamese (medical domain)\\n\"\n        f\"### English: {src_en}\\n\"\n        \"### Vietnamese:\"\n    )\n\ndef ddp_setup():\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        dist.init_process_group(backend=\"nccl\")\n        rank = dist.get_rank()\n        world = dist.get_world_size()\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", \"0\"))\n        torch.cuda.set_device(local_rank)\n        return True, rank, world, local_rank\n    return False, 0, 1, 0\n\n@torch.inference_mode()\ndef main():\n    ap = argparse.ArgumentParser()\n    ap.add_argument(\"--base_model_id\", type=str, default=\"Qwen/Qwen2.5-1.5B-Instruct\")\n    ap.add_argument(\"--adapter_dir\", type=str, required=True)\n    ap.add_argument(\"--dataset_dir\", type=str, required=True)\n\n    ap.add_argument(\"--batch_size\", type=int, default=8)        # per GPU\n    ap.add_argument(\"--max_prompt_len\", type=int, default=512)  # prompt max len\n    ap.add_argument(\"--max_new_tokens\", type=int, default=128)  # reduce to avoid rambling\n    ap.add_argument(\"--num_beams\", type=int, default=4)\n    ap.add_argument(\"--out_hyp\", type=str, required=True)\n    args = ap.parse_args()\n\n    is_ddp, rank, world, local_rank = ddp_setup()\n\n    dsd = load_from_disk(args.dataset_dir)\n    if \"test\" not in dsd:\n        raise ValueError(\"dataset_dir phải có split 'test' (dsd['test']).\")\n    test = dsd[\"test\"]\n    n = len(test)\n\n    tok = AutoTokenizer.from_pretrained(args.base_model_id, use_fast=True, trust_remote_code=True)\n    if tok.pad_token is None:\n        tok.pad_token = tok.eos_token\n    tok.padding_side = \"left\"\n\n    base = AutoModelForCausalLM.from_pretrained(\n        args.base_model_id,\n        torch_dtype=torch.float16,\n        device_map={\"\": local_rank} if torch.cuda.is_available() else None,\n        low_cpu_mem_usage=True,\n        trust_remote_code=True,\n    )\n    model = PeftModel.from_pretrained(base, args.adapter_dir)\n    model.eval()\n    model.config.use_cache = True\n\n    my_idxs = list(range(rank, n, world))\n    results = []  # list[(idx, hyp)]\n\n    for start in range(0, len(my_idxs), args.batch_size):\n        idxs = my_idxs[start:start+args.batch_size]\n        batch_prompts = [make_prompt_en2vi(test[i][\"src\"]) for i in idxs]\n\n        enc = tok(\n            batch_prompts,\n            return_tensors=\"pt\",\n            padding=True,\n            truncation=True,\n            max_length=args.max_prompt_len,\n        ).to(model.device)\n\n        out = model.generate(\n            **enc,\n            do_sample=False,\n            num_beams=args.num_beams,\n            max_new_tokens=args.max_new_tokens,\n            eos_token_id=tok.eos_token_id,\n            pad_token_id=tok.pad_token_id,\n            use_cache=True,\n        )\n\n        # ✅ FIX: cắt phần prompt theo chiều dài input đã PAD trong batch (works for left/right padding)\n        input_len = enc[\"input_ids\"].shape[1]\n\n        for j, idx in enumerate(idxs):\n            gen_ids = out[j, input_len:]\n            hyp = tok.decode(gen_ids, skip_special_tokens=True).strip()\n\n            # (optional) nếu model thỉnh thoảng tự in lại nhãn\n            if \"### Vietnamese:\" in hyp:\n                hyp = hyp.split(\"### Vietnamese:\")[-1].strip()\n\n            results.append((idx, hyp))\n\n    # gather về rank0\n    if is_ddp:\n        gathered = [None for _ in range(world)]\n        dist.all_gather_object(gathered, results)\n        if rank == 0:\n            merged = {}\n            for part in gathered:\n                for idx, hyp in part:\n                    merged[idx] = hyp\n            hyps = [merged[i] for i in range(n)]\n        dist.barrier()\n        dist.destroy_process_group()\n        if rank != 0:\n            return\n    else:\n        merged = {idx: hyp for idx, hyp in results}\n        hyps = [merged[i] for i in range(n)]\n\n    refs = [ex[\"tgt\"] for ex in test]\n\n    with open(args.out_hyp, \"w\", encoding=\"utf-8\") as f:\n        for h in hyps:\n            f.write(h.replace(\"\\n\", \" \") + \"\\n\")\n\n    bleu = BLEU(tokenize=\"13a\")\n    score = bleu.corpus_score(hyps, [refs])\n    print(\"TEST BLEU:\", score.score)\n    print(\"Signature:\", score.format(signature=True))\n\n    # quick debug samples (rank0 only)\n    for i in [0, 1, 2, 3, 4]:\n        print(\"\\\\n--- sample\", i, \"---\")\n        print(\"SRC:\", test[i][\"src\"])\n        print(\"REF:\", refs[i])\n        print(\"HYP:\", hyps[i])\n\nif __name__ == \"__main__\":\n    main()\n\"\"\", encoding=\"utf-8\")\n\nprint(\"Wrote FIXED DDP eval script.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:18:28.371787Z","iopub.execute_input":"2025-12-16T06:18:28.371952Z","iopub.status.idle":"2025-12-16T06:18:28.382416Z","shell.execute_reply.started":"2025-12-16T06:18:28.371936Z","shell.execute_reply":"2025-12-16T06:18:28.381696Z"}},"outputs":[{"name":"stdout","text":"Wrote FIXED DDP eval script.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"MODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\n\n# đọc từ /kaggle/input (read-only)\nDS_DIR    = \"/kaggle/input/qwenoutput/vlsp_en2vi_run/dataset_en2vi_raw\"\nTRAIN_OUT = \"/kaggle/input/qwenoutput/vlsp_en2vi_run/lora_en2vi_qwen2.5_1.5b\"\n\n# ghi output BLEU/hyp ra /kaggle/working (writeable)\nHYP_PATH = \"/kaggle/working/test_hyp_en2vi_fixed.txt\"\n\n!TOKENIZERS_PARALLELISM=false OMP_NUM_THREADS=1 torchrun --nproc_per_node=2 /kaggle/working/eval_test_bleu_en2vi_ddp.py \\\n  --base_model_id \"{MODEL_ID}\" \\\n  --adapter_dir \"{TRAIN_OUT}\" \\\n  --dataset_dir \"{DS_DIR}\" \\\n  --batch_size 8 \\\n  --max_new_tokens 128 \\\n  --out_hyp \"{HYP_PATH}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-16T06:22:37.072174Z","iopub.execute_input":"2025-12-16T06:22:37.072511Z","iopub.status.idle":"2025-12-16T06:42:56.375196Z","shell.execute_reply.started":"2025-12-16T06:22:37.072489Z","shell.execute_reply":"2025-12-16T06:42:56.374265Z"}},"outputs":[{"name":"stdout","text":"tokenizer_config.json: 7.30kB [00:00, 29.0MB/s]\nvocab.json: 2.78MB [00:00, 66.8MB/s]\nmerges.txt: 1.67MB [00:00, 107MB/s]\ntokenizer.json: 7.03MB [00:00, 185MB/s]\nconfig.json: 100%|█████████████████████████████| 660/660 [00:00<00:00, 4.42MB/s]\nmodel.safetensors: 100%|████████████████████| 3.09G/3.09G [00:10<00:00, 306MB/s]\ngeneration_config.json: 100%|██████████████████| 242/242 [00:00<00:00, 1.74MB/s]\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:567: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:572: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:589: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n  warnings.warn(\nTEST BLEU: 48.71697059103983\nSignature: BLEU|True = 48.72 75.4/57.4/44.6/35.4 (BP = 0.953 ratio = 0.954 hyp_len = 96193 ref_len = 100870)\n\\n--- sample 0 ---\nSRC: Knowledge, practices in public health service utilization among health insurance card’s holders and influencing factors in Vientiane, Lao\nREF: Thực trạng kiến thức và thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố ảnh hưởng tại tỉnh Viêng Chăn, CHDCND Lào, năm 2017\nHYP: Kiến thức, thực hành sử dụng dịch vụ y tế công cộng của người sử dụng thẻ bảo hiểm y tế và các yếu tố liên quan tại thành phố Vientiane, Lào\n\\n--- sample 1 ---\nSRC: Describe knowledge, practices in public health service utilization among health insurance card's holders and influencing factors in Vientiane, Lao PDR, 2017.\nREF: Mô tả thực trạng kiến thức, thực hành của người có thẻ bảo hiểm y tế trong sử dụng dịch vụ khám chữa bệnh ở các cơ sở y tế công và một số yếu tố liên quan tại tỉnh Viêng Chăn, Cộng hoà Dân chủ Nhân dân Lào năm 2017.\nHYP: Mô tả kiến thức, thực hành sử dụng dịch vụ y tế công và các yếu tố liên quan của người sử dụng thẻ bảo hiểm y tế tại thành phố Vientiane, Lào, năm 2017.\n\\n--- sample 2 ---\nSRC: Methodology: A cross sectional study was used among 928 adult health insurance card's holders in Phone Hong and Keo Oudom districts, Vientiane province.\nREF: Phương pháp: Thiết kế nghiên mô tả cắt ngang được thực hiện trên 928 người trưởng thành có thẻ bảo hiểm y tế tại 2 huyện Phone Hong và Keo Oudom, tỉnh Viêng Chăn.\nHYP: Đối tượng và phương pháp nghiên cứu: Nghiên cứu mô tả cắt ngang được thực hiện trên 928 người có thẻ BHYT ở 2 huyện Điện Biên Phủ.\n\\n--- sample 3 ---\nSRC: Results: Percentage of card's holders who knew the finance-free utilization of the first registered public health services was 44.5% and being provided health insurance information was 34.8%.\nREF: Kết quả: Tỷ lệ người biết được khám chữa bệnh (KCB) miễn phí tại nơi đăng ký ban đầu chiếm 44,5%, được cung cấp thông tin về bảo hiểm y tế (BHYT) chiếm 34,8%.\nHYP: Kết quả: Tỷ lệ người dân biết sử dụng dịch vụ y tế công miễn phí lần đầu là 44,5% và được cung cấp thông tin BHYT là 34,8%.\n\\n--- sample 4 ---\nSRC: Percentage of card's holders who went to the first registered public health services was 61.8%.\nREF: Tỷ lệ người có thẻ BHYT thực hành khám chữa bệnh đúng nơi đăng ký KCB ban đầu chiếm 61,8%.\nHYP: Tỷ lệ người dân đến khám sức khoẻ ban đầu là 61,8%.\n","output_type":"stream"}],"execution_count":6}]}