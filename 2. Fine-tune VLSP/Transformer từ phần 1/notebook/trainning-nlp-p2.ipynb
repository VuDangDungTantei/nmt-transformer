{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3918886",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:04.691821Z",
     "iopub.status.busy": "2025-12-16T11:36:04.691066Z",
     "iopub.status.idle": "2025-12-16T11:36:04.714922Z",
     "shell.execute_reply": "2025-12-16T11:36:04.714039Z"
    },
    "papermill": {
     "duration": 0.028892,
     "end_time": "2025-12-16T11:36:04.716120",
     "exception": false,
     "start_time": "2025-12-16T11:36:04.687228",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_ROOT: /kaggle/input/envi-nmt-code/src\n",
      "PROCESSED_DIR: /kaggle/input/envi-nmt-data-p2/data_p2/processed\n",
      "SPM_PATH: /kaggle/input/envi-nmt-data-p2/data_p2/spm/spm.model exists: True\n",
      "CKPT_PATH: /kaggle/input/envi-nmt-data-p2/data_p2/best_transformer_v3.pt exists: True\n",
      "train.en exists: True\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "CODE_ROOT = Path(\"/kaggle/input/envi-nmt-code/src\")\n",
    "DATA_ROOT = Path(\"/kaggle/input/envi-nmt-data-p2/data_p2\")\n",
    "\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"     # train.en, train.vi, dev.*, test.*\n",
    "SPM_PATH      = DATA_ROOT / \"spm\" / \"spm.model\"\n",
    "CKPT_PATH     = Path(\"/kaggle/input/envi-nmt-data-p2/data_p2/best_transformer_v3.pt\")\n",
    "\n",
    "sys.path.insert(0, str(CODE_ROOT))\n",
    "\n",
    "print(\"CODE_ROOT:\", CODE_ROOT)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"SPM_PATH:\", SPM_PATH, \"exists:\", SPM_PATH.exists())\n",
    "print(\"CKPT_PATH:\", CKPT_PATH, \"exists:\", CKPT_PATH.exists())\n",
    "print(\"train.en exists:\", (PROCESSED_DIR/\"train.en\").exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c05d2ca7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:04.721528Z",
     "iopub.status.busy": "2025-12-16T11:36:04.721013Z",
     "iopub.status.idle": "2025-12-16T11:36:09.312906Z",
     "shell.execute_reply": "2025-12-16T11:36:09.312209Z"
    },
    "papermill": {
     "duration": 4.595945,
     "end_time": "2025-12-16T11:36:09.314307",
     "exception": false,
     "start_time": "2025-12-16T11:36:04.718362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip -q install sacrebleu sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2610bfd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:09.320082Z",
     "iopub.status.busy": "2025-12-16T11:36:09.319801Z",
     "iopub.status.idle": "2025-12-16T11:36:16.032377Z",
     "shell.execute_reply": "2025-12-16T11:36:16.031183Z"
    },
    "papermill": {
     "duration": 6.717523,
     "end_time": "2025-12-16T11:36:16.034119",
     "exception": false,
     "start_time": "2025-12-16T11:36:09.316596",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'src_ids': torch.Size([64, 80]), 'tgt_in_ids': torch.Size([64, 80]), 'tgt_out_ids': torch.Size([64, 80]), 'src_padding_mask': torch.Size([64, 80]), 'tgt_padding_mask': torch.Size([64, 80])}\n",
      "BOS check: True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tokenizer import SubwordTokenizer\n",
    "from dataset import NMTDataset, collate_fn\n",
    "\n",
    "tok = SubwordTokenizer(str(SPM_PATH))\n",
    "pad_id = tok.pad_id\n",
    "\n",
    "train_ds = NMTDataset(str(PROCESSED_DIR), split=\"train\", tokenizer=tok, max_src_len=80, max_tgt_len=80)\n",
    "dev_ds   = NMTDataset(str(PROCESSED_DIR), split=\"dev\",   tokenizer=tok, max_src_len=80, max_tgt_len=80)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True,  num_workers=2,\n",
    "                          collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "dev_loader   = DataLoader(dev_ds,   batch_size=64, shuffle=False, num_workers=2,\n",
    "                          collate_fn=lambda b: collate_fn(b, pad_id=pad_id))\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print({k: v.shape for k, v in batch.items()})\n",
    "print(\"BOS check:\", batch[\"tgt_in_ids\"][0,0].item() == tok.bos_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76975aa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:16.042142Z",
     "iopub.status.busy": "2025-12-16T11:36:16.041268Z",
     "iopub.status.idle": "2025-12-16T11:36:16.050121Z",
     "shell.execute_reply": "2025-12-16T11:36:16.049102Z"
    },
    "papermill": {
     "duration": 0.014896,
     "end_time": "2025-12-16T11:36:16.052184",
     "exception": false,
     "start_time": "2025-12-16T11:36:16.037288",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def make_src_mask(src_padding_mask: torch.Tensor):\n",
    "    \"\"\"\n",
    "    src_padding_mask: (B,S) bool, True tại PAD\n",
    "    return: (B,1,1,S) bool, True = allowed (không bị mask)\n",
    "    \"\"\"\n",
    "    return (~src_padding_mask).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "def build_causal_cache(max_tgt_len: int, device: torch.device):\n",
    "    \"\"\"\n",
    "    Tạo causal mask 1 lần:\n",
    "    shape (max_tgt_len, max_tgt_len), True ở (i>=j)\n",
    "    \"\"\"\n",
    "    return torch.tril(\n",
    "        torch.ones((max_tgt_len, max_tgt_len), dtype=torch.bool, device=device)\n",
    "    )\n",
    "\n",
    "def make_tgt_mask(tgt_padding_mask: torch.Tensor, causal_cache: torch.Tensor):\n",
    "    \"\"\"\n",
    "    tgt_padding_mask: (B,T) bool, True tại PAD\n",
    "    causal_cache: (maxT,maxT) bool (đã build 1 lần)\n",
    "    return: (B,1,T,T) bool, True = allowed (causal + pad theo KEY)\n",
    "    \"\"\"\n",
    "    B, T = tgt_padding_mask.shape\n",
    "\n",
    "    nonpad = (~tgt_padding_mask)              # (B,T) True = token thật\n",
    "    causal = causal_cache[:T, :T]             # (T,T)\n",
    "\n",
    "    causal = causal.unsqueeze(0).unsqueeze(1) # (1,1,T,T)\n",
    "    nonpad_k = nonpad.unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
    "\n",
    "    return nonpad_k & causal                  # (B,1,T,T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "148ec5ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:16.059174Z",
     "iopub.status.busy": "2025-12-16T11:36:16.058483Z",
     "iopub.status.idle": "2025-12-16T11:36:17.539214Z",
     "shell.execute_reply": "2025-12-16T11:36:17.538278Z"
    },
    "papermill": {
     "duration": 1.485281,
     "end_time": "2025-12-16T11:36:17.540480",
     "exception": false,
     "start_time": "2025-12-16T11:36:16.055199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded checkpoint (strict=True)\n",
      "Missing keys: 0\n",
      "Unexpected keys: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from model import Transformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "SRC_VOCAB = 8000\n",
    "TGT_VOCAB = 8000\n",
    "D_MODEL   = 384\n",
    "N_LAYERS  = 4\n",
    "N_HEADS   = 8\n",
    "D_FF      = 1536\n",
    "DROPOUT   = 0.1\n",
    "MAX_LEN   = 5000\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=SRC_VOCAB,\n",
    "    tgt_vocab_size=TGT_VOCAB,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(device)\n",
    "\n",
    "state = torch.load(str(CKPT_PATH), map_location=device)\n",
    "missing, unexpected = model.load_state_dict(state, strict=True)\n",
    "\n",
    "print(\"✅ Loaded checkpoint (strict=True)\")\n",
    "print(\"Missing keys:\", len(missing))\n",
    "print(\"Unexpected keys:\", len(unexpected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31259a77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T11:36:17.546365Z",
     "iopub.status.busy": "2025-12-16T11:36:17.545784Z",
     "iopub.status.idle": "2025-12-16T12:32:00.778257Z",
     "shell.execute_reply": "2025-12-16T12:32:00.777458Z"
    },
    "papermill": {
     "duration": 3343.236706,
     "end_time": "2025-12-16T12:32:00.779476",
     "exception": false,
     "start_time": "2025-12-16T11:36:17.542770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f491890304e94799b21e03e36aa52f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 1/3:   0%|          | 0/7650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train epoch 1: first batch ok\n",
      "[epoch 1/3] step=10 batch=10/7650 loss=7.1836 avg=7.9732 ppl=2902.04 lr=3.00e-04\n",
      "[epoch 1/3] step=20 batch=20/7650 loss=6.8976 avg=7.4648 ppl=1745.58 lr=3.00e-04\n",
      "[epoch 1/3] step=30 batch=30/7650 loss=6.6993 avg=7.2193 ppl=1365.49 lr=3.00e-04\n",
      "[epoch 1/3] step=40 batch=40/7650 loss=6.2765 avg=7.0484 ppl=1151.07 lr=3.00e-04\n",
      "[epoch 1/3] step=50 batch=50/7650 loss=6.4063 avg=6.9228 ppl=1015.16 lr=3.00e-04\n",
      "[epoch 1/3] step=60 batch=60/7650 loss=6.1744 avg=6.8044 ppl=901.84 lr=3.00e-04\n",
      "[epoch 1/3] step=70 batch=70/7650 loss=5.9963 avg=6.7047 ppl=816.19 lr=3.00e-04\n",
      "[epoch 1/3] step=80 batch=80/7650 loss=5.8949 avg=6.6154 ppl=746.52 lr=3.00e-04\n",
      "[epoch 1/3] step=90 batch=90/7650 loss=5.6300 avg=6.5291 ppl=684.76 lr=3.00e-04\n",
      "[epoch 1/3] step=100 batch=100/7650 loss=6.0306 avg=6.4560 ppl=636.53 lr=3.00e-04\n",
      "[epoch 1/3] step=110 batch=110/7650 loss=5.6456 avg=6.3803 ppl=590.09 lr=3.00e-04\n",
      "[epoch 1/3] step=120 batch=120/7650 loss=5.5894 avg=6.3130 ppl=551.68 lr=3.00e-04\n",
      "[epoch 1/3] step=130 batch=130/7650 loss=5.3158 avg=6.2446 ppl=515.20 lr=3.00e-04\n",
      "[epoch 1/3] step=140 batch=140/7650 loss=5.2953 avg=6.1799 ppl=482.93 lr=3.00e-04\n",
      "[epoch 1/3] step=150 batch=150/7650 loss=5.2429 avg=6.1189 ppl=454.38 lr=3.00e-04\n",
      "[epoch 1/3] step=160 batch=160/7650 loss=5.3948 avg=6.0649 ppl=430.46 lr=3.00e-04\n",
      "[epoch 1/3] step=170 batch=170/7650 loss=5.0977 avg=6.0137 ppl=408.97 lr=3.00e-04\n",
      "[epoch 1/3] step=180 batch=180/7650 loss=5.2878 avg=5.9645 ppl=389.35 lr=3.00e-04\n",
      "[epoch 1/3] step=190 batch=190/7650 loss=4.9598 avg=5.9125 ppl=369.63 lr=3.00e-04\n",
      "[epoch 1/3] step=200 batch=200/7650 loss=4.9555 avg=5.8642 ppl=352.18 lr=3.00e-04\n",
      "[epoch 1/3] step=210 batch=210/7650 loss=5.1897 avg=5.8193 ppl=336.73 lr=3.00e-04\n",
      "[epoch 1/3] step=220 batch=220/7650 loss=4.7484 avg=5.7781 ppl=323.14 lr=3.00e-04\n",
      "[epoch 1/3] step=230 batch=230/7650 loss=4.9005 avg=5.7372 ppl=310.18 lr=3.00e-04\n",
      "[epoch 1/3] step=240 batch=240/7650 loss=4.8627 avg=5.6978 ppl=298.20 lr=3.00e-04\n",
      "[epoch 1/3] step=250 batch=250/7650 loss=4.6921 avg=5.6577 ppl=286.50 lr=3.00e-04\n",
      "[epoch 1/3] step=260 batch=260/7650 loss=4.7418 avg=5.6211 ppl=276.20 lr=3.00e-04\n",
      "[epoch 1/3] step=270 batch=270/7650 loss=4.7236 avg=5.5853 ppl=266.48 lr=3.00e-04\n",
      "[epoch 1/3] step=280 batch=280/7650 loss=4.5965 avg=5.5529 ppl=258.00 lr=3.00e-04\n",
      "[epoch 1/3] step=290 batch=290/7650 loss=4.5277 avg=5.5195 ppl=249.51 lr=3.00e-04\n",
      "[epoch 1/3] step=300 batch=300/7650 loss=4.5953 avg=5.4870 ppl=241.54 lr=3.00e-04\n",
      "[epoch 1/3] step=310 batch=310/7650 loss=4.4392 avg=5.4555 ppl=234.03 lr=3.00e-04\n",
      "[epoch 1/3] step=320 batch=320/7650 loss=4.3160 avg=5.4248 ppl=226.98 lr=3.00e-04\n",
      "[epoch 1/3] step=330 batch=330/7650 loss=4.5713 avg=5.3943 ppl=220.14 lr=3.00e-04\n",
      "[epoch 1/3] step=340 batch=340/7650 loss=4.3028 avg=5.3645 ppl=213.68 lr=3.00e-04\n",
      "[epoch 1/3] step=350 batch=350/7650 loss=4.2599 avg=5.3343 ppl=207.33 lr=3.00e-04\n",
      "[epoch 1/3] step=360 batch=360/7650 loss=4.6343 avg=5.3081 ppl=201.97 lr=3.00e-04\n",
      "[epoch 1/3] step=370 batch=370/7650 loss=4.1860 avg=5.2819 ppl=196.75 lr=3.00e-04\n",
      "[epoch 1/3] step=380 batch=380/7650 loss=4.1736 avg=5.2547 ppl=191.47 lr=3.00e-04\n",
      "[epoch 1/3] step=390 batch=390/7650 loss=4.4146 avg=5.2273 ppl=186.29 lr=3.00e-04\n",
      "[epoch 1/3] step=400 batch=400/7650 loss=4.3051 avg=5.2014 ppl=181.53 lr=3.00e-04\n",
      "[epoch 1/3] step=410 batch=410/7650 loss=4.1457 avg=5.1770 ppl=177.16 lr=3.00e-04\n",
      "[epoch 1/3] step=420 batch=420/7650 loss=4.2752 avg=5.1523 ppl=172.83 lr=3.00e-04\n",
      "[epoch 1/3] step=430 batch=430/7650 loss=4.2947 avg=5.1287 ppl=168.79 lr=3.00e-04\n",
      "[epoch 1/3] step=440 batch=440/7650 loss=3.9403 avg=5.1055 ppl=164.93 lr=3.00e-04\n",
      "[epoch 1/3] step=450 batch=450/7650 loss=3.9358 avg=5.0834 ppl=161.33 lr=3.00e-04\n",
      "[epoch 1/3] step=460 batch=460/7650 loss=4.0398 avg=5.0605 ppl=157.67 lr=3.00e-04\n",
      "[epoch 1/3] step=470 batch=470/7650 loss=4.0326 avg=5.0373 ppl=154.06 lr=3.00e-04\n",
      "[epoch 1/3] step=480 batch=480/7650 loss=3.9284 avg=5.0152 ppl=150.69 lr=3.00e-04\n",
      "[epoch 1/3] step=490 batch=490/7650 loss=3.9094 avg=4.9936 ppl=147.46 lr=3.00e-04\n",
      "[epoch 1/3] step=500 batch=500/7650 loss=3.9329 avg=4.9713 ppl=144.22 lr=3.00e-04\n",
      "[epoch 1/3] step=510 batch=510/7650 loss=3.9389 avg=4.9507 ppl=141.27 lr=3.00e-04\n",
      "[epoch 1/3] step=520 batch=520/7650 loss=3.7431 avg=4.9292 ppl=138.27 lr=3.00e-04\n",
      "[epoch 1/3] step=530 batch=530/7650 loss=4.1802 avg=4.9090 ppl=135.51 lr=3.00e-04\n",
      "[epoch 1/3] step=540 batch=540/7650 loss=3.9874 avg=4.8902 ppl=132.98 lr=3.00e-04\n",
      "[epoch 1/3] step=550 batch=550/7650 loss=3.7786 avg=4.8710 ppl=130.45 lr=3.00e-04\n",
      "[epoch 1/3] step=560 batch=560/7650 loss=3.8208 avg=4.8526 ppl=128.07 lr=3.00e-04\n",
      "[epoch 1/3] step=570 batch=570/7650 loss=3.7186 avg=4.8336 ppl=125.66 lr=3.00e-04\n",
      "[epoch 1/3] step=580 batch=580/7650 loss=3.6140 avg=4.8150 ppl=123.34 lr=3.00e-04\n",
      "[epoch 1/3] step=590 batch=590/7650 loss=3.6370 avg=4.7958 ppl=121.00 lr=3.00e-04\n",
      "[epoch 1/3] step=600 batch=600/7650 loss=3.8443 avg=4.7778 ppl=118.85 lr=3.00e-04\n",
      "[epoch 1/3] step=610 batch=610/7650 loss=3.6619 avg=4.7607 ppl=116.83 lr=3.00e-04\n",
      "[epoch 1/3] step=620 batch=620/7650 loss=3.8021 avg=4.7431 ppl=114.79 lr=3.00e-04\n",
      "[epoch 1/3] step=630 batch=630/7650 loss=3.7776 avg=4.7270 ppl=112.96 lr=3.00e-04\n",
      "[epoch 1/3] step=640 batch=640/7650 loss=3.7263 avg=4.7110 ppl=111.17 lr=3.00e-04\n",
      "[epoch 1/3] step=650 batch=650/7650 loss=3.5827 avg=4.6930 ppl=109.18 lr=3.00e-04\n",
      "[epoch 1/3] step=660 batch=660/7650 loss=3.6652 avg=4.6771 ppl=107.46 lr=3.00e-04\n",
      "[epoch 1/3] step=670 batch=670/7650 loss=3.5789 avg=4.6609 ppl=105.73 lr=3.00e-04\n",
      "[epoch 1/3] step=680 batch=680/7650 loss=3.6102 avg=4.6465 ppl=104.22 lr=3.00e-04\n",
      "[epoch 1/3] step=690 batch=690/7650 loss=3.5809 avg=4.6312 ppl=102.64 lr=3.00e-04\n",
      "[epoch 1/3] step=700 batch=700/7650 loss=3.4475 avg=4.6157 ppl=101.06 lr=3.00e-04\n",
      "[epoch 1/3] step=710 batch=710/7650 loss=3.6771 avg=4.6007 ppl=99.55 lr=3.00e-04\n",
      "[epoch 1/3] step=720 batch=720/7650 loss=3.4996 avg=4.5855 ppl=98.05 lr=3.00e-04\n",
      "[epoch 1/3] step=730 batch=730/7650 loss=3.7021 avg=4.5716 ppl=96.69 lr=3.00e-04\n",
      "[epoch 1/3] step=740 batch=740/7650 loss=3.3383 avg=4.5575 ppl=95.35 lr=3.00e-04\n",
      "[epoch 1/3] step=750 batch=750/7650 loss=3.5741 avg=4.5431 ppl=93.98 lr=3.00e-04\n",
      "[epoch 1/3] step=760 batch=760/7650 loss=3.3127 avg=4.5282 ppl=92.59 lr=3.00e-04\n",
      "[epoch 1/3] step=770 batch=770/7650 loss=3.6562 avg=4.5146 ppl=91.34 lr=3.00e-04\n",
      "[epoch 1/3] step=780 batch=780/7650 loss=3.4017 avg=4.5004 ppl=90.05 lr=3.00e-04\n",
      "[epoch 1/3] step=790 batch=790/7650 loss=3.2381 avg=4.4866 ppl=88.82 lr=3.00e-04\n",
      "[epoch 1/3] step=800 batch=800/7650 loss=3.5642 avg=4.4742 ppl=87.73 lr=3.00e-04\n",
      "[epoch 1/3] step=810 batch=810/7650 loss=3.4385 avg=4.4620 ppl=86.66 lr=3.00e-04\n",
      "[epoch 1/3] step=820 batch=820/7650 loss=3.6607 avg=4.4498 ppl=85.61 lr=3.00e-04\n",
      "[epoch 1/3] step=830 batch=830/7650 loss=3.6754 avg=4.4376 ppl=84.57 lr=3.00e-04\n",
      "[epoch 1/3] step=840 batch=840/7650 loss=3.1919 avg=4.4249 ppl=83.51 lr=3.00e-04\n",
      "[epoch 1/3] step=850 batch=850/7650 loss=3.3958 avg=4.4125 ppl=82.47 lr=3.00e-04\n",
      "[epoch 1/3] step=860 batch=860/7650 loss=3.5806 avg=4.4006 ppl=81.50 lr=3.00e-04\n",
      "[epoch 1/3] step=870 batch=870/7650 loss=3.3986 avg=4.3888 ppl=80.55 lr=3.00e-04\n",
      "[epoch 1/3] step=880 batch=880/7650 loss=3.4784 avg=4.3765 ppl=79.56 lr=3.00e-04\n",
      "[epoch 1/3] step=890 batch=890/7650 loss=3.2698 avg=4.3645 ppl=78.61 lr=3.00e-04\n",
      "[epoch 1/3] step=900 batch=900/7650 loss=3.2884 avg=4.3525 ppl=77.67 lr=3.00e-04\n",
      "[epoch 1/3] step=910 batch=910/7650 loss=3.3262 avg=4.3408 ppl=76.77 lr=3.00e-04\n",
      "[epoch 1/3] step=920 batch=920/7650 loss=3.1785 avg=4.3293 ppl=75.89 lr=3.00e-04\n",
      "[epoch 1/3] step=930 batch=930/7650 loss=3.3966 avg=4.3182 ppl=75.05 lr=3.00e-04\n",
      "[epoch 1/3] step=940 batch=940/7650 loss=3.3084 avg=4.3065 ppl=74.18 lr=3.00e-04\n",
      "[epoch 1/3] step=950 batch=950/7650 loss=3.2781 avg=4.2958 ppl=73.39 lr=3.00e-04\n",
      "[epoch 1/3] step=960 batch=960/7650 loss=3.0249 avg=4.2850 ppl=72.61 lr=3.00e-04\n",
      "[epoch 1/3] step=970 batch=970/7650 loss=3.3253 avg=4.2743 ppl=71.83 lr=3.00e-04\n",
      "[epoch 1/3] step=980 batch=980/7650 loss=3.2199 avg=4.2636 ppl=71.07 lr=3.00e-04\n",
      "[epoch 1/3] step=990 batch=990/7650 loss=3.1763 avg=4.2536 ppl=70.36 lr=3.00e-04\n",
      "[epoch 1/3] step=1000 batch=1000/7650 loss=3.3138 avg=4.2430 ppl=69.61 lr=3.00e-04\n",
      "[epoch 1/3] step=1010 batch=1010/7650 loss=3.2507 avg=4.2326 ppl=68.90 lr=3.00e-04\n",
      "[epoch 1/3] step=1020 batch=1020/7650 loss=3.3847 avg=4.2229 ppl=68.23 lr=3.00e-04\n",
      "[epoch 1/3] step=1030 batch=1030/7650 loss=3.2803 avg=4.2126 ppl=67.53 lr=3.00e-04\n",
      "[epoch 1/3] step=1040 batch=1040/7650 loss=2.9326 avg=4.2020 ppl=66.82 lr=3.00e-04\n",
      "[epoch 1/3] step=1050 batch=1050/7650 loss=3.2179 avg=4.1924 ppl=66.18 lr=3.00e-04\n",
      "[epoch 1/3] step=1060 batch=1060/7650 loss=3.1598 avg=4.1834 ppl=65.59 lr=3.00e-04\n",
      "[epoch 1/3] step=1070 batch=1070/7650 loss=3.2911 avg=4.1737 ppl=64.95 lr=3.00e-04\n",
      "[epoch 1/3] step=1080 batch=1080/7650 loss=3.2373 avg=4.1648 ppl=64.38 lr=3.00e-04\n",
      "[epoch 1/3] step=1090 batch=1090/7650 loss=3.0568 avg=4.1552 ppl=63.76 lr=3.00e-04\n",
      "[epoch 1/3] step=1100 batch=1100/7650 loss=3.1594 avg=4.1462 ppl=63.19 lr=3.00e-04\n",
      "[epoch 1/3] step=1110 batch=1110/7650 loss=3.0874 avg=4.1367 ppl=62.60 lr=3.00e-04\n",
      "[epoch 1/3] step=1120 batch=1120/7650 loss=3.0901 avg=4.1276 ppl=62.03 lr=3.00e-04\n",
      "[epoch 1/3] step=1130 batch=1130/7650 loss=3.0370 avg=4.1178 ppl=61.42 lr=3.00e-04\n",
      "[epoch 1/3] step=1140 batch=1140/7650 loss=3.2058 avg=4.1090 ppl=60.88 lr=3.00e-04\n",
      "[epoch 1/3] step=1150 batch=1150/7650 loss=3.1958 avg=4.1004 ppl=60.36 lr=3.00e-04\n",
      "[epoch 1/3] step=1160 batch=1160/7650 loss=3.0790 avg=4.0917 ppl=59.84 lr=3.00e-04\n",
      "[epoch 1/3] step=1170 batch=1170/7650 loss=3.0408 avg=4.0833 ppl=59.34 lr=3.00e-04\n",
      "[epoch 1/3] step=1180 batch=1180/7650 loss=3.0875 avg=4.0743 ppl=58.81 lr=3.00e-04\n",
      "[epoch 1/3] step=1190 batch=1190/7650 loss=3.2565 avg=4.0661 ppl=58.33 lr=3.00e-04\n",
      "[epoch 1/3] step=1200 batch=1200/7650 loss=2.9665 avg=4.0579 ppl=57.85 lr=3.00e-04\n",
      "[epoch 1/3] step=1210 batch=1210/7650 loss=3.0263 avg=4.0491 ppl=57.35 lr=3.00e-04\n",
      "[epoch 1/3] step=1220 batch=1220/7650 loss=2.9535 avg=4.0408 ppl=56.87 lr=3.00e-04\n",
      "[epoch 1/3] step=1230 batch=1230/7650 loss=3.0056 avg=4.0326 ppl=56.41 lr=3.00e-04\n",
      "[epoch 1/3] step=1240 batch=1240/7650 loss=3.0133 avg=4.0238 ppl=55.91 lr=3.00e-04\n",
      "[epoch 1/3] step=1250 batch=1250/7650 loss=2.8828 avg=4.0156 ppl=55.45 lr=3.00e-04\n",
      "[epoch 1/3] step=1260 batch=1260/7650 loss=2.9755 avg=4.0072 ppl=54.99 lr=3.00e-04\n",
      "[epoch 1/3] step=1270 batch=1270/7650 loss=2.8696 avg=3.9994 ppl=54.57 lr=3.00e-04\n",
      "[epoch 1/3] step=1280 batch=1280/7650 loss=3.0169 avg=3.9916 ppl=54.14 lr=3.00e-04\n",
      "[epoch 1/3] step=1290 batch=1290/7650 loss=2.9617 avg=3.9843 ppl=53.75 lr=3.00e-04\n",
      "[epoch 1/3] step=1300 batch=1300/7650 loss=2.8713 avg=3.9767 ppl=53.34 lr=3.00e-04\n",
      "[epoch 1/3] step=1310 batch=1310/7650 loss=3.1765 avg=3.9693 ppl=52.95 lr=3.00e-04\n",
      "[epoch 1/3] step=1320 batch=1320/7650 loss=2.9089 avg=3.9617 ppl=52.54 lr=3.00e-04\n",
      "[epoch 1/3] step=1330 batch=1330/7650 loss=2.9551 avg=3.9538 ppl=52.13 lr=3.00e-04\n",
      "[epoch 1/3] step=1340 batch=1340/7650 loss=3.0304 avg=3.9464 ppl=51.75 lr=3.00e-04\n",
      "[epoch 1/3] step=1350 batch=1350/7650 loss=3.0740 avg=3.9393 ppl=51.38 lr=3.00e-04\n",
      "[epoch 1/3] step=1360 batch=1360/7650 loss=2.6476 avg=3.9316 ppl=50.99 lr=3.00e-04\n",
      "[epoch 1/3] step=1370 batch=1370/7650 loss=3.1639 avg=3.9243 ppl=50.62 lr=3.00e-04\n",
      "[epoch 1/3] step=1380 batch=1380/7650 loss=2.8521 avg=3.9172 ppl=50.26 lr=3.00e-04\n",
      "[epoch 1/3] step=1390 batch=1390/7650 loss=2.9592 avg=3.9102 ppl=49.91 lr=3.00e-04\n",
      "[epoch 1/3] step=1400 batch=1400/7650 loss=2.9404 avg=3.9027 ppl=49.54 lr=3.00e-04\n",
      "[epoch 1/3] step=1410 batch=1410/7650 loss=3.0239 avg=3.8960 ppl=49.21 lr=3.00e-04\n",
      "[epoch 1/3] step=1420 batch=1420/7650 loss=2.7853 avg=3.8891 ppl=48.87 lr=3.00e-04\n",
      "[epoch 1/3] step=1430 batch=1430/7650 loss=2.8291 avg=3.8821 ppl=48.53 lr=3.00e-04\n",
      "[epoch 1/3] step=1440 batch=1440/7650 loss=2.8498 avg=3.8750 ppl=48.18 lr=3.00e-04\n",
      "[epoch 1/3] step=1450 batch=1450/7650 loss=2.9572 avg=3.8680 ppl=47.85 lr=3.00e-04\n",
      "[epoch 1/3] step=1460 batch=1460/7650 loss=2.8963 avg=3.8615 ppl=47.54 lr=3.00e-04\n",
      "[epoch 1/3] step=1470 batch=1470/7650 loss=2.5571 avg=3.8548 ppl=47.22 lr=3.00e-04\n",
      "[epoch 1/3] step=1480 batch=1480/7650 loss=2.8648 avg=3.8485 ppl=46.92 lr=3.00e-04\n",
      "[epoch 1/3] step=1490 batch=1490/7650 loss=3.0083 avg=3.8420 ppl=46.62 lr=3.00e-04\n",
      "[epoch 1/3] step=1500 batch=1500/7650 loss=2.8018 avg=3.8354 ppl=46.31 lr=3.00e-04\n",
      "[epoch 1/3] step=1510 batch=1510/7650 loss=2.7853 avg=3.8287 ppl=46.00 lr=3.00e-04\n",
      "[epoch 1/3] step=1520 batch=1520/7650 loss=3.0056 avg=3.8221 ppl=45.70 lr=3.00e-04\n",
      "[epoch 1/3] step=1530 batch=1530/7650 loss=2.7323 avg=3.8155 ppl=45.40 lr=3.00e-04\n",
      "[epoch 1/3] step=1540 batch=1540/7650 loss=2.7639 avg=3.8089 ppl=45.10 lr=3.00e-04\n",
      "[epoch 1/3] step=1550 batch=1550/7650 loss=2.9152 avg=3.8024 ppl=44.81 lr=3.00e-04\n",
      "[epoch 1/3] step=1560 batch=1560/7650 loss=2.9127 avg=3.7962 ppl=44.53 lr=3.00e-04\n",
      "[epoch 1/3] step=1570 batch=1570/7650 loss=2.7439 avg=3.7896 ppl=44.24 lr=3.00e-04\n",
      "[epoch 1/3] step=1580 batch=1580/7650 loss=2.8319 avg=3.7834 ppl=43.96 lr=3.00e-04\n",
      "[epoch 1/3] step=1590 batch=1590/7650 loss=2.6750 avg=3.7774 ppl=43.70 lr=3.00e-04\n",
      "[epoch 1/3] step=1600 batch=1600/7650 loss=2.9907 avg=3.7709 ppl=43.42 lr=3.00e-04\n",
      "[epoch 1/3] step=1610 batch=1610/7650 loss=2.7682 avg=3.7648 ppl=43.15 lr=3.00e-04\n",
      "[epoch 1/3] step=1620 batch=1620/7650 loss=2.7195 avg=3.7585 ppl=42.88 lr=3.00e-04\n",
      "[epoch 1/3] step=1630 batch=1630/7650 loss=2.6205 avg=3.7524 ppl=42.62 lr=3.00e-04\n",
      "[epoch 1/3] step=1640 batch=1640/7650 loss=2.6784 avg=3.7462 ppl=42.36 lr=3.00e-04\n",
      "[epoch 1/3] step=1650 batch=1650/7650 loss=2.5587 avg=3.7397 ppl=42.09 lr=3.00e-04\n",
      "[epoch 1/3] step=1660 batch=1660/7650 loss=2.6932 avg=3.7337 ppl=41.83 lr=3.00e-04\n",
      "[epoch 1/3] step=1670 batch=1670/7650 loss=2.6713 avg=3.7282 ppl=41.60 lr=3.00e-04\n",
      "[epoch 1/3] step=1680 batch=1680/7650 loss=2.7805 avg=3.7222 ppl=41.36 lr=3.00e-04\n",
      "[epoch 1/3] step=1690 batch=1690/7650 loss=2.6558 avg=3.7168 ppl=41.13 lr=3.00e-04\n",
      "[epoch 1/3] step=1700 batch=1700/7650 loss=2.8151 avg=3.7112 ppl=40.90 lr=3.00e-04\n",
      "[epoch 1/3] step=1710 batch=1710/7650 loss=2.6942 avg=3.7055 ppl=40.67 lr=3.00e-04\n",
      "[epoch 1/3] step=1720 batch=1720/7650 loss=2.7005 avg=3.6998 ppl=40.44 lr=3.00e-04\n",
      "[epoch 1/3] step=1730 batch=1730/7650 loss=2.8382 avg=3.6941 ppl=40.21 lr=3.00e-04\n",
      "[epoch 1/3] step=1740 batch=1740/7650 loss=2.6597 avg=3.6885 ppl=39.98 lr=3.00e-04\n",
      "[epoch 1/3] step=1750 batch=1750/7650 loss=2.5792 avg=3.6831 ppl=39.77 lr=3.00e-04\n",
      "[epoch 1/3] step=1760 batch=1760/7650 loss=2.7243 avg=3.6777 ppl=39.55 lr=3.00e-04\n",
      "[epoch 1/3] step=1770 batch=1770/7650 loss=2.7254 avg=3.6721 ppl=39.33 lr=3.00e-04\n",
      "[epoch 1/3] step=1780 batch=1780/7650 loss=2.6924 avg=3.6669 ppl=39.13 lr=3.00e-04\n",
      "[epoch 1/3] step=1790 batch=1790/7650 loss=2.5005 avg=3.6613 ppl=38.91 lr=3.00e-04\n",
      "[epoch 1/3] step=1800 batch=1800/7650 loss=2.6945 avg=3.6557 ppl=38.70 lr=3.00e-04\n",
      "[epoch 1/3] step=1810 batch=1810/7650 loss=2.8080 avg=3.6503 ppl=38.49 lr=3.00e-04\n",
      "[epoch 1/3] step=1820 batch=1820/7650 loss=2.8722 avg=3.6453 ppl=38.29 lr=3.00e-04\n",
      "[epoch 1/3] step=1830 batch=1830/7650 loss=2.6177 avg=3.6396 ppl=38.08 lr=3.00e-04\n",
      "[epoch 1/3] step=1840 batch=1840/7650 loss=2.6789 avg=3.6343 ppl=37.88 lr=3.00e-04\n",
      "[epoch 1/3] step=1850 batch=1850/7650 loss=2.7184 avg=3.6289 ppl=37.67 lr=3.00e-04\n",
      "[epoch 1/3] step=1860 batch=1860/7650 loss=2.9516 avg=3.6240 ppl=37.49 lr=3.00e-04\n",
      "[epoch 1/3] step=1870 batch=1870/7650 loss=2.5701 avg=3.6186 ppl=37.29 lr=3.00e-04\n",
      "[epoch 1/3] step=1880 batch=1880/7650 loss=2.4445 avg=3.6134 ppl=37.09 lr=3.00e-04\n",
      "[epoch 1/3] step=1890 batch=1890/7650 loss=2.8643 avg=3.6085 ppl=36.91 lr=3.00e-04\n",
      "[epoch 1/3] step=1900 batch=1900/7650 loss=2.7229 avg=3.6033 ppl=36.72 lr=3.00e-04\n",
      "[epoch 1/3] step=1910 batch=1910/7650 loss=2.8317 avg=3.5986 ppl=36.55 lr=3.00e-04\n",
      "[epoch 1/3] step=1920 batch=1920/7650 loss=2.8366 avg=3.5938 ppl=36.37 lr=3.00e-04\n",
      "[epoch 1/3] step=1930 batch=1930/7650 loss=2.6314 avg=3.5885 ppl=36.18 lr=3.00e-04\n",
      "[epoch 1/3] step=1940 batch=1940/7650 loss=2.5503 avg=3.5834 ppl=35.99 lr=3.00e-04\n",
      "[epoch 1/3] step=1950 batch=1950/7650 loss=2.7498 avg=3.5785 ppl=35.82 lr=3.00e-04\n",
      "[epoch 1/3] step=1960 batch=1960/7650 loss=2.6620 avg=3.5736 ppl=35.64 lr=3.00e-04\n",
      "[epoch 1/3] step=1970 batch=1970/7650 loss=2.6249 avg=3.5685 ppl=35.46 lr=3.00e-04\n",
      "[epoch 1/3] step=1980 batch=1980/7650 loss=2.6260 avg=3.5639 ppl=35.30 lr=3.00e-04\n",
      "[epoch 1/3] step=1990 batch=1990/7650 loss=2.7679 avg=3.5592 ppl=35.14 lr=3.00e-04\n",
      "[epoch 1/3] step=2000 batch=2000/7650 loss=2.6590 avg=3.5544 ppl=34.97 lr=3.00e-04\n",
      "[epoch 1/3] step=2010 batch=2010/7650 loss=2.5925 avg=3.5497 ppl=34.80 lr=3.00e-04\n",
      "[epoch 1/3] step=2020 batch=2020/7650 loss=2.5099 avg=3.5449 ppl=34.64 lr=3.00e-04\n",
      "[epoch 1/3] step=2030 batch=2030/7650 loss=2.6551 avg=3.5402 ppl=34.47 lr=3.00e-04\n",
      "[epoch 1/3] step=2040 batch=2040/7650 loss=2.5854 avg=3.5356 ppl=34.31 lr=3.00e-04\n",
      "[epoch 1/3] step=2050 batch=2050/7650 loss=2.6857 avg=3.5308 ppl=34.15 lr=3.00e-04\n",
      "[epoch 1/3] step=2060 batch=2060/7650 loss=2.6022 avg=3.5261 ppl=33.99 lr=3.00e-04\n",
      "[epoch 1/3] step=2070 batch=2070/7650 loss=2.3034 avg=3.5215 ppl=33.83 lr=3.00e-04\n",
      "[epoch 1/3] step=2080 batch=2080/7650 loss=2.7674 avg=3.5170 ppl=33.68 lr=3.00e-04\n",
      "[epoch 1/3] step=2090 batch=2090/7650 loss=2.3996 avg=3.5122 ppl=33.52 lr=3.00e-04\n",
      "[epoch 1/3] step=2100 batch=2100/7650 loss=2.6697 avg=3.5074 ppl=33.36 lr=3.00e-04\n",
      "[epoch 1/3] step=2110 batch=2110/7650 loss=2.6146 avg=3.5027 ppl=33.20 lr=3.00e-04\n",
      "[epoch 1/3] step=2120 batch=2120/7650 loss=2.4586 avg=3.4982 ppl=33.06 lr=3.00e-04\n",
      "[epoch 1/3] step=2130 batch=2130/7650 loss=2.5131 avg=3.4935 ppl=32.90 lr=3.00e-04\n",
      "[epoch 1/3] step=2140 batch=2140/7650 loss=2.1452 avg=3.4891 ppl=32.76 lr=3.00e-04\n",
      "[epoch 1/3] step=2150 batch=2150/7650 loss=2.6583 avg=3.4846 ppl=32.61 lr=3.00e-04\n",
      "[epoch 1/3] step=2160 batch=2160/7650 loss=2.3555 avg=3.4802 ppl=32.46 lr=3.00e-04\n",
      "[epoch 1/3] step=2170 batch=2170/7650 loss=2.4547 avg=3.4759 ppl=32.33 lr=3.00e-04\n",
      "[epoch 1/3] step=2180 batch=2180/7650 loss=2.5212 avg=3.4718 ppl=32.19 lr=3.00e-04\n",
      "[epoch 1/3] step=2190 batch=2190/7650 loss=2.6238 avg=3.4673 ppl=32.05 lr=3.00e-04\n",
      "[epoch 1/3] step=2200 batch=2200/7650 loss=2.7368 avg=3.4631 ppl=31.91 lr=3.00e-04\n",
      "[epoch 1/3] step=2210 batch=2210/7650 loss=2.6320 avg=3.4587 ppl=31.78 lr=3.00e-04\n",
      "[epoch 1/3] step=2220 batch=2220/7650 loss=2.4810 avg=3.4543 ppl=31.63 lr=3.00e-04\n",
      "[epoch 1/3] step=2230 batch=2230/7650 loss=2.5342 avg=3.4502 ppl=31.51 lr=3.00e-04\n",
      "[epoch 1/3] step=2240 batch=2240/7650 loss=2.5484 avg=3.4459 ppl=31.37 lr=3.00e-04\n",
      "[epoch 1/3] step=2250 batch=2250/7650 loss=2.5801 avg=3.4418 ppl=31.24 lr=3.00e-04\n",
      "[epoch 1/3] step=2260 batch=2260/7650 loss=2.6110 avg=3.4375 ppl=31.11 lr=3.00e-04\n",
      "[epoch 1/3] step=2270 batch=2270/7650 loss=2.3203 avg=3.4333 ppl=30.98 lr=3.00e-04\n",
      "[epoch 1/3] step=2280 batch=2280/7650 loss=2.3812 avg=3.4291 ppl=30.85 lr=3.00e-04\n",
      "[epoch 1/3] step=2290 batch=2290/7650 loss=2.5860 avg=3.4252 ppl=30.73 lr=3.00e-04\n",
      "[epoch 1/3] step=2300 batch=2300/7650 loss=2.5359 avg=3.4211 ppl=30.60 lr=3.00e-04\n",
      "[epoch 1/3] step=2310 batch=2310/7650 loss=2.4409 avg=3.4170 ppl=30.48 lr=3.00e-04\n",
      "[epoch 1/3] step=2320 batch=2320/7650 loss=2.5759 avg=3.4132 ppl=30.36 lr=3.00e-04\n",
      "[epoch 1/3] step=2330 batch=2330/7650 loss=2.5739 avg=3.4094 ppl=30.25 lr=3.00e-04\n",
      "[epoch 1/3] step=2340 batch=2340/7650 loss=2.3708 avg=3.4053 ppl=30.12 lr=3.00e-04\n",
      "[epoch 1/3] step=2350 batch=2350/7650 loss=2.3334 avg=3.4012 ppl=30.00 lr=3.00e-04\n",
      "[epoch 1/3] step=2360 batch=2360/7650 loss=2.5877 avg=3.3974 ppl=29.89 lr=3.00e-04\n",
      "[epoch 1/3] step=2370 batch=2370/7650 loss=2.3326 avg=3.3937 ppl=29.78 lr=3.00e-04\n",
      "[epoch 1/3] step=2380 batch=2380/7650 loss=2.4760 avg=3.3902 ppl=29.67 lr=3.00e-04\n",
      "[epoch 1/3] step=2390 batch=2390/7650 loss=2.5792 avg=3.3862 ppl=29.55 lr=3.00e-04\n",
      "[epoch 1/3] step=2400 batch=2400/7650 loss=2.3308 avg=3.3823 ppl=29.44 lr=3.00e-04\n",
      "[epoch 1/3] step=2410 batch=2410/7650 loss=2.5489 avg=3.3786 ppl=29.33 lr=3.00e-04\n",
      "[epoch 1/3] step=2420 batch=2420/7650 loss=2.2067 avg=3.3746 ppl=29.21 lr=3.00e-04\n",
      "[epoch 1/3] step=2430 batch=2430/7650 loss=2.5031 avg=3.3705 ppl=29.09 lr=3.00e-04\n",
      "[epoch 1/3] step=2440 batch=2440/7650 loss=2.4562 avg=3.3669 ppl=28.99 lr=3.00e-04\n",
      "[epoch 1/3] step=2450 batch=2450/7650 loss=2.3768 avg=3.3631 ppl=28.88 lr=3.00e-04\n",
      "[epoch 1/3] step=2460 batch=2460/7650 loss=2.5373 avg=3.3595 ppl=28.77 lr=3.00e-04\n",
      "[epoch 1/3] step=2470 batch=2470/7650 loss=2.5194 avg=3.3558 ppl=28.67 lr=3.00e-04\n",
      "[epoch 1/3] step=2480 batch=2480/7650 loss=2.3959 avg=3.3519 ppl=28.56 lr=3.00e-04\n",
      "[epoch 1/3] step=2490 batch=2490/7650 loss=2.5395 avg=3.3485 ppl=28.46 lr=3.00e-04\n",
      "[epoch 1/3] step=2500 batch=2500/7650 loss=2.2438 avg=3.3448 ppl=28.35 lr=3.00e-04\n",
      "[epoch 1/3] step=2510 batch=2510/7650 loss=2.5213 avg=3.3412 ppl=28.25 lr=3.00e-04\n",
      "[epoch 1/3] step=2520 batch=2520/7650 loss=2.4310 avg=3.3377 ppl=28.16 lr=3.00e-04\n",
      "[epoch 1/3] step=2530 batch=2530/7650 loss=2.3261 avg=3.3341 ppl=28.05 lr=3.00e-04\n",
      "[epoch 1/3] step=2540 batch=2540/7650 loss=2.4726 avg=3.3304 ppl=27.95 lr=3.00e-04\n",
      "[epoch 1/3] step=2550 batch=2550/7650 loss=2.6068 avg=3.3269 ppl=27.85 lr=3.00e-04\n",
      "[epoch 1/3] step=2560 batch=2560/7650 loss=2.5586 avg=3.3232 ppl=27.75 lr=3.00e-04\n",
      "[epoch 1/3] step=2570 batch=2570/7650 loss=2.2641 avg=3.3198 ppl=27.65 lr=3.00e-04\n",
      "[epoch 1/3] step=2580 batch=2580/7650 loss=2.5280 avg=3.3163 ppl=27.56 lr=3.00e-04\n",
      "[epoch 1/3] step=2590 batch=2590/7650 loss=2.3559 avg=3.3131 ppl=27.47 lr=3.00e-04\n",
      "[epoch 1/3] step=2600 batch=2600/7650 loss=2.4560 avg=3.3096 ppl=27.37 lr=3.00e-04\n",
      "[epoch 1/3] step=2610 batch=2610/7650 loss=2.4820 avg=3.3062 ppl=27.28 lr=3.00e-04\n",
      "[epoch 1/3] step=2620 batch=2620/7650 loss=2.2771 avg=3.3024 ppl=27.18 lr=3.00e-04\n",
      "[epoch 1/3] step=2630 batch=2630/7650 loss=2.3920 avg=3.2987 ppl=27.08 lr=3.00e-04\n",
      "[epoch 1/3] step=2640 batch=2640/7650 loss=2.4112 avg=3.2953 ppl=26.99 lr=3.00e-04\n",
      "[epoch 1/3] step=2650 batch=2650/7650 loss=2.3996 avg=3.2919 ppl=26.89 lr=3.00e-04\n",
      "[epoch 1/3] step=2660 batch=2660/7650 loss=2.5515 avg=3.2887 ppl=26.81 lr=3.00e-04\n",
      "[epoch 1/3] step=2670 batch=2670/7650 loss=2.5288 avg=3.2854 ppl=26.72 lr=3.00e-04\n",
      "[epoch 1/3] step=2680 batch=2680/7650 loss=2.2339 avg=3.2822 ppl=26.63 lr=3.00e-04\n",
      "[epoch 1/3] step=2690 batch=2690/7650 loss=2.5460 avg=3.2788 ppl=26.54 lr=3.00e-04\n",
      "[epoch 1/3] step=2700 batch=2700/7650 loss=2.4356 avg=3.2759 ppl=26.47 lr=3.00e-04\n",
      "[epoch 1/3] step=2710 batch=2710/7650 loss=2.4516 avg=3.2728 ppl=26.38 lr=3.00e-04\n",
      "[epoch 1/3] step=2720 batch=2720/7650 loss=2.1960 avg=3.2696 ppl=26.30 lr=3.00e-04\n",
      "[epoch 1/3] step=2730 batch=2730/7650 loss=2.1263 avg=3.2663 ppl=26.21 lr=3.00e-04\n",
      "[epoch 1/3] step=2740 batch=2740/7650 loss=2.3479 avg=3.2631 ppl=26.13 lr=3.00e-04\n",
      "[epoch 1/3] step=2750 batch=2750/7650 loss=2.3797 avg=3.2599 ppl=26.05 lr=3.00e-04\n",
      "[epoch 1/3] step=2760 batch=2760/7650 loss=2.4564 avg=3.2567 ppl=25.96 lr=3.00e-04\n",
      "[epoch 1/3] step=2770 batch=2770/7650 loss=2.5546 avg=3.2537 ppl=25.89 lr=3.00e-04\n",
      "[epoch 1/3] step=2780 batch=2780/7650 loss=2.3225 avg=3.2507 ppl=25.81 lr=3.00e-04\n",
      "[epoch 1/3] step=2790 batch=2790/7650 loss=2.2164 avg=3.2476 ppl=25.73 lr=3.00e-04\n",
      "[epoch 1/3] step=2800 batch=2800/7650 loss=2.3340 avg=3.2444 ppl=25.65 lr=3.00e-04\n",
      "[epoch 1/3] step=2810 batch=2810/7650 loss=2.5901 avg=3.2412 ppl=25.56 lr=3.00e-04\n",
      "[epoch 1/3] step=2820 batch=2820/7650 loss=2.4591 avg=3.2382 ppl=25.49 lr=3.00e-04\n",
      "[epoch 1/3] step=2830 batch=2830/7650 loss=2.4037 avg=3.2352 ppl=25.41 lr=3.00e-04\n",
      "[epoch 1/3] step=2840 batch=2840/7650 loss=2.4700 avg=3.2322 ppl=25.33 lr=3.00e-04\n",
      "[epoch 1/3] step=2850 batch=2850/7650 loss=2.6363 avg=3.2291 ppl=25.26 lr=3.00e-04\n",
      "[epoch 1/3] step=2860 batch=2860/7650 loss=2.4992 avg=3.2261 ppl=25.18 lr=3.00e-04\n",
      "[epoch 1/3] step=2870 batch=2870/7650 loss=2.3865 avg=3.2230 ppl=25.10 lr=3.00e-04\n",
      "[epoch 1/3] step=2880 batch=2880/7650 loss=2.3339 avg=3.2200 ppl=25.03 lr=3.00e-04\n",
      "[epoch 1/3] step=2890 batch=2890/7650 loss=2.2299 avg=3.2166 ppl=24.94 lr=3.00e-04\n",
      "[epoch 1/3] step=2900 batch=2900/7650 loss=2.4648 avg=3.2136 ppl=24.87 lr=3.00e-04\n",
      "[epoch 1/3] step=2910 batch=2910/7650 loss=2.2232 avg=3.2106 ppl=24.79 lr=3.00e-04\n",
      "[epoch 1/3] step=2920 batch=2920/7650 loss=2.2249 avg=3.2076 ppl=24.72 lr=3.00e-04\n",
      "[epoch 1/3] step=2930 batch=2930/7650 loss=2.3342 avg=3.2046 ppl=24.65 lr=3.00e-04\n",
      "[epoch 1/3] step=2940 batch=2940/7650 loss=2.2160 avg=3.2016 ppl=24.57 lr=3.00e-04\n",
      "[epoch 1/3] step=2950 batch=2950/7650 loss=2.4299 avg=3.1988 ppl=24.50 lr=3.00e-04\n",
      "[epoch 1/3] step=2960 batch=2960/7650 loss=2.3494 avg=3.1961 ppl=24.44 lr=3.00e-04\n",
      "[epoch 1/3] step=2970 batch=2970/7650 loss=2.2586 avg=3.1931 ppl=24.36 lr=3.00e-04\n",
      "[epoch 1/3] step=2980 batch=2980/7650 loss=2.2222 avg=3.1901 ppl=24.29 lr=3.00e-04\n",
      "[epoch 1/3] step=2990 batch=2990/7650 loss=2.3086 avg=3.1869 ppl=24.21 lr=3.00e-04\n",
      "[epoch 1/3] step=3000 batch=3000/7650 loss=2.4786 avg=3.1839 ppl=24.14 lr=3.00e-04\n",
      "[epoch 1/3] step=3010 batch=3010/7650 loss=2.3378 avg=3.1811 ppl=24.07 lr=3.00e-04\n",
      "[epoch 1/3] step=3020 batch=3020/7650 loss=2.5427 avg=3.1783 ppl=24.01 lr=3.00e-04\n",
      "[epoch 1/3] step=3030 batch=3030/7650 loss=2.4112 avg=3.1756 ppl=23.94 lr=3.00e-04\n",
      "[epoch 1/3] step=3040 batch=3040/7650 loss=2.2851 avg=3.1729 ppl=23.88 lr=3.00e-04\n",
      "[epoch 1/3] step=3050 batch=3050/7650 loss=2.2571 avg=3.1702 ppl=23.81 lr=3.00e-04\n",
      "[epoch 1/3] step=3060 batch=3060/7650 loss=2.1620 avg=3.1675 ppl=23.75 lr=3.00e-04\n",
      "[epoch 1/3] step=3070 batch=3070/7650 loss=2.4958 avg=3.1646 ppl=23.68 lr=3.00e-04\n",
      "[epoch 1/3] step=3080 batch=3080/7650 loss=2.1331 avg=3.1618 ppl=23.61 lr=3.00e-04\n",
      "[epoch 1/3] step=3090 batch=3090/7650 loss=2.2449 avg=3.1590 ppl=23.55 lr=3.00e-04\n",
      "[epoch 1/3] step=3100 batch=3100/7650 loss=2.2561 avg=3.1563 ppl=23.48 lr=3.00e-04\n",
      "[epoch 1/3] step=3110 batch=3110/7650 loss=2.2594 avg=3.1537 ppl=23.42 lr=3.00e-04\n",
      "[epoch 1/3] step=3120 batch=3120/7650 loss=2.0420 avg=3.1509 ppl=23.36 lr=3.00e-04\n",
      "[epoch 1/3] step=3130 batch=3130/7650 loss=2.4628 avg=3.1484 ppl=23.30 lr=3.00e-04\n",
      "[epoch 1/3] step=3140 batch=3140/7650 loss=2.2825 avg=3.1457 ppl=23.24 lr=3.00e-04\n",
      "[epoch 1/3] step=3150 batch=3150/7650 loss=2.2325 avg=3.1430 ppl=23.17 lr=3.00e-04\n",
      "[epoch 1/3] step=3160 batch=3160/7650 loss=2.3481 avg=3.1403 ppl=23.11 lr=3.00e-04\n",
      "[epoch 1/3] step=3170 batch=3170/7650 loss=2.3855 avg=3.1377 ppl=23.05 lr=3.00e-04\n",
      "[epoch 1/3] step=3180 batch=3180/7650 loss=2.3029 avg=3.1350 ppl=22.99 lr=3.00e-04\n",
      "[epoch 1/3] step=3190 batch=3190/7650 loss=2.1478 avg=3.1323 ppl=22.93 lr=3.00e-04\n",
      "[epoch 1/3] step=3200 batch=3200/7650 loss=2.3421 avg=3.1299 ppl=22.87 lr=3.00e-04\n",
      "[epoch 1/3] step=3210 batch=3210/7650 loss=2.3447 avg=3.1271 ppl=22.81 lr=3.00e-04\n",
      "[epoch 1/3] step=3220 batch=3220/7650 loss=2.2642 avg=3.1245 ppl=22.75 lr=3.00e-04\n",
      "[epoch 1/3] step=3230 batch=3230/7650 loss=2.2672 avg=3.1218 ppl=22.69 lr=3.00e-04\n",
      "[epoch 1/3] step=3240 batch=3240/7650 loss=2.3924 avg=3.1193 ppl=22.63 lr=3.00e-04\n",
      "[epoch 1/3] step=3250 batch=3250/7650 loss=2.4441 avg=3.1167 ppl=22.57 lr=3.00e-04\n",
      "[epoch 1/3] step=3260 batch=3260/7650 loss=2.3478 avg=3.1142 ppl=22.52 lr=3.00e-04\n",
      "[epoch 1/3] step=3270 batch=3270/7650 loss=2.3401 avg=3.1116 ppl=22.46 lr=3.00e-04\n",
      "[epoch 1/3] step=3280 batch=3280/7650 loss=2.1551 avg=3.1090 ppl=22.40 lr=3.00e-04\n",
      "[epoch 1/3] step=3290 batch=3290/7650 loss=2.3054 avg=3.1064 ppl=22.34 lr=3.00e-04\n",
      "[epoch 1/3] step=3300 batch=3300/7650 loss=2.1717 avg=3.1039 ppl=22.29 lr=3.00e-04\n",
      "[epoch 1/3] step=3310 batch=3310/7650 loss=2.4779 avg=3.1014 ppl=22.23 lr=3.00e-04\n",
      "[epoch 1/3] step=3320 batch=3320/7650 loss=2.2524 avg=3.0991 ppl=22.18 lr=3.00e-04\n",
      "[epoch 1/3] step=3330 batch=3330/7650 loss=2.1783 avg=3.0964 ppl=22.12 lr=3.00e-04\n",
      "[epoch 1/3] step=3340 batch=3340/7650 loss=2.1730 avg=3.0940 ppl=22.07 lr=3.00e-04\n",
      "[epoch 1/3] step=3350 batch=3350/7650 loss=2.2187 avg=3.0917 ppl=22.01 lr=3.00e-04\n",
      "[epoch 1/3] step=3360 batch=3360/7650 loss=2.1651 avg=3.0893 ppl=21.96 lr=3.00e-04\n",
      "[epoch 1/3] step=3370 batch=3370/7650 loss=2.1910 avg=3.0869 ppl=21.91 lr=3.00e-04\n",
      "[epoch 1/3] step=3380 batch=3380/7650 loss=2.4497 avg=3.0845 ppl=21.86 lr=3.00e-04\n",
      "[epoch 1/3] step=3390 batch=3390/7650 loss=2.1046 avg=3.0820 ppl=21.80 lr=3.00e-04\n",
      "[epoch 1/3] step=3400 batch=3400/7650 loss=2.3477 avg=3.0797 ppl=21.75 lr=3.00e-04\n",
      "[epoch 1/3] step=3410 batch=3410/7650 loss=2.2676 avg=3.0772 ppl=21.70 lr=3.00e-04\n",
      "[epoch 1/3] step=3420 batch=3420/7650 loss=2.1492 avg=3.0750 ppl=21.65 lr=3.00e-04\n",
      "[epoch 1/3] step=3430 batch=3430/7650 loss=2.1041 avg=3.0724 ppl=21.59 lr=3.00e-04\n",
      "[epoch 1/3] step=3440 batch=3440/7650 loss=2.3395 avg=3.0700 ppl=21.54 lr=3.00e-04\n",
      "[epoch 1/3] step=3450 batch=3450/7650 loss=2.2374 avg=3.0677 ppl=21.49 lr=3.00e-04\n",
      "[epoch 1/3] step=3460 batch=3460/7650 loss=2.0911 avg=3.0652 ppl=21.44 lr=3.00e-04\n",
      "[epoch 1/3] step=3470 batch=3470/7650 loss=2.0924 avg=3.0627 ppl=21.39 lr=3.00e-04\n",
      "[epoch 1/3] step=3480 batch=3480/7650 loss=2.5487 avg=3.0603 ppl=21.33 lr=3.00e-04\n",
      "[epoch 1/3] step=3490 batch=3490/7650 loss=2.1771 avg=3.0580 ppl=21.29 lr=3.00e-04\n",
      "[epoch 1/3] step=3500 batch=3500/7650 loss=2.3168 avg=3.0556 ppl=21.23 lr=3.00e-04\n",
      "[epoch 1/3] step=3510 batch=3510/7650 loss=2.2887 avg=3.0533 ppl=21.19 lr=3.00e-04\n",
      "[epoch 1/3] step=3520 batch=3520/7650 loss=2.1179 avg=3.0509 ppl=21.13 lr=3.00e-04\n",
      "[epoch 1/3] step=3530 batch=3530/7650 loss=2.2964 avg=3.0487 ppl=21.09 lr=3.00e-04\n",
      "[epoch 1/3] step=3540 batch=3540/7650 loss=2.2510 avg=3.0463 ppl=21.04 lr=3.00e-04\n",
      "[epoch 1/3] step=3550 batch=3550/7650 loss=2.4293 avg=3.0440 ppl=20.99 lr=3.00e-04\n",
      "[epoch 1/3] step=3560 batch=3560/7650 loss=2.1819 avg=3.0417 ppl=20.94 lr=3.00e-04\n",
      "[epoch 1/3] step=3570 batch=3570/7650 loss=2.2234 avg=3.0393 ppl=20.89 lr=3.00e-04\n",
      "[epoch 1/3] step=3580 batch=3580/7650 loss=2.2649 avg=3.0370 ppl=20.84 lr=3.00e-04\n",
      "[epoch 1/3] step=3590 batch=3590/7650 loss=2.2557 avg=3.0347 ppl=20.79 lr=3.00e-04\n",
      "[epoch 1/3] step=3600 batch=3600/7650 loss=2.0566 avg=3.0325 ppl=20.75 lr=3.00e-04\n",
      "[epoch 1/3] step=3610 batch=3610/7650 loss=2.2914 avg=3.0304 ppl=20.70 lr=3.00e-04\n",
      "[epoch 1/3] step=3620 batch=3620/7650 loss=2.2560 avg=3.0282 ppl=20.66 lr=3.00e-04\n",
      "[epoch 1/3] step=3630 batch=3630/7650 loss=1.9351 avg=3.0258 ppl=20.61 lr=3.00e-04\n",
      "[epoch 1/3] step=3640 batch=3640/7650 loss=2.2488 avg=3.0235 ppl=20.56 lr=3.00e-04\n",
      "[epoch 1/3] step=3650 batch=3650/7650 loss=2.1173 avg=3.0213 ppl=20.52 lr=3.00e-04\n",
      "[epoch 1/3] step=3660 batch=3660/7650 loss=2.3383 avg=3.0190 ppl=20.47 lr=3.00e-04\n",
      "[epoch 1/3] step=3670 batch=3670/7650 loss=2.0651 avg=3.0167 ppl=20.42 lr=3.00e-04\n",
      "[epoch 1/3] step=3680 batch=3680/7650 loss=2.1397 avg=3.0144 ppl=20.38 lr=3.00e-04\n",
      "[epoch 1/3] step=3690 batch=3690/7650 loss=1.9833 avg=3.0120 ppl=20.33 lr=3.00e-04\n",
      "[epoch 1/3] step=3700 batch=3700/7650 loss=2.3027 avg=3.0096 ppl=20.28 lr=3.00e-04\n",
      "[epoch 1/3] step=3710 batch=3710/7650 loss=2.1511 avg=3.0074 ppl=20.24 lr=3.00e-04\n",
      "[epoch 1/3] step=3720 batch=3720/7650 loss=2.3190 avg=3.0055 ppl=20.20 lr=3.00e-04\n",
      "[epoch 1/3] step=3730 batch=3730/7650 loss=2.2033 avg=3.0032 ppl=20.15 lr=3.00e-04\n",
      "[epoch 1/3] step=3740 batch=3740/7650 loss=1.9955 avg=3.0012 ppl=20.11 lr=3.00e-04\n",
      "[epoch 1/3] step=3750 batch=3750/7650 loss=2.2607 avg=2.9991 ppl=20.07 lr=3.00e-04\n",
      "[epoch 1/3] step=3760 batch=3760/7650 loss=2.2952 avg=2.9969 ppl=20.02 lr=3.00e-04\n",
      "[epoch 1/3] step=3770 batch=3770/7650 loss=2.3176 avg=2.9948 ppl=19.98 lr=3.00e-04\n",
      "[epoch 1/3] step=3780 batch=3780/7650 loss=2.2459 avg=2.9926 ppl=19.94 lr=3.00e-04\n",
      "[epoch 1/3] step=3790 batch=3790/7650 loss=2.1464 avg=2.9905 ppl=19.89 lr=3.00e-04\n",
      "[epoch 1/3] step=3800 batch=3800/7650 loss=2.2383 avg=2.9882 ppl=19.85 lr=3.00e-04\n",
      "[epoch 1/3] step=3810 batch=3810/7650 loss=2.3431 avg=2.9860 ppl=19.81 lr=3.00e-04\n",
      "[epoch 1/3] step=3820 batch=3820/7650 loss=2.1393 avg=2.9839 ppl=19.77 lr=3.00e-04\n",
      "[epoch 1/3] step=3830 batch=3830/7650 loss=2.2943 avg=2.9818 ppl=19.72 lr=3.00e-04\n",
      "[epoch 1/3] step=3840 batch=3840/7650 loss=2.2026 avg=2.9798 ppl=19.68 lr=3.00e-04\n",
      "[epoch 1/3] step=3850 batch=3850/7650 loss=2.1354 avg=2.9778 ppl=19.64 lr=3.00e-04\n",
      "[epoch 1/3] step=3860 batch=3860/7650 loss=2.2900 avg=2.9757 ppl=19.60 lr=3.00e-04\n",
      "[epoch 1/3] step=3870 batch=3870/7650 loss=2.1965 avg=2.9736 ppl=19.56 lr=3.00e-04\n",
      "[epoch 1/3] step=3880 batch=3880/7650 loss=2.0750 avg=2.9714 ppl=19.52 lr=3.00e-04\n",
      "[epoch 1/3] step=3890 batch=3890/7650 loss=2.2191 avg=2.9694 ppl=19.48 lr=3.00e-04\n",
      "[epoch 1/3] step=3900 batch=3900/7650 loss=2.2356 avg=2.9673 ppl=19.44 lr=3.00e-04\n",
      "[epoch 1/3] step=3910 batch=3910/7650 loss=2.1151 avg=2.9652 ppl=19.40 lr=3.00e-04\n",
      "[epoch 1/3] step=3920 batch=3920/7650 loss=2.2960 avg=2.9630 ppl=19.36 lr=3.00e-04\n",
      "[epoch 1/3] step=3930 batch=3930/7650 loss=2.1092 avg=2.9609 ppl=19.32 lr=3.00e-04\n",
      "[epoch 1/3] step=3940 batch=3940/7650 loss=2.3513 avg=2.9588 ppl=19.28 lr=3.00e-04\n",
      "[epoch 1/3] step=3950 batch=3950/7650 loss=2.1159 avg=2.9567 ppl=19.23 lr=3.00e-04\n",
      "[epoch 1/3] step=3960 batch=3960/7650 loss=2.1749 avg=2.9546 ppl=19.19 lr=3.00e-04\n",
      "[epoch 1/3] step=3970 batch=3970/7650 loss=2.2468 avg=2.9525 ppl=19.15 lr=3.00e-04\n",
      "[epoch 1/3] step=3980 batch=3980/7650 loss=2.0898 avg=2.9505 ppl=19.11 lr=3.00e-04\n",
      "[epoch 1/3] step=3990 batch=3990/7650 loss=2.2003 avg=2.9485 ppl=19.08 lr=3.00e-04\n",
      "[epoch 1/3] step=4000 batch=4000/7650 loss=2.1537 avg=2.9464 ppl=19.04 lr=3.00e-04\n",
      "[epoch 1/3] step=4010 batch=4010/7650 loss=1.9723 avg=2.9443 ppl=19.00 lr=3.00e-04\n",
      "[epoch 1/3] step=4020 batch=4020/7650 loss=2.0674 avg=2.9424 ppl=18.96 lr=3.00e-04\n",
      "[epoch 1/3] step=4030 batch=4030/7650 loss=2.3025 avg=2.9402 ppl=18.92 lr=3.00e-04\n",
      "[epoch 1/3] step=4040 batch=4040/7650 loss=2.1247 avg=2.9383 ppl=18.88 lr=3.00e-04\n",
      "[epoch 1/3] step=4050 batch=4050/7650 loss=1.9652 avg=2.9361 ppl=18.84 lr=3.00e-04\n",
      "[epoch 1/3] step=4060 batch=4060/7650 loss=2.2015 avg=2.9342 ppl=18.81 lr=3.00e-04\n",
      "[epoch 1/3] step=4070 batch=4070/7650 loss=2.1226 avg=2.9323 ppl=18.77 lr=3.00e-04\n",
      "[epoch 1/3] step=4080 batch=4080/7650 loss=2.1050 avg=2.9304 ppl=18.73 lr=3.00e-04\n",
      "[epoch 1/3] step=4090 batch=4090/7650 loss=1.9920 avg=2.9283 ppl=18.70 lr=3.00e-04\n",
      "[epoch 1/3] step=4100 batch=4100/7650 loss=2.1863 avg=2.9265 ppl=18.66 lr=3.00e-04\n",
      "[epoch 1/3] step=4110 batch=4110/7650 loss=2.4040 avg=2.9246 ppl=18.63 lr=3.00e-04\n",
      "[epoch 1/3] step=4120 batch=4120/7650 loss=2.1584 avg=2.9228 ppl=18.59 lr=3.00e-04\n",
      "[epoch 1/3] step=4130 batch=4130/7650 loss=1.8707 avg=2.9207 ppl=18.55 lr=3.00e-04\n",
      "[epoch 1/3] step=4140 batch=4140/7650 loss=2.1305 avg=2.9189 ppl=18.52 lr=3.00e-04\n",
      "[epoch 1/3] step=4150 batch=4150/7650 loss=2.3852 avg=2.9172 ppl=18.49 lr=3.00e-04\n",
      "[epoch 1/3] step=4160 batch=4160/7650 loss=2.1429 avg=2.9153 ppl=18.45 lr=3.00e-04\n",
      "[epoch 1/3] step=4170 batch=4170/7650 loss=2.1514 avg=2.9136 ppl=18.42 lr=3.00e-04\n",
      "[epoch 1/3] step=4180 batch=4180/7650 loss=1.9738 avg=2.9116 ppl=18.39 lr=3.00e-04\n",
      "[epoch 1/3] step=4190 batch=4190/7650 loss=2.1984 avg=2.9096 ppl=18.35 lr=3.00e-04\n",
      "[epoch 1/3] step=4200 batch=4200/7650 loss=1.8477 avg=2.9078 ppl=18.32 lr=3.00e-04\n",
      "[epoch 1/3] step=4210 batch=4210/7650 loss=2.0072 avg=2.9059 ppl=18.28 lr=3.00e-04\n",
      "[epoch 1/3] step=4220 batch=4220/7650 loss=2.1148 avg=2.9041 ppl=18.25 lr=3.00e-04\n",
      "[epoch 1/3] step=4230 batch=4230/7650 loss=2.4037 avg=2.9025 ppl=18.22 lr=3.00e-04\n",
      "[epoch 1/3] step=4240 batch=4240/7650 loss=2.0849 avg=2.9006 ppl=18.19 lr=3.00e-04\n",
      "[epoch 1/3] step=4250 batch=4250/7650 loss=1.7932 avg=2.8986 ppl=18.15 lr=3.00e-04\n",
      "[epoch 1/3] step=4260 batch=4260/7650 loss=1.9240 avg=2.8968 ppl=18.12 lr=3.00e-04\n",
      "[epoch 1/3] step=4270 batch=4270/7650 loss=2.2514 avg=2.8950 ppl=18.08 lr=3.00e-04\n",
      "[epoch 1/3] step=4280 batch=4280/7650 loss=2.1022 avg=2.8932 ppl=18.05 lr=3.00e-04\n",
      "[epoch 1/3] step=4290 batch=4290/7650 loss=2.2293 avg=2.8915 ppl=18.02 lr=3.00e-04\n",
      "[epoch 1/3] step=4300 batch=4300/7650 loss=1.9914 avg=2.8896 ppl=17.99 lr=3.00e-04\n",
      "[epoch 1/3] step=4310 batch=4310/7650 loss=2.0992 avg=2.8878 ppl=17.95 lr=3.00e-04\n",
      "[epoch 1/3] step=4320 batch=4320/7650 loss=2.2185 avg=2.8859 ppl=17.92 lr=3.00e-04\n",
      "[epoch 1/3] step=4330 batch=4330/7650 loss=2.0518 avg=2.8842 ppl=17.89 lr=3.00e-04\n",
      "[epoch 1/3] step=4340 batch=4340/7650 loss=2.1298 avg=2.8824 ppl=17.86 lr=3.00e-04\n",
      "[epoch 1/3] step=4350 batch=4350/7650 loss=1.8523 avg=2.8805 ppl=17.82 lr=3.00e-04\n",
      "[epoch 1/3] step=4360 batch=4360/7650 loss=2.3272 avg=2.8790 ppl=17.80 lr=3.00e-04\n",
      "[epoch 1/3] step=4370 batch=4370/7650 loss=2.1254 avg=2.8772 ppl=17.76 lr=3.00e-04\n",
      "[epoch 1/3] step=4380 batch=4380/7650 loss=2.2289 avg=2.8753 ppl=17.73 lr=3.00e-04\n",
      "[epoch 1/3] step=4390 batch=4390/7650 loss=2.1104 avg=2.8735 ppl=17.70 lr=3.00e-04\n",
      "[epoch 1/3] step=4400 batch=4400/7650 loss=2.1868 avg=2.8718 ppl=17.67 lr=3.00e-04\n",
      "[epoch 1/3] step=4410 batch=4410/7650 loss=2.1962 avg=2.8700 ppl=17.64 lr=3.00e-04\n",
      "[epoch 1/3] step=4420 batch=4420/7650 loss=2.1276 avg=2.8684 ppl=17.61 lr=3.00e-04\n",
      "[epoch 1/3] step=4430 batch=4430/7650 loss=2.0971 avg=2.8667 ppl=17.58 lr=3.00e-04\n",
      "[epoch 1/3] step=4440 batch=4440/7650 loss=2.0658 avg=2.8648 ppl=17.55 lr=3.00e-04\n",
      "[epoch 1/3] step=4450 batch=4450/7650 loss=2.0911 avg=2.8630 ppl=17.51 lr=3.00e-04\n",
      "[epoch 1/3] step=4460 batch=4460/7650 loss=2.0527 avg=2.8613 ppl=17.48 lr=3.00e-04\n",
      "[epoch 1/3] step=4470 batch=4470/7650 loss=2.0903 avg=2.8594 ppl=17.45 lr=3.00e-04\n",
      "[epoch 1/3] step=4480 batch=4480/7650 loss=2.1235 avg=2.8576 ppl=17.42 lr=3.00e-04\n",
      "[epoch 1/3] step=4490 batch=4490/7650 loss=1.9740 avg=2.8558 ppl=17.39 lr=3.00e-04\n",
      "[epoch 1/3] step=4500 batch=4500/7650 loss=2.1149 avg=2.8542 ppl=17.36 lr=3.00e-04\n",
      "[epoch 1/3] step=4510 batch=4510/7650 loss=1.9206 avg=2.8524 ppl=17.33 lr=3.00e-04\n",
      "[epoch 1/3] step=4520 batch=4520/7650 loss=2.0100 avg=2.8507 ppl=17.30 lr=3.00e-04\n",
      "[epoch 1/3] step=4530 batch=4530/7650 loss=2.3928 avg=2.8492 ppl=17.27 lr=3.00e-04\n",
      "[epoch 1/3] step=4540 batch=4540/7650 loss=2.0321 avg=2.8476 ppl=17.25 lr=3.00e-04\n",
      "[epoch 1/3] step=4550 batch=4550/7650 loss=2.0746 avg=2.8460 ppl=17.22 lr=3.00e-04\n",
      "[epoch 1/3] step=4560 batch=4560/7650 loss=2.0297 avg=2.8443 ppl=17.19 lr=3.00e-04\n",
      "[epoch 1/3] step=4570 batch=4570/7650 loss=2.1621 avg=2.8426 ppl=17.16 lr=3.00e-04\n",
      "[epoch 1/3] step=4580 batch=4580/7650 loss=1.9016 avg=2.8411 ppl=17.13 lr=3.00e-04\n",
      "[epoch 1/3] step=4590 batch=4590/7650 loss=2.1329 avg=2.8394 ppl=17.10 lr=3.00e-04\n",
      "[epoch 1/3] step=4600 batch=4600/7650 loss=2.0647 avg=2.8376 ppl=17.08 lr=3.00e-04\n",
      "[epoch 1/3] step=4610 batch=4610/7650 loss=2.0428 avg=2.8359 ppl=17.05 lr=3.00e-04\n",
      "[epoch 1/3] step=4620 batch=4620/7650 loss=1.8717 avg=2.8342 ppl=17.02 lr=3.00e-04\n",
      "[epoch 1/3] step=4630 batch=4630/7650 loss=1.9314 avg=2.8324 ppl=16.99 lr=3.00e-04\n",
      "[epoch 1/3] step=4640 batch=4640/7650 loss=1.8242 avg=2.8307 ppl=16.96 lr=3.00e-04\n",
      "[epoch 1/3] step=4650 batch=4650/7650 loss=2.2251 avg=2.8293 ppl=16.93 lr=3.00e-04\n",
      "[epoch 1/3] step=4660 batch=4660/7650 loss=2.2632 avg=2.8278 ppl=16.91 lr=3.00e-04\n",
      "[epoch 1/3] step=4670 batch=4670/7650 loss=2.1517 avg=2.8262 ppl=16.88 lr=3.00e-04\n",
      "[epoch 1/3] step=4680 batch=4680/7650 loss=1.9325 avg=2.8246 ppl=16.85 lr=3.00e-04\n",
      "[epoch 1/3] step=4690 batch=4690/7650 loss=1.8747 avg=2.8228 ppl=16.82 lr=3.00e-04\n",
      "[epoch 1/3] step=4700 batch=4700/7650 loss=2.1594 avg=2.8213 ppl=16.80 lr=3.00e-04\n",
      "[epoch 1/3] step=4710 batch=4710/7650 loss=2.1300 avg=2.8197 ppl=16.77 lr=3.00e-04\n",
      "[epoch 1/3] step=4720 batch=4720/7650 loss=2.1821 avg=2.8182 ppl=16.75 lr=3.00e-04\n",
      "[epoch 1/3] step=4730 batch=4730/7650 loss=2.2817 avg=2.8167 ppl=16.72 lr=3.00e-04\n",
      "[epoch 1/3] step=4740 batch=4740/7650 loss=2.1563 avg=2.8151 ppl=16.69 lr=3.00e-04\n",
      "[epoch 1/3] step=4750 batch=4750/7650 loss=2.1134 avg=2.8136 ppl=16.67 lr=3.00e-04\n",
      "[epoch 1/3] step=4760 batch=4760/7650 loss=2.1956 avg=2.8121 ppl=16.65 lr=3.00e-04\n",
      "[epoch 1/3] step=4770 batch=4770/7650 loss=2.1167 avg=2.8107 ppl=16.62 lr=3.00e-04\n",
      "[epoch 1/3] step=4780 batch=4780/7650 loss=2.3376 avg=2.8093 ppl=16.60 lr=3.00e-04\n",
      "[epoch 1/3] step=4790 batch=4790/7650 loss=2.0932 avg=2.8077 ppl=16.57 lr=3.00e-04\n",
      "[epoch 1/3] step=4800 batch=4800/7650 loss=2.0684 avg=2.8062 ppl=16.55 lr=3.00e-04\n",
      "[epoch 1/3] step=4810 batch=4810/7650 loss=2.0199 avg=2.8047 ppl=16.52 lr=3.00e-04\n",
      "[epoch 1/3] step=4820 batch=4820/7650 loss=2.1508 avg=2.8033 ppl=16.50 lr=3.00e-04\n",
      "[epoch 1/3] step=4830 batch=4830/7650 loss=1.9755 avg=2.8018 ppl=16.47 lr=3.00e-04\n",
      "[epoch 1/3] step=4840 batch=4840/7650 loss=1.8836 avg=2.8002 ppl=16.45 lr=3.00e-04\n",
      "[epoch 1/3] step=4850 batch=4850/7650 loss=1.9883 avg=2.7986 ppl=16.42 lr=3.00e-04\n",
      "[epoch 1/3] step=4860 batch=4860/7650 loss=2.1331 avg=2.7972 ppl=16.40 lr=3.00e-04\n",
      "[epoch 1/3] step=4870 batch=4870/7650 loss=2.0581 avg=2.7957 ppl=16.37 lr=3.00e-04\n",
      "[epoch 1/3] step=4880 batch=4880/7650 loss=2.1105 avg=2.7943 ppl=16.35 lr=3.00e-04\n",
      "[epoch 1/3] step=4890 batch=4890/7650 loss=1.9616 avg=2.7927 ppl=16.33 lr=3.00e-04\n",
      "[epoch 1/3] step=4900 batch=4900/7650 loss=2.0170 avg=2.7911 ppl=16.30 lr=3.00e-04\n",
      "[epoch 1/3] step=4910 batch=4910/7650 loss=2.1634 avg=2.7898 ppl=16.28 lr=3.00e-04\n",
      "[epoch 1/3] step=4920 batch=4920/7650 loss=2.1229 avg=2.7884 ppl=16.25 lr=3.00e-04\n",
      "[epoch 1/3] step=4930 batch=4930/7650 loss=2.0263 avg=2.7869 ppl=16.23 lr=3.00e-04\n",
      "[epoch 1/3] step=4940 batch=4940/7650 loss=2.0216 avg=2.7854 ppl=16.21 lr=3.00e-04\n",
      "[epoch 1/3] step=4950 batch=4950/7650 loss=2.2319 avg=2.7840 ppl=16.18 lr=3.00e-04\n",
      "[epoch 1/3] step=4960 batch=4960/7650 loss=1.9240 avg=2.7824 ppl=16.16 lr=3.00e-04\n",
      "[epoch 1/3] step=4970 batch=4970/7650 loss=1.8507 avg=2.7808 ppl=16.13 lr=3.00e-04\n",
      "[epoch 1/3] step=4980 batch=4980/7650 loss=1.9651 avg=2.7793 ppl=16.11 lr=3.00e-04\n",
      "[epoch 1/3] step=4990 batch=4990/7650 loss=2.0760 avg=2.7780 ppl=16.09 lr=3.00e-04\n",
      "[epoch 1/3] step=5000 batch=5000/7650 loss=2.2502 avg=2.7767 ppl=16.07 lr=3.00e-04\n",
      "[epoch 1/3] step=5010 batch=5010/7650 loss=1.9682 avg=2.7752 ppl=16.04 lr=3.00e-04\n",
      "[epoch 1/3] step=5020 batch=5020/7650 loss=1.9122 avg=2.7737 ppl=16.02 lr=3.00e-04\n",
      "[epoch 1/3] step=5030 batch=5030/7650 loss=2.0647 avg=2.7722 ppl=15.99 lr=3.00e-04\n",
      "[epoch 1/3] step=5040 batch=5040/7650 loss=1.9456 avg=2.7708 ppl=15.97 lr=3.00e-04\n",
      "[epoch 1/3] step=5050 batch=5050/7650 loss=2.1394 avg=2.7695 ppl=15.95 lr=3.00e-04\n",
      "[epoch 1/3] step=5060 batch=5060/7650 loss=1.9830 avg=2.7681 ppl=15.93 lr=3.00e-04\n",
      "[epoch 1/3] step=5070 batch=5070/7650 loss=1.8214 avg=2.7666 ppl=15.90 lr=3.00e-04\n",
      "[epoch 1/3] step=5080 batch=5080/7650 loss=2.1072 avg=2.7652 ppl=15.88 lr=3.00e-04\n",
      "[epoch 1/3] step=5090 batch=5090/7650 loss=2.0848 avg=2.7638 ppl=15.86 lr=3.00e-04\n",
      "[epoch 1/3] step=5100 batch=5100/7650 loss=1.7857 avg=2.7624 ppl=15.84 lr=3.00e-04\n",
      "[epoch 1/3] step=5110 batch=5110/7650 loss=1.9206 avg=2.7610 ppl=15.82 lr=3.00e-04\n",
      "[epoch 1/3] step=5120 batch=5120/7650 loss=1.9806 avg=2.7596 ppl=15.79 lr=3.00e-04\n",
      "[epoch 1/3] step=5130 batch=5130/7650 loss=2.0388 avg=2.7582 ppl=15.77 lr=3.00e-04\n",
      "[epoch 1/3] step=5140 batch=5140/7650 loss=2.0659 avg=2.7569 ppl=15.75 lr=3.00e-04\n",
      "[epoch 1/3] step=5150 batch=5150/7650 loss=1.9323 avg=2.7556 ppl=15.73 lr=3.00e-04\n",
      "[epoch 1/3] step=5160 batch=5160/7650 loss=2.0334 avg=2.7542 ppl=15.71 lr=3.00e-04\n",
      "[epoch 1/3] step=5170 batch=5170/7650 loss=1.9696 avg=2.7529 ppl=15.69 lr=3.00e-04\n",
      "[epoch 1/3] step=5180 batch=5180/7650 loss=2.0010 avg=2.7516 ppl=15.67 lr=3.00e-04\n",
      "[epoch 1/3] step=5190 batch=5190/7650 loss=2.0715 avg=2.7502 ppl=15.65 lr=3.00e-04\n",
      "[epoch 1/3] step=5200 batch=5200/7650 loss=2.1621 avg=2.7489 ppl=15.63 lr=3.00e-04\n",
      "[epoch 1/3] step=5210 batch=5210/7650 loss=1.9218 avg=2.7476 ppl=15.60 lr=3.00e-04\n",
      "[epoch 1/3] step=5220 batch=5220/7650 loss=2.0439 avg=2.7462 ppl=15.58 lr=3.00e-04\n",
      "[epoch 1/3] step=5230 batch=5230/7650 loss=1.9494 avg=2.7449 ppl=15.56 lr=3.00e-04\n",
      "[epoch 1/3] step=5240 batch=5240/7650 loss=1.8475 avg=2.7435 ppl=15.54 lr=3.00e-04\n",
      "[epoch 1/3] step=5250 batch=5250/7650 loss=1.9034 avg=2.7421 ppl=15.52 lr=3.00e-04\n",
      "[epoch 1/3] step=5260 batch=5260/7650 loss=1.9410 avg=2.7408 ppl=15.50 lr=3.00e-04\n",
      "[epoch 1/3] step=5270 batch=5270/7650 loss=2.2304 avg=2.7396 ppl=15.48 lr=3.00e-04\n",
      "[epoch 1/3] step=5280 batch=5280/7650 loss=1.9422 avg=2.7381 ppl=15.46 lr=3.00e-04\n",
      "[epoch 1/3] step=5290 batch=5290/7650 loss=2.0159 avg=2.7369 ppl=15.44 lr=3.00e-04\n",
      "[epoch 1/3] step=5300 batch=5300/7650 loss=2.0832 avg=2.7356 ppl=15.42 lr=3.00e-04\n",
      "[epoch 1/3] step=5310 batch=5310/7650 loss=1.8693 avg=2.7343 ppl=15.40 lr=3.00e-04\n",
      "[epoch 1/3] step=5320 batch=5320/7650 loss=1.9671 avg=2.7330 ppl=15.38 lr=3.00e-04\n",
      "[epoch 1/3] step=5330 batch=5330/7650 loss=1.9385 avg=2.7316 ppl=15.36 lr=3.00e-04\n",
      "[epoch 1/3] step=5340 batch=5340/7650 loss=2.1242 avg=2.7302 ppl=15.34 lr=3.00e-04\n",
      "[epoch 1/3] step=5350 batch=5350/7650 loss=2.1388 avg=2.7289 ppl=15.32 lr=3.00e-04\n",
      "[epoch 1/3] step=5360 batch=5360/7650 loss=1.9936 avg=2.7276 ppl=15.30 lr=3.00e-04\n",
      "[epoch 1/3] step=5370 batch=5370/7650 loss=2.0136 avg=2.7263 ppl=15.28 lr=3.00e-04\n",
      "[epoch 1/3] step=5380 batch=5380/7650 loss=2.0579 avg=2.7250 ppl=15.26 lr=3.00e-04\n",
      "[epoch 1/3] step=5390 batch=5390/7650 loss=2.1493 avg=2.7237 ppl=15.24 lr=3.00e-04\n",
      "[epoch 1/3] step=5400 batch=5400/7650 loss=1.9939 avg=2.7224 ppl=15.22 lr=3.00e-04\n",
      "[epoch 1/3] step=5410 batch=5410/7650 loss=1.9447 avg=2.7211 ppl=15.20 lr=3.00e-04\n",
      "[epoch 1/3] step=5420 batch=5420/7650 loss=2.0273 avg=2.7198 ppl=15.18 lr=3.00e-04\n",
      "[epoch 1/3] step=5430 batch=5430/7650 loss=1.9563 avg=2.7186 ppl=15.16 lr=3.00e-04\n",
      "[epoch 1/3] step=5440 batch=5440/7650 loss=2.0184 avg=2.7173 ppl=15.14 lr=3.00e-04\n",
      "[epoch 1/3] step=5450 batch=5450/7650 loss=2.0767 avg=2.7160 ppl=15.12 lr=3.00e-04\n",
      "[epoch 1/3] step=5460 batch=5460/7650 loss=2.0556 avg=2.7147 ppl=15.10 lr=3.00e-04\n",
      "[epoch 1/3] step=5470 batch=5470/7650 loss=2.1581 avg=2.7134 ppl=15.08 lr=3.00e-04\n",
      "[epoch 1/3] step=5480 batch=5480/7650 loss=2.1128 avg=2.7122 ppl=15.06 lr=3.00e-04\n",
      "[epoch 1/3] step=5490 batch=5490/7650 loss=2.0853 avg=2.7110 ppl=15.04 lr=3.00e-04\n",
      "[epoch 1/3] step=5500 batch=5500/7650 loss=1.8636 avg=2.7097 ppl=15.03 lr=3.00e-04\n",
      "[epoch 1/3] step=5510 batch=5510/7650 loss=2.0464 avg=2.7084 ppl=15.01 lr=3.00e-04\n",
      "[epoch 1/3] step=5520 batch=5520/7650 loss=1.9263 avg=2.7071 ppl=14.99 lr=3.00e-04\n",
      "[epoch 1/3] step=5530 batch=5530/7650 loss=2.1896 avg=2.7059 ppl=14.97 lr=3.00e-04\n",
      "[epoch 1/3] step=5540 batch=5540/7650 loss=1.9170 avg=2.7047 ppl=14.95 lr=3.00e-04\n",
      "[epoch 1/3] step=5550 batch=5550/7650 loss=1.7795 avg=2.7035 ppl=14.93 lr=3.00e-04\n",
      "[epoch 1/3] step=5560 batch=5560/7650 loss=2.1829 avg=2.7022 ppl=14.91 lr=3.00e-04\n",
      "[epoch 1/3] step=5570 batch=5570/7650 loss=1.7401 avg=2.7008 ppl=14.89 lr=3.00e-04\n",
      "[epoch 1/3] step=5580 batch=5580/7650 loss=1.9860 avg=2.6995 ppl=14.87 lr=3.00e-04\n",
      "[epoch 1/3] step=5590 batch=5590/7650 loss=1.8346 avg=2.6983 ppl=14.85 lr=3.00e-04\n",
      "[epoch 1/3] step=5600 batch=5600/7650 loss=1.9098 avg=2.6971 ppl=14.84 lr=3.00e-04\n",
      "[epoch 1/3] step=5610 batch=5610/7650 loss=1.7469 avg=2.6957 ppl=14.82 lr=3.00e-04\n",
      "[epoch 1/3] step=5620 batch=5620/7650 loss=1.9041 avg=2.6944 ppl=14.80 lr=3.00e-04\n",
      "[epoch 1/3] step=5630 batch=5630/7650 loss=1.7748 avg=2.6931 ppl=14.78 lr=3.00e-04\n",
      "[epoch 1/3] step=5640 batch=5640/7650 loss=1.9563 avg=2.6918 ppl=14.76 lr=3.00e-04\n",
      "[epoch 1/3] step=5650 batch=5650/7650 loss=2.0503 avg=2.6907 ppl=14.74 lr=3.00e-04\n",
      "[epoch 1/3] step=5660 batch=5660/7650 loss=2.0354 avg=2.6895 ppl=14.72 lr=3.00e-04\n",
      "[epoch 1/3] step=5670 batch=5670/7650 loss=2.0242 avg=2.6881 ppl=14.70 lr=3.00e-04\n",
      "[epoch 1/3] step=5680 batch=5680/7650 loss=1.9765 avg=2.6869 ppl=14.69 lr=3.00e-04\n",
      "[epoch 1/3] step=5690 batch=5690/7650 loss=1.9518 avg=2.6857 ppl=14.67 lr=3.00e-04\n",
      "[epoch 1/3] step=5700 batch=5700/7650 loss=2.1892 avg=2.6845 ppl=14.65 lr=3.00e-04\n",
      "[epoch 1/3] step=5710 batch=5710/7650 loss=2.0994 avg=2.6833 ppl=14.63 lr=3.00e-04\n",
      "[epoch 1/3] step=5720 batch=5720/7650 loss=2.0842 avg=2.6822 ppl=14.62 lr=3.00e-04\n",
      "[epoch 1/3] step=5730 batch=5730/7650 loss=1.9054 avg=2.6811 ppl=14.60 lr=3.00e-04\n",
      "[epoch 1/3] step=5740 batch=5740/7650 loss=2.0052 avg=2.6799 ppl=14.58 lr=3.00e-04\n",
      "[epoch 1/3] step=5750 batch=5750/7650 loss=1.8292 avg=2.6786 ppl=14.56 lr=3.00e-04\n",
      "[epoch 1/3] step=5760 batch=5760/7650 loss=1.8762 avg=2.6774 ppl=14.55 lr=3.00e-04\n",
      "[epoch 1/3] step=5770 batch=5770/7650 loss=1.5863 avg=2.6761 ppl=14.53 lr=3.00e-04\n",
      "[epoch 1/3] step=5780 batch=5780/7650 loss=1.9309 avg=2.6749 ppl=14.51 lr=3.00e-04\n",
      "[epoch 1/3] step=5790 batch=5790/7650 loss=2.0331 avg=2.6738 ppl=14.49 lr=3.00e-04\n",
      "[epoch 1/3] step=5800 batch=5800/7650 loss=1.9995 avg=2.6727 ppl=14.48 lr=3.00e-04\n",
      "[epoch 1/3] step=5810 batch=5810/7650 loss=1.8808 avg=2.6715 ppl=14.46 lr=3.00e-04\n",
      "[epoch 1/3] step=5820 batch=5820/7650 loss=2.0787 avg=2.6703 ppl=14.44 lr=3.00e-04\n",
      "[epoch 1/3] step=5830 batch=5830/7650 loss=2.0186 avg=2.6691 ppl=14.43 lr=3.00e-04\n",
      "[epoch 1/3] step=5840 batch=5840/7650 loss=1.9618 avg=2.6678 ppl=14.41 lr=3.00e-04\n",
      "[epoch 1/3] step=5850 batch=5850/7650 loss=1.8359 avg=2.6666 ppl=14.39 lr=3.00e-04\n",
      "[epoch 1/3] step=5860 batch=5860/7650 loss=1.9338 avg=2.6654 ppl=14.37 lr=3.00e-04\n",
      "[epoch 1/3] step=5870 batch=5870/7650 loss=1.9474 avg=2.6642 ppl=14.36 lr=3.00e-04\n",
      "[epoch 1/3] step=5880 batch=5880/7650 loss=2.1679 avg=2.6631 ppl=14.34 lr=3.00e-04\n",
      "[epoch 1/3] step=5890 batch=5890/7650 loss=1.9392 avg=2.6619 ppl=14.32 lr=3.00e-04\n",
      "[epoch 1/3] step=5900 batch=5900/7650 loss=2.2284 avg=2.6607 ppl=14.31 lr=3.00e-04\n",
      "[epoch 1/3] step=5910 batch=5910/7650 loss=2.0190 avg=2.6597 ppl=14.29 lr=3.00e-04\n",
      "[epoch 1/3] step=5920 batch=5920/7650 loss=1.9040 avg=2.6584 ppl=14.27 lr=3.00e-04\n",
      "[epoch 1/3] step=5930 batch=5930/7650 loss=2.0078 avg=2.6573 ppl=14.26 lr=3.00e-04\n",
      "[epoch 1/3] step=5940 batch=5940/7650 loss=2.0604 avg=2.6561 ppl=14.24 lr=3.00e-04\n",
      "[epoch 1/3] step=5950 batch=5950/7650 loss=2.0038 avg=2.6550 ppl=14.22 lr=3.00e-04\n",
      "[epoch 1/3] step=5960 batch=5960/7650 loss=1.8557 avg=2.6538 ppl=14.21 lr=3.00e-04\n",
      "[epoch 1/3] step=5970 batch=5970/7650 loss=1.6964 avg=2.6526 ppl=14.19 lr=3.00e-04\n",
      "[epoch 1/3] step=5980 batch=5980/7650 loss=2.3403 avg=2.6516 ppl=14.18 lr=3.00e-04\n",
      "[epoch 1/3] step=5990 batch=5990/7650 loss=2.1447 avg=2.6504 ppl=14.16 lr=3.00e-04\n",
      "[epoch 1/3] step=6000 batch=6000/7650 loss=1.7174 avg=2.6492 ppl=14.14 lr=3.00e-04\n",
      "[epoch 1/3] step=6010 batch=6010/7650 loss=1.8555 avg=2.6480 ppl=14.13 lr=3.00e-04\n",
      "[epoch 1/3] step=6020 batch=6020/7650 loss=2.1998 avg=2.6468 ppl=14.11 lr=3.00e-04\n",
      "[epoch 1/3] step=6030 batch=6030/7650 loss=1.8683 avg=2.6457 ppl=14.09 lr=3.00e-04\n",
      "[epoch 1/3] step=6040 batch=6040/7650 loss=2.1632 avg=2.6446 ppl=14.08 lr=3.00e-04\n",
      "[epoch 1/3] step=6050 batch=6050/7650 loss=1.8684 avg=2.6434 ppl=14.06 lr=3.00e-04\n",
      "[epoch 1/3] step=6060 batch=6060/7650 loss=1.6962 avg=2.6423 ppl=14.05 lr=3.00e-04\n",
      "[epoch 1/3] step=6070 batch=6070/7650 loss=1.9014 avg=2.6412 ppl=14.03 lr=3.00e-04\n",
      "[epoch 1/3] step=6080 batch=6080/7650 loss=2.0679 avg=2.6402 ppl=14.02 lr=3.00e-04\n",
      "[epoch 1/3] step=6090 batch=6090/7650 loss=1.7903 avg=2.6391 ppl=14.00 lr=3.00e-04\n",
      "[epoch 1/3] step=6100 batch=6100/7650 loss=1.9745 avg=2.6380 ppl=13.99 lr=3.00e-04\n",
      "[epoch 1/3] step=6110 batch=6110/7650 loss=2.0985 avg=2.6370 ppl=13.97 lr=3.00e-04\n",
      "[epoch 1/3] step=6120 batch=6120/7650 loss=1.8508 avg=2.6358 ppl=13.95 lr=3.00e-04\n",
      "[epoch 1/3] step=6130 batch=6130/7650 loss=1.9904 avg=2.6347 ppl=13.94 lr=3.00e-04\n",
      "[epoch 1/3] step=6140 batch=6140/7650 loss=1.9974 avg=2.6337 ppl=13.93 lr=3.00e-04\n",
      "[epoch 1/3] step=6150 batch=6150/7650 loss=2.0850 avg=2.6326 ppl=13.91 lr=3.00e-04\n",
      "[epoch 1/3] step=6160 batch=6160/7650 loss=1.9220 avg=2.6315 ppl=13.90 lr=3.00e-04\n",
      "[epoch 1/3] step=6170 batch=6170/7650 loss=2.0061 avg=2.6304 ppl=13.88 lr=3.00e-04\n",
      "[epoch 1/3] step=6180 batch=6180/7650 loss=2.0295 avg=2.6292 ppl=13.86 lr=3.00e-04\n",
      "[epoch 1/3] step=6190 batch=6190/7650 loss=2.0792 avg=2.6281 ppl=13.85 lr=3.00e-04\n",
      "[epoch 1/3] step=6200 batch=6200/7650 loss=2.0871 avg=2.6272 ppl=13.83 lr=3.00e-04\n",
      "[epoch 1/3] step=6210 batch=6210/7650 loss=1.9463 avg=2.6260 ppl=13.82 lr=3.00e-04\n",
      "[epoch 1/3] step=6220 batch=6220/7650 loss=1.8431 avg=2.6249 ppl=13.80 lr=3.00e-04\n",
      "[epoch 1/3] step=6230 batch=6230/7650 loss=1.8442 avg=2.6237 ppl=13.79 lr=3.00e-04\n",
      "[epoch 1/3] step=6240 batch=6240/7650 loss=2.1013 avg=2.6226 ppl=13.77 lr=3.00e-04\n",
      "[epoch 1/3] step=6250 batch=6250/7650 loss=1.9026 avg=2.6216 ppl=13.76 lr=3.00e-04\n",
      "[epoch 1/3] step=6260 batch=6260/7650 loss=2.1345 avg=2.6206 ppl=13.74 lr=3.00e-04\n",
      "[epoch 1/3] step=6270 batch=6270/7650 loss=2.0288 avg=2.6196 ppl=13.73 lr=3.00e-04\n",
      "[epoch 1/3] step=6280 batch=6280/7650 loss=1.8088 avg=2.6184 ppl=13.71 lr=3.00e-04\n",
      "[epoch 1/3] step=6290 batch=6290/7650 loss=1.9709 avg=2.6173 ppl=13.70 lr=3.00e-04\n",
      "[epoch 1/3] step=6300 batch=6300/7650 loss=1.8946 avg=2.6162 ppl=13.68 lr=3.00e-04\n",
      "[epoch 1/3] step=6310 batch=6310/7650 loss=2.0208 avg=2.6151 ppl=13.67 lr=3.00e-04\n",
      "[epoch 1/3] step=6320 batch=6320/7650 loss=1.8040 avg=2.6140 ppl=13.65 lr=3.00e-04\n",
      "[epoch 1/3] step=6330 batch=6330/7650 loss=1.8520 avg=2.6129 ppl=13.64 lr=3.00e-04\n",
      "[epoch 1/3] step=6340 batch=6340/7650 loss=1.8705 avg=2.6118 ppl=13.62 lr=3.00e-04\n",
      "[epoch 1/3] step=6350 batch=6350/7650 loss=1.8025 avg=2.6107 ppl=13.61 lr=3.00e-04\n",
      "[epoch 1/3] step=6360 batch=6360/7650 loss=1.9712 avg=2.6096 ppl=13.59 lr=3.00e-04\n",
      "[epoch 1/3] step=6370 batch=6370/7650 loss=1.8257 avg=2.6084 ppl=13.58 lr=3.00e-04\n",
      "[epoch 1/3] step=6380 batch=6380/7650 loss=1.9872 avg=2.6074 ppl=13.56 lr=3.00e-04\n",
      "[epoch 1/3] step=6390 batch=6390/7650 loss=2.0634 avg=2.6064 ppl=13.55 lr=3.00e-04\n",
      "[epoch 1/3] step=6400 batch=6400/7650 loss=1.9951 avg=2.6053 ppl=13.54 lr=3.00e-04\n",
      "[epoch 1/3] step=6410 batch=6410/7650 loss=1.6676 avg=2.6043 ppl=13.52 lr=3.00e-04\n",
      "[epoch 1/3] step=6420 batch=6420/7650 loss=1.7906 avg=2.6032 ppl=13.51 lr=3.00e-04\n",
      "[epoch 1/3] step=6430 batch=6430/7650 loss=1.8402 avg=2.6021 ppl=13.49 lr=3.00e-04\n",
      "[epoch 1/3] step=6440 batch=6440/7650 loss=2.0441 avg=2.6012 ppl=13.48 lr=3.00e-04\n",
      "[epoch 1/3] step=6450 batch=6450/7650 loss=1.6731 avg=2.6001 ppl=13.46 lr=3.00e-04\n",
      "[epoch 1/3] step=6460 batch=6460/7650 loss=2.0179 avg=2.5992 ppl=13.45 lr=3.00e-04\n",
      "[epoch 1/3] step=6470 batch=6470/7650 loss=1.7624 avg=2.5980 ppl=13.44 lr=3.00e-04\n",
      "[epoch 1/3] step=6480 batch=6480/7650 loss=1.7679 avg=2.5969 ppl=13.42 lr=3.00e-04\n",
      "[epoch 1/3] step=6490 batch=6490/7650 loss=1.9182 avg=2.5958 ppl=13.41 lr=3.00e-04\n",
      "[epoch 1/3] step=6500 batch=6500/7650 loss=1.9461 avg=2.5948 ppl=13.39 lr=3.00e-04\n",
      "[epoch 1/3] step=6510 batch=6510/7650 loss=1.8470 avg=2.5939 ppl=13.38 lr=3.00e-04\n",
      "[epoch 1/3] step=6520 batch=6520/7650 loss=1.9181 avg=2.5929 ppl=13.37 lr=3.00e-04\n",
      "[epoch 1/3] step=6530 batch=6530/7650 loss=1.9837 avg=2.5919 ppl=13.36 lr=3.00e-04\n",
      "[epoch 1/3] step=6540 batch=6540/7650 loss=2.1300 avg=2.5909 ppl=13.34 lr=3.00e-04\n",
      "[epoch 1/3] step=6550 batch=6550/7650 loss=1.9848 avg=2.5898 ppl=13.33 lr=3.00e-04\n",
      "[epoch 1/3] step=6560 batch=6560/7650 loss=1.8306 avg=2.5888 ppl=13.31 lr=3.00e-04\n",
      "[epoch 1/3] step=6570 batch=6570/7650 loss=1.9171 avg=2.5877 ppl=13.30 lr=3.00e-04\n",
      "[epoch 1/3] step=6580 batch=6580/7650 loss=1.9021 avg=2.5866 ppl=13.28 lr=3.00e-04\n",
      "[epoch 1/3] step=6590 batch=6590/7650 loss=1.9814 avg=2.5855 ppl=13.27 lr=3.00e-04\n",
      "[epoch 1/3] step=6600 batch=6600/7650 loss=2.0296 avg=2.5845 ppl=13.26 lr=3.00e-04\n",
      "[epoch 1/3] step=6610 batch=6610/7650 loss=1.9514 avg=2.5834 ppl=13.24 lr=3.00e-04\n",
      "[epoch 1/3] step=6620 batch=6620/7650 loss=2.0690 avg=2.5824 ppl=13.23 lr=3.00e-04\n",
      "[epoch 1/3] step=6630 batch=6630/7650 loss=2.0323 avg=2.5814 ppl=13.22 lr=3.00e-04\n",
      "[epoch 1/3] step=6640 batch=6640/7650 loss=1.8078 avg=2.5804 ppl=13.20 lr=3.00e-04\n",
      "[epoch 1/3] step=6650 batch=6650/7650 loss=1.9416 avg=2.5794 ppl=13.19 lr=3.00e-04\n",
      "[epoch 1/3] step=6660 batch=6660/7650 loss=2.1761 avg=2.5785 ppl=13.18 lr=3.00e-04\n",
      "[epoch 1/3] step=6670 batch=6670/7650 loss=1.8268 avg=2.5774 ppl=13.16 lr=3.00e-04\n",
      "[epoch 1/3] step=6680 batch=6680/7650 loss=1.8637 avg=2.5765 ppl=13.15 lr=3.00e-04\n",
      "[epoch 1/3] step=6690 batch=6690/7650 loss=1.9080 avg=2.5755 ppl=13.14 lr=3.00e-04\n",
      "[epoch 1/3] step=6700 batch=6700/7650 loss=1.9694 avg=2.5745 ppl=13.13 lr=3.00e-04\n",
      "[epoch 1/3] step=6710 batch=6710/7650 loss=2.1264 avg=2.5736 ppl=13.11 lr=3.00e-04\n",
      "[epoch 1/3] step=6720 batch=6720/7650 loss=1.8059 avg=2.5726 ppl=13.10 lr=3.00e-04\n",
      "[epoch 1/3] step=6730 batch=6730/7650 loss=1.8193 avg=2.5717 ppl=13.09 lr=3.00e-04\n",
      "[epoch 1/3] step=6740 batch=6740/7650 loss=1.8452 avg=2.5707 ppl=13.07 lr=3.00e-04\n",
      "[epoch 1/3] step=6750 batch=6750/7650 loss=1.7456 avg=2.5697 ppl=13.06 lr=3.00e-04\n",
      "[epoch 1/3] step=6760 batch=6760/7650 loss=2.1054 avg=2.5687 ppl=13.05 lr=3.00e-04\n",
      "[epoch 1/3] step=6770 batch=6770/7650 loss=1.9312 avg=2.5678 ppl=13.04 lr=3.00e-04\n",
      "[epoch 1/3] step=6780 batch=6780/7650 loss=1.8468 avg=2.5667 ppl=13.02 lr=3.00e-04\n",
      "[epoch 1/3] step=6790 batch=6790/7650 loss=1.7940 avg=2.5658 ppl=13.01 lr=3.00e-04\n",
      "[epoch 1/3] step=6800 batch=6800/7650 loss=1.9385 avg=2.5648 ppl=13.00 lr=3.00e-04\n",
      "[epoch 1/3] step=6810 batch=6810/7650 loss=2.0112 avg=2.5638 ppl=12.98 lr=3.00e-04\n",
      "[epoch 1/3] step=6820 batch=6820/7650 loss=1.9152 avg=2.5628 ppl=12.97 lr=3.00e-04\n",
      "[epoch 1/3] step=6830 batch=6830/7650 loss=1.8190 avg=2.5618 ppl=12.96 lr=3.00e-04\n",
      "[epoch 1/3] step=6840 batch=6840/7650 loss=1.8115 avg=2.5609 ppl=12.95 lr=3.00e-04\n",
      "[epoch 1/3] step=6850 batch=6850/7650 loss=2.0297 avg=2.5599 ppl=12.94 lr=3.00e-04\n",
      "[epoch 1/3] step=6860 batch=6860/7650 loss=1.8304 avg=2.5590 ppl=12.92 lr=3.00e-04\n",
      "[epoch 1/3] step=6870 batch=6870/7650 loss=1.7187 avg=2.5581 ppl=12.91 lr=3.00e-04\n",
      "[epoch 1/3] step=6880 batch=6880/7650 loss=2.0895 avg=2.5572 ppl=12.90 lr=3.00e-04\n",
      "[epoch 1/3] step=6890 batch=6890/7650 loss=1.9927 avg=2.5563 ppl=12.89 lr=3.00e-04\n",
      "[epoch 1/3] step=6900 batch=6900/7650 loss=1.8117 avg=2.5553 ppl=12.88 lr=3.00e-04\n",
      "[epoch 1/3] step=6910 batch=6910/7650 loss=1.6942 avg=2.5543 ppl=12.86 lr=3.00e-04\n",
      "[epoch 1/3] step=6920 batch=6920/7650 loss=2.0646 avg=2.5534 ppl=12.85 lr=3.00e-04\n",
      "[epoch 1/3] step=6930 batch=6930/7650 loss=2.0161 avg=2.5525 ppl=12.84 lr=3.00e-04\n",
      "[epoch 1/3] step=6940 batch=6940/7650 loss=1.9650 avg=2.5516 ppl=12.83 lr=3.00e-04\n",
      "[epoch 1/3] step=6950 batch=6950/7650 loss=2.0274 avg=2.5507 ppl=12.82 lr=3.00e-04\n",
      "[epoch 1/3] step=6960 batch=6960/7650 loss=1.8050 avg=2.5498 ppl=12.80 lr=3.00e-04\n",
      "[epoch 1/3] step=6970 batch=6970/7650 loss=1.9514 avg=2.5489 ppl=12.79 lr=3.00e-04\n",
      "[epoch 1/3] step=6980 batch=6980/7650 loss=1.8625 avg=2.5479 ppl=12.78 lr=3.00e-04\n",
      "[epoch 1/3] step=6990 batch=6990/7650 loss=1.7059 avg=2.5471 ppl=12.77 lr=3.00e-04\n",
      "[epoch 1/3] step=7000 batch=7000/7650 loss=1.8880 avg=2.5461 ppl=12.76 lr=3.00e-04\n",
      "[epoch 1/3] step=7010 batch=7010/7650 loss=1.7454 avg=2.5452 ppl=12.75 lr=3.00e-04\n",
      "[epoch 1/3] step=7020 batch=7020/7650 loss=1.8522 avg=2.5443 ppl=12.73 lr=3.00e-04\n",
      "[epoch 1/3] step=7030 batch=7030/7650 loss=1.7897 avg=2.5434 ppl=12.72 lr=3.00e-04\n",
      "[epoch 1/3] step=7040 batch=7040/7650 loss=1.8124 avg=2.5425 ppl=12.71 lr=3.00e-04\n",
      "[epoch 1/3] step=7050 batch=7050/7650 loss=1.9529 avg=2.5415 ppl=12.70 lr=3.00e-04\n",
      "[epoch 1/3] step=7060 batch=7060/7650 loss=1.8166 avg=2.5406 ppl=12.69 lr=3.00e-04\n",
      "[epoch 1/3] step=7070 batch=7070/7650 loss=1.8960 avg=2.5396 ppl=12.68 lr=3.00e-04\n",
      "[epoch 1/3] step=7080 batch=7080/7650 loss=1.9801 avg=2.5387 ppl=12.66 lr=3.00e-04\n",
      "[epoch 1/3] step=7090 batch=7090/7650 loss=1.8695 avg=2.5378 ppl=12.65 lr=3.00e-04\n",
      "[epoch 1/3] step=7100 batch=7100/7650 loss=2.0729 avg=2.5369 ppl=12.64 lr=3.00e-04\n",
      "[epoch 1/3] step=7110 batch=7110/7650 loss=1.9362 avg=2.5360 ppl=12.63 lr=3.00e-04\n",
      "[epoch 1/3] step=7120 batch=7120/7650 loss=1.7257 avg=2.5350 ppl=12.62 lr=3.00e-04\n",
      "[epoch 1/3] step=7130 batch=7130/7650 loss=1.8388 avg=2.5340 ppl=12.60 lr=3.00e-04\n",
      "[epoch 1/3] step=7140 batch=7140/7650 loss=1.8229 avg=2.5331 ppl=12.59 lr=3.00e-04\n",
      "[epoch 1/3] step=7150 batch=7150/7650 loss=1.8676 avg=2.5323 ppl=12.58 lr=3.00e-04\n",
      "[epoch 1/3] step=7160 batch=7160/7650 loss=1.8774 avg=2.5313 ppl=12.57 lr=3.00e-04\n",
      "[epoch 1/3] step=7170 batch=7170/7650 loss=1.8116 avg=2.5305 ppl=12.56 lr=3.00e-04\n",
      "[epoch 1/3] step=7180 batch=7180/7650 loss=1.8055 avg=2.5296 ppl=12.55 lr=3.00e-04\n",
      "[epoch 1/3] step=7190 batch=7190/7650 loss=1.7180 avg=2.5286 ppl=12.54 lr=3.00e-04\n",
      "[epoch 1/3] step=7200 batch=7200/7650 loss=1.8868 avg=2.5277 ppl=12.52 lr=3.00e-04\n",
      "[epoch 1/3] step=7210 batch=7210/7650 loss=1.8811 avg=2.5267 ppl=12.51 lr=3.00e-04\n",
      "[epoch 1/3] step=7220 batch=7220/7650 loss=1.8450 avg=2.5259 ppl=12.50 lr=3.00e-04\n",
      "[epoch 1/3] step=7230 batch=7230/7650 loss=1.7933 avg=2.5250 ppl=12.49 lr=3.00e-04\n",
      "[epoch 1/3] step=7240 batch=7240/7650 loss=1.7805 avg=2.5240 ppl=12.48 lr=3.00e-04\n",
      "[epoch 1/3] step=7250 batch=7250/7650 loss=1.6615 avg=2.5231 ppl=12.47 lr=3.00e-04\n",
      "[epoch 1/3] step=7260 batch=7260/7650 loss=2.0397 avg=2.5222 ppl=12.46 lr=3.00e-04\n",
      "[epoch 1/3] step=7270 batch=7270/7650 loss=2.0226 avg=2.5213 ppl=12.45 lr=3.00e-04\n",
      "[epoch 1/3] step=7280 batch=7280/7650 loss=1.7626 avg=2.5204 ppl=12.43 lr=3.00e-04\n",
      "[epoch 1/3] step=7290 batch=7290/7650 loss=1.8760 avg=2.5196 ppl=12.42 lr=3.00e-04\n",
      "[epoch 1/3] step=7300 batch=7300/7650 loss=1.7877 avg=2.5187 ppl=12.41 lr=3.00e-04\n",
      "[epoch 1/3] step=7310 batch=7310/7650 loss=1.8224 avg=2.5178 ppl=12.40 lr=3.00e-04\n",
      "[epoch 1/3] step=7320 batch=7320/7650 loss=1.9907 avg=2.5170 ppl=12.39 lr=3.00e-04\n",
      "[epoch 1/3] step=7330 batch=7330/7650 loss=1.8541 avg=2.5161 ppl=12.38 lr=3.00e-04\n",
      "[epoch 1/3] step=7340 batch=7340/7650 loss=1.9124 avg=2.5153 ppl=12.37 lr=3.00e-04\n",
      "[epoch 1/3] step=7350 batch=7350/7650 loss=2.0127 avg=2.5145 ppl=12.36 lr=3.00e-04\n",
      "[epoch 1/3] step=7360 batch=7360/7650 loss=1.6881 avg=2.5136 ppl=12.35 lr=3.00e-04\n",
      "[epoch 1/3] step=7370 batch=7370/7650 loss=1.7724 avg=2.5127 ppl=12.34 lr=3.00e-04\n",
      "[epoch 1/3] step=7380 batch=7380/7650 loss=1.9804 avg=2.5118 ppl=12.33 lr=3.00e-04\n",
      "[epoch 1/3] step=7390 batch=7390/7650 loss=2.0202 avg=2.5108 ppl=12.31 lr=3.00e-04\n",
      "[epoch 1/3] step=7400 batch=7400/7650 loss=1.9628 avg=2.5099 ppl=12.30 lr=3.00e-04\n",
      "[epoch 1/3] step=7410 batch=7410/7650 loss=1.8409 avg=2.5092 ppl=12.29 lr=3.00e-04\n",
      "[epoch 1/3] step=7420 batch=7420/7650 loss=1.9077 avg=2.5083 ppl=12.28 lr=3.00e-04\n",
      "[epoch 1/3] step=7430 batch=7430/7650 loss=1.7872 avg=2.5075 ppl=12.27 lr=3.00e-04\n",
      "[epoch 1/3] step=7440 batch=7440/7650 loss=1.7103 avg=2.5066 ppl=12.26 lr=3.00e-04\n",
      "[epoch 1/3] step=7450 batch=7450/7650 loss=1.9797 avg=2.5057 ppl=12.25 lr=3.00e-04\n",
      "[epoch 1/3] step=7460 batch=7460/7650 loss=2.0129 avg=2.5049 ppl=12.24 lr=3.00e-04\n",
      "[epoch 1/3] step=7470 batch=7470/7650 loss=1.6979 avg=2.5041 ppl=12.23 lr=3.00e-04\n",
      "[epoch 1/3] step=7480 batch=7480/7650 loss=1.5945 avg=2.5031 ppl=12.22 lr=3.00e-04\n",
      "[epoch 1/3] step=7490 batch=7490/7650 loss=1.8809 avg=2.5023 ppl=12.21 lr=3.00e-04\n",
      "[epoch 1/3] step=7500 batch=7500/7650 loss=1.7449 avg=2.5013 ppl=12.20 lr=3.00e-04\n",
      "[epoch 1/3] step=7510 batch=7510/7650 loss=1.7785 avg=2.5005 ppl=12.19 lr=3.00e-04\n",
      "[epoch 1/3] step=7520 batch=7520/7650 loss=1.9616 avg=2.4996 ppl=12.18 lr=3.00e-04\n",
      "[epoch 1/3] step=7530 batch=7530/7650 loss=1.8170 avg=2.4988 ppl=12.17 lr=3.00e-04\n",
      "[epoch 1/3] step=7540 batch=7540/7650 loss=1.8458 avg=2.4979 ppl=12.16 lr=3.00e-04\n",
      "[epoch 1/3] step=7550 batch=7550/7650 loss=1.7518 avg=2.4969 ppl=12.15 lr=3.00e-04\n",
      "[epoch 1/3] step=7560 batch=7560/7650 loss=1.8646 avg=2.4961 ppl=12.14 lr=3.00e-04\n",
      "[epoch 1/3] step=7570 batch=7570/7650 loss=1.8357 avg=2.4952 ppl=12.12 lr=3.00e-04\n",
      "[epoch 1/3] step=7580 batch=7580/7650 loss=1.6663 avg=2.4944 ppl=12.11 lr=3.00e-04\n",
      "[epoch 1/3] step=7590 batch=7590/7650 loss=1.8867 avg=2.4935 ppl=12.10 lr=3.00e-04\n",
      "[epoch 1/3] step=7600 batch=7600/7650 loss=1.8225 avg=2.4926 ppl=12.09 lr=3.00e-04\n",
      "[epoch 1/3] step=7610 batch=7610/7650 loss=1.7020 avg=2.4918 ppl=12.08 lr=3.00e-04\n",
      "[epoch 1/3] step=7620 batch=7620/7650 loss=1.8025 avg=2.4910 ppl=12.07 lr=3.00e-04\n",
      "[epoch 1/3] step=7630 batch=7630/7650 loss=1.9337 avg=2.4902 ppl=12.06 lr=3.00e-04\n",
      "[epoch 1/3] step=7640 batch=7640/7650 loss=1.7460 avg=2.4892 ppl=12.05 lr=3.00e-04\n",
      "[epoch 1/3] step=7650 batch=7650/7650 loss=1.7936 avg=2.4884 ppl=12.04 lr=3.00e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "864fc286c483497293d19062b486bcef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev 1/3:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dev epoch 1: first batch ok\n",
      "\n",
      "Epoch 1 DONE | train_loss=2.4884 ppl=12.04 | dev_loss=1.6929 ppl=5.44\n",
      "\n",
      "✅ saved best: /kaggle/working/vlsp_finetune/best_finetune.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10ae6e8df1bb4f3d990cd0e2e54a64c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 2/3:   0%|          | 0/7650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train epoch 2: first batch ok\n",
      "[epoch 2/3] step=7660 batch=10/7650 loss=1.7755 avg=1.8079 ppl=6.10 lr=3.00e-04\n",
      "[epoch 2/3] step=7670 batch=20/7650 loss=1.8677 avg=1.8112 ppl=6.12 lr=3.00e-04\n",
      "[epoch 2/3] step=7680 batch=30/7650 loss=1.8308 avg=1.8108 ppl=6.12 lr=3.00e-04\n",
      "[epoch 2/3] step=7690 batch=40/7650 loss=1.9466 avg=1.8014 ppl=6.06 lr=3.00e-04\n",
      "[epoch 2/3] step=7700 batch=50/7650 loss=1.7230 avg=1.8039 ppl=6.07 lr=3.00e-04\n",
      "[epoch 2/3] step=7710 batch=60/7650 loss=1.6065 avg=1.8056 ppl=6.08 lr=3.00e-04\n",
      "[epoch 2/3] step=7720 batch=70/7650 loss=1.6489 avg=1.7936 ppl=6.01 lr=3.00e-04\n",
      "[epoch 2/3] step=7730 batch=80/7650 loss=1.5639 avg=1.7943 ppl=6.02 lr=3.00e-04\n",
      "[epoch 2/3] step=7740 batch=90/7650 loss=1.5829 avg=1.7937 ppl=6.01 lr=3.00e-04\n",
      "[epoch 2/3] step=7750 batch=100/7650 loss=1.6540 avg=1.7902 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=7760 batch=110/7650 loss=1.7773 avg=1.7918 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7770 batch=120/7650 loss=1.8820 avg=1.7974 ppl=6.03 lr=3.00e-04\n",
      "[epoch 2/3] step=7780 batch=130/7650 loss=1.7113 avg=1.7967 ppl=6.03 lr=3.00e-04\n",
      "[epoch 2/3] step=7790 batch=140/7650 loss=1.7114 avg=1.7959 ppl=6.02 lr=3.00e-04\n",
      "[epoch 2/3] step=7800 batch=150/7650 loss=1.7349 avg=1.7926 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7810 batch=160/7650 loss=1.9121 avg=1.7927 ppl=6.01 lr=3.00e-04\n",
      "[epoch 2/3] step=7820 batch=170/7650 loss=1.7862 avg=1.7893 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=7830 batch=180/7650 loss=1.8628 avg=1.7876 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=7840 batch=190/7650 loss=1.7848 avg=1.7863 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=7850 batch=200/7650 loss=1.8644 avg=1.7873 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=7860 batch=210/7650 loss=1.8835 avg=1.7856 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=7870 batch=220/7650 loss=1.6953 avg=1.7849 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=7880 batch=230/7650 loss=1.8320 avg=1.7865 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=7890 batch=240/7650 loss=1.7990 avg=1.7881 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=7900 batch=250/7650 loss=1.7335 avg=1.7864 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=7910 batch=260/7650 loss=1.8702 avg=1.7884 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=7920 batch=270/7650 loss=1.7715 avg=1.7914 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7930 batch=280/7650 loss=1.6740 avg=1.7908 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=7940 batch=290/7650 loss=1.7445 avg=1.7914 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7950 batch=300/7650 loss=1.7328 avg=1.7914 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7960 batch=310/7650 loss=1.9722 avg=1.7916 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=7970 batch=320/7650 loss=1.7754 avg=1.7900 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=7980 batch=330/7650 loss=1.8734 avg=1.7884 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=7990 batch=340/7650 loss=1.8760 avg=1.7889 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8000 batch=350/7650 loss=1.8651 avg=1.7901 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8010 batch=360/7650 loss=1.6123 avg=1.7894 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8020 batch=370/7650 loss=1.7588 avg=1.7900 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8030 batch=380/7650 loss=1.8581 avg=1.7901 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8040 batch=390/7650 loss=1.7633 avg=1.7887 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8050 batch=400/7650 loss=1.6969 avg=1.7876 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8060 batch=410/7650 loss=1.8247 avg=1.7875 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8070 batch=420/7650 loss=1.9130 avg=1.7880 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8080 batch=430/7650 loss=1.9082 avg=1.7875 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8090 batch=440/7650 loss=1.8586 avg=1.7887 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8100 batch=450/7650 loss=1.7339 avg=1.7884 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8110 batch=460/7650 loss=1.6451 avg=1.7885 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8120 batch=470/7650 loss=1.6834 avg=1.7870 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8130 batch=480/7650 loss=1.9563 avg=1.7868 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8140 batch=490/7650 loss=1.7640 avg=1.7874 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8150 batch=500/7650 loss=1.7673 avg=1.7871 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8160 batch=510/7650 loss=1.9141 avg=1.7884 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8170 batch=520/7650 loss=1.8822 avg=1.7894 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8180 batch=530/7650 loss=1.8294 avg=1.7890 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8190 batch=540/7650 loss=1.7417 avg=1.7880 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8200 batch=550/7650 loss=1.9775 avg=1.7888 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8210 batch=560/7650 loss=1.7305 avg=1.7896 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8220 batch=570/7650 loss=1.7611 avg=1.7902 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8230 batch=580/7650 loss=2.0312 avg=1.7906 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8240 batch=590/7650 loss=1.9100 avg=1.7912 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=8250 batch=600/7650 loss=1.7419 avg=1.7915 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=8260 batch=610/7650 loss=1.6256 avg=1.7905 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8270 batch=620/7650 loss=1.7716 avg=1.7901 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8280 batch=630/7650 loss=2.0283 avg=1.7910 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=8290 batch=640/7650 loss=1.8399 avg=1.7903 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8300 batch=650/7650 loss=1.8779 avg=1.7906 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8310 batch=660/7650 loss=1.8658 avg=1.7908 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8320 batch=670/7650 loss=1.8139 avg=1.7911 ppl=6.00 lr=3.00e-04\n",
      "[epoch 2/3] step=8330 batch=680/7650 loss=1.8169 avg=1.7904 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8340 batch=690/7650 loss=1.7545 avg=1.7896 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8350 batch=700/7650 loss=1.8513 avg=1.7898 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8360 batch=710/7650 loss=1.7942 avg=1.7892 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8370 batch=720/7650 loss=1.7499 avg=1.7895 ppl=5.99 lr=3.00e-04\n",
      "[epoch 2/3] step=8380 batch=730/7650 loss=1.6439 avg=1.7886 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8390 batch=740/7650 loss=1.5259 avg=1.7891 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8400 batch=750/7650 loss=1.7751 avg=1.7885 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8410 batch=760/7650 loss=1.7969 avg=1.7882 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8420 batch=770/7650 loss=2.0060 avg=1.7885 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8430 batch=780/7650 loss=1.8204 avg=1.7885 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8440 batch=790/7650 loss=1.8415 avg=1.7882 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8450 batch=800/7650 loss=1.7193 avg=1.7879 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8460 batch=810/7650 loss=1.7638 avg=1.7874 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8470 batch=820/7650 loss=1.8769 avg=1.7877 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8480 batch=830/7650 loss=1.6442 avg=1.7879 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8490 batch=840/7650 loss=1.9314 avg=1.7883 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8500 batch=850/7650 loss=1.6508 avg=1.7876 ppl=5.98 lr=3.00e-04\n",
      "[epoch 2/3] step=8510 batch=860/7650 loss=1.8792 avg=1.7874 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8520 batch=870/7650 loss=1.6477 avg=1.7860 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8530 batch=880/7650 loss=1.7537 avg=1.7862 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8540 batch=890/7650 loss=1.7204 avg=1.7865 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8550 batch=900/7650 loss=1.6436 avg=1.7863 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8560 batch=910/7650 loss=1.7320 avg=1.7860 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8570 batch=920/7650 loss=1.5543 avg=1.7860 ppl=5.97 lr=3.00e-04\n",
      "[epoch 2/3] step=8580 batch=930/7650 loss=1.8832 avg=1.7859 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=8590 batch=940/7650 loss=1.5144 avg=1.7851 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=8600 batch=950/7650 loss=1.8169 avg=1.7848 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=8610 batch=960/7650 loss=1.5366 avg=1.7843 ppl=5.96 lr=3.00e-04\n",
      "[epoch 2/3] step=8620 batch=970/7650 loss=1.8655 avg=1.7841 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8630 batch=980/7650 loss=1.8003 avg=1.7840 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8640 batch=990/7650 loss=1.8775 avg=1.7838 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8650 batch=1000/7650 loss=1.7863 avg=1.7838 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8660 batch=1010/7650 loss=1.6470 avg=1.7837 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8670 batch=1020/7650 loss=1.9132 avg=1.7833 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8680 batch=1030/7650 loss=1.7442 avg=1.7833 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8690 batch=1040/7650 loss=1.6985 avg=1.7832 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8700 batch=1050/7650 loss=1.7176 avg=1.7828 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8710 batch=1060/7650 loss=1.7944 avg=1.7828 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8720 batch=1070/7650 loss=1.9493 avg=1.7832 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8730 batch=1080/7650 loss=1.7390 avg=1.7831 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8740 batch=1090/7650 loss=1.7706 avg=1.7831 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8750 batch=1100/7650 loss=1.5708 avg=1.7831 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8760 batch=1110/7650 loss=1.9415 avg=1.7827 ppl=5.95 lr=3.00e-04\n",
      "[epoch 2/3] step=8770 batch=1120/7650 loss=1.6156 avg=1.7824 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8780 batch=1130/7650 loss=1.7175 avg=1.7823 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8790 batch=1140/7650 loss=1.7427 avg=1.7824 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8800 batch=1150/7650 loss=1.8565 avg=1.7822 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8810 batch=1160/7650 loss=1.7677 avg=1.7816 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8820 batch=1170/7650 loss=1.8497 avg=1.7811 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8830 batch=1180/7650 loss=1.7488 avg=1.7810 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8840 batch=1190/7650 loss=1.6692 avg=1.7810 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8850 batch=1200/7650 loss=1.6509 avg=1.7808 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8860 batch=1210/7650 loss=1.7236 avg=1.7807 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8870 batch=1220/7650 loss=1.7397 avg=1.7804 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8880 batch=1230/7650 loss=1.6941 avg=1.7807 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8890 batch=1240/7650 loss=1.7122 avg=1.7805 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8900 batch=1250/7650 loss=1.6110 avg=1.7802 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8910 batch=1260/7650 loss=1.7574 avg=1.7803 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8920 batch=1270/7650 loss=1.9486 avg=1.7809 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8930 batch=1280/7650 loss=1.7426 avg=1.7809 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8940 batch=1290/7650 loss=1.7557 avg=1.7805 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8950 batch=1300/7650 loss=1.8706 avg=1.7808 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8960 batch=1310/7650 loss=1.8003 avg=1.7810 ppl=5.94 lr=3.00e-04\n",
      "[epoch 2/3] step=8970 batch=1320/7650 loss=1.6430 avg=1.7804 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8980 batch=1330/7650 loss=2.0576 avg=1.7807 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=8990 batch=1340/7650 loss=1.9413 avg=1.7808 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9000 batch=1350/7650 loss=1.7632 avg=1.7808 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9010 batch=1360/7650 loss=1.6478 avg=1.7803 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9020 batch=1370/7650 loss=1.6667 avg=1.7802 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9030 batch=1380/7650 loss=1.8435 avg=1.7798 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9040 batch=1390/7650 loss=1.6674 avg=1.7799 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9050 batch=1400/7650 loss=1.6849 avg=1.7798 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9060 batch=1410/7650 loss=1.7862 avg=1.7799 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9070 batch=1420/7650 loss=1.8309 avg=1.7794 ppl=5.93 lr=3.00e-04\n",
      "[epoch 2/3] step=9080 batch=1430/7650 loss=1.7205 avg=1.7790 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9090 batch=1440/7650 loss=1.8741 avg=1.7791 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9100 batch=1450/7650 loss=1.6747 avg=1.7789 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9110 batch=1460/7650 loss=1.5484 avg=1.7784 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9120 batch=1470/7650 loss=1.6514 avg=1.7780 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9130 batch=1480/7650 loss=1.6455 avg=1.7775 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9140 batch=1490/7650 loss=1.6631 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9150 batch=1500/7650 loss=1.5714 avg=1.7767 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9160 batch=1510/7650 loss=1.8738 avg=1.7766 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9170 batch=1520/7650 loss=1.7737 avg=1.7768 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9180 batch=1530/7650 loss=2.0046 avg=1.7771 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9190 batch=1540/7650 loss=1.8296 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9200 batch=1550/7650 loss=1.6737 avg=1.7767 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9210 batch=1560/7650 loss=1.9089 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9220 batch=1570/7650 loss=1.8592 avg=1.7771 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9230 batch=1580/7650 loss=1.8118 avg=1.7773 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9240 batch=1590/7650 loss=1.8274 avg=1.7776 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9250 batch=1600/7650 loss=1.8283 avg=1.7778 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9260 batch=1610/7650 loss=1.6833 avg=1.7779 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9270 batch=1620/7650 loss=1.8370 avg=1.7779 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9280 batch=1630/7650 loss=1.8536 avg=1.7780 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9290 batch=1640/7650 loss=1.8248 avg=1.7777 ppl=5.92 lr=3.00e-04\n",
      "[epoch 2/3] step=9300 batch=1650/7650 loss=1.5447 avg=1.7774 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9310 batch=1660/7650 loss=2.0057 avg=1.7774 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9320 batch=1670/7650 loss=2.0113 avg=1.7772 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9330 batch=1680/7650 loss=1.8892 avg=1.7766 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9340 batch=1690/7650 loss=1.7869 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9350 batch=1700/7650 loss=1.7706 avg=1.7773 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9360 batch=1710/7650 loss=1.6596 avg=1.7772 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9370 batch=1720/7650 loss=1.6581 avg=1.7772 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9380 batch=1730/7650 loss=1.7225 avg=1.7769 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9390 batch=1740/7650 loss=1.8065 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9400 batch=1750/7650 loss=1.7239 avg=1.7770 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9410 batch=1760/7650 loss=1.5047 avg=1.7768 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9420 batch=1770/7650 loss=1.8475 avg=1.7768 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9430 batch=1780/7650 loss=1.9329 avg=1.7769 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9440 batch=1790/7650 loss=1.6049 avg=1.7766 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9450 batch=1800/7650 loss=1.8215 avg=1.7763 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9460 batch=1810/7650 loss=1.8411 avg=1.7763 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9470 batch=1820/7650 loss=1.7843 avg=1.7761 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9480 batch=1830/7650 loss=1.5270 avg=1.7761 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9490 batch=1840/7650 loss=1.7703 avg=1.7759 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9500 batch=1850/7650 loss=1.7511 avg=1.7759 ppl=5.91 lr=3.00e-04\n",
      "[epoch 2/3] step=9510 batch=1860/7650 loss=1.6812 avg=1.7754 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9520 batch=1870/7650 loss=1.7214 avg=1.7752 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9530 batch=1880/7650 loss=1.7806 avg=1.7750 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9540 batch=1890/7650 loss=1.7558 avg=1.7746 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9550 batch=1900/7650 loss=1.8353 avg=1.7745 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9560 batch=1910/7650 loss=1.9979 avg=1.7745 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9570 batch=1920/7650 loss=1.7641 avg=1.7744 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9580 batch=1930/7650 loss=1.6286 avg=1.7744 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9590 batch=1940/7650 loss=1.6899 avg=1.7742 ppl=5.90 lr=3.00e-04\n",
      "[epoch 2/3] step=9600 batch=1950/7650 loss=1.5922 avg=1.7739 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9610 batch=1960/7650 loss=1.6020 avg=1.7738 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9620 batch=1970/7650 loss=1.7402 avg=1.7737 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9630 batch=1980/7650 loss=1.9532 avg=1.7739 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9640 batch=1990/7650 loss=1.7808 avg=1.7738 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9650 batch=2000/7650 loss=1.6054 avg=1.7735 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9660 batch=2010/7650 loss=1.4595 avg=1.7732 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9670 batch=2020/7650 loss=1.7650 avg=1.7731 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9680 batch=2030/7650 loss=1.8390 avg=1.7727 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9690 batch=2040/7650 loss=1.7116 avg=1.7723 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9700 batch=2050/7650 loss=1.7155 avg=1.7725 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9710 batch=2060/7650 loss=1.7400 avg=1.7724 ppl=5.89 lr=3.00e-04\n",
      "[epoch 2/3] step=9720 batch=2070/7650 loss=1.7192 avg=1.7722 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9730 batch=2080/7650 loss=1.6964 avg=1.7719 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9740 batch=2090/7650 loss=1.8215 avg=1.7720 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9750 batch=2100/7650 loss=1.7312 avg=1.7719 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9760 batch=2110/7650 loss=1.7660 avg=1.7719 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9770 batch=2120/7650 loss=1.7368 avg=1.7718 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9780 batch=2130/7650 loss=1.9017 avg=1.7718 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9790 batch=2140/7650 loss=1.5761 avg=1.7717 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9800 batch=2150/7650 loss=1.7353 avg=1.7715 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9810 batch=2160/7650 loss=1.7759 avg=1.7713 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9820 batch=2170/7650 loss=1.6605 avg=1.7710 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9830 batch=2180/7650 loss=1.7233 avg=1.7709 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9840 batch=2190/7650 loss=1.7187 avg=1.7708 ppl=5.88 lr=3.00e-04\n",
      "[epoch 2/3] step=9850 batch=2200/7650 loss=1.8313 avg=1.7706 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9860 batch=2210/7650 loss=1.7994 avg=1.7706 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9870 batch=2220/7650 loss=1.6119 avg=1.7706 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9880 batch=2230/7650 loss=1.7450 avg=1.7704 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9890 batch=2240/7650 loss=1.8923 avg=1.7701 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9900 batch=2250/7650 loss=1.5931 avg=1.7700 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9910 batch=2260/7650 loss=1.7930 avg=1.7697 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9920 batch=2270/7650 loss=1.5901 avg=1.7695 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9930 batch=2280/7650 loss=1.6655 avg=1.7693 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9940 batch=2290/7650 loss=1.7160 avg=1.7691 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9950 batch=2300/7650 loss=2.0202 avg=1.7691 ppl=5.87 lr=3.00e-04\n",
      "[epoch 2/3] step=9960 batch=2310/7650 loss=1.7077 avg=1.7689 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=9970 batch=2320/7650 loss=1.8131 avg=1.7686 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=9980 batch=2330/7650 loss=1.8891 avg=1.7686 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=9990 batch=2340/7650 loss=1.6665 avg=1.7686 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10000 batch=2350/7650 loss=1.7076 avg=1.7684 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10010 batch=2360/7650 loss=1.6098 avg=1.7683 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10020 batch=2370/7650 loss=1.7870 avg=1.7684 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10030 batch=2380/7650 loss=1.5782 avg=1.7682 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10040 batch=2390/7650 loss=1.7512 avg=1.7684 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10050 batch=2400/7650 loss=1.5500 avg=1.7683 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10060 batch=2410/7650 loss=1.6463 avg=1.7681 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10070 batch=2420/7650 loss=1.8392 avg=1.7680 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10080 batch=2430/7650 loss=1.6597 avg=1.7678 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10090 batch=2440/7650 loss=1.6842 avg=1.7675 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10100 batch=2450/7650 loss=1.7327 avg=1.7674 ppl=5.86 lr=3.00e-04\n",
      "[epoch 2/3] step=10110 batch=2460/7650 loss=1.5726 avg=1.7670 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10120 batch=2470/7650 loss=1.6188 avg=1.7668 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10130 batch=2480/7650 loss=1.5337 avg=1.7666 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10140 batch=2490/7650 loss=1.7774 avg=1.7662 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10150 batch=2500/7650 loss=1.7750 avg=1.7662 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10160 batch=2510/7650 loss=1.7905 avg=1.7660 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10170 batch=2520/7650 loss=1.6344 avg=1.7657 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10180 batch=2530/7650 loss=1.9983 avg=1.7660 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10190 batch=2540/7650 loss=1.8005 avg=1.7660 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10200 batch=2550/7650 loss=1.8106 avg=1.7661 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10210 batch=2560/7650 loss=1.8434 avg=1.7658 ppl=5.85 lr=3.00e-04\n",
      "[epoch 2/3] step=10220 batch=2570/7650 loss=1.6090 avg=1.7656 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10230 batch=2580/7650 loss=2.0040 avg=1.7654 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10240 batch=2590/7650 loss=1.9050 avg=1.7655 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10250 batch=2600/7650 loss=1.8692 avg=1.7652 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10260 batch=2610/7650 loss=1.7611 avg=1.7653 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10270 batch=2620/7650 loss=1.5752 avg=1.7651 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10280 batch=2630/7650 loss=1.6840 avg=1.7647 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10290 batch=2640/7650 loss=1.7325 avg=1.7645 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10300 batch=2650/7650 loss=1.6556 avg=1.7643 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10310 batch=2660/7650 loss=1.6929 avg=1.7639 ppl=5.84 lr=3.00e-04\n",
      "[epoch 2/3] step=10320 batch=2670/7650 loss=1.5513 avg=1.7636 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10330 batch=2680/7650 loss=1.6879 avg=1.7634 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10340 batch=2690/7650 loss=1.6698 avg=1.7631 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10350 batch=2700/7650 loss=1.9178 avg=1.7631 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10360 batch=2710/7650 loss=1.7639 avg=1.7627 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10370 batch=2720/7650 loss=1.5957 avg=1.7626 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10380 batch=2730/7650 loss=1.9705 avg=1.7627 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10390 batch=2740/7650 loss=1.7983 avg=1.7626 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10400 batch=2750/7650 loss=1.6936 avg=1.7624 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10410 batch=2760/7650 loss=1.6244 avg=1.7622 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10420 batch=2770/7650 loss=1.8121 avg=1.7622 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10430 batch=2780/7650 loss=1.6859 avg=1.7622 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10440 batch=2790/7650 loss=1.9199 avg=1.7622 ppl=5.83 lr=3.00e-04\n",
      "[epoch 2/3] step=10450 batch=2800/7650 loss=1.7522 avg=1.7621 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10460 batch=2810/7650 loss=1.7286 avg=1.7618 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10470 batch=2820/7650 loss=1.6358 avg=1.7616 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10480 batch=2830/7650 loss=2.0456 avg=1.7614 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10490 batch=2840/7650 loss=1.5230 avg=1.7613 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10500 batch=2850/7650 loss=1.7874 avg=1.7613 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10510 batch=2860/7650 loss=1.5606 avg=1.7608 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10520 batch=2870/7650 loss=1.5633 avg=1.7606 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10530 batch=2880/7650 loss=1.6397 avg=1.7605 ppl=5.82 lr=3.00e-04\n",
      "[epoch 2/3] step=10540 batch=2890/7650 loss=1.7033 avg=1.7603 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10550 batch=2900/7650 loss=1.7920 avg=1.7601 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10560 batch=2910/7650 loss=1.6960 avg=1.7599 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10570 batch=2920/7650 loss=1.7071 avg=1.7594 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10580 batch=2930/7650 loss=1.8762 avg=1.7594 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10590 batch=2940/7650 loss=1.6671 avg=1.7592 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10600 batch=2950/7650 loss=1.7255 avg=1.7592 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10610 batch=2960/7650 loss=1.6111 avg=1.7591 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10620 batch=2970/7650 loss=1.7065 avg=1.7589 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10630 batch=2980/7650 loss=1.6692 avg=1.7588 ppl=5.81 lr=3.00e-04\n",
      "[epoch 2/3] step=10640 batch=2990/7650 loss=1.6253 avg=1.7586 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10650 batch=3000/7650 loss=1.8273 avg=1.7586 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10660 batch=3010/7650 loss=1.8206 avg=1.7585 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10670 batch=3020/7650 loss=1.7812 avg=1.7585 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10680 batch=3030/7650 loss=1.8272 avg=1.7583 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10690 batch=3040/7650 loss=1.7515 avg=1.7582 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10700 batch=3050/7650 loss=1.7701 avg=1.7578 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10710 batch=3060/7650 loss=1.7878 avg=1.7579 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10720 batch=3070/7650 loss=1.5987 avg=1.7578 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10730 batch=3080/7650 loss=1.7423 avg=1.7576 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10740 batch=3090/7650 loss=1.7387 avg=1.7575 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10750 batch=3100/7650 loss=1.9396 avg=1.7574 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10760 batch=3110/7650 loss=1.6539 avg=1.7572 ppl=5.80 lr=3.00e-04\n",
      "[epoch 2/3] step=10770 batch=3120/7650 loss=1.6639 avg=1.7568 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10780 batch=3130/7650 loss=1.6650 avg=1.7566 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10790 batch=3140/7650 loss=1.6630 avg=1.7563 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10800 batch=3150/7650 loss=1.7056 avg=1.7559 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10810 batch=3160/7650 loss=1.6189 avg=1.7559 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10820 batch=3170/7650 loss=1.5133 avg=1.7555 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10830 batch=3180/7650 loss=1.8910 avg=1.7555 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10840 batch=3190/7650 loss=1.8429 avg=1.7555 ppl=5.79 lr=3.00e-04\n",
      "[epoch 2/3] step=10850 batch=3200/7650 loss=1.8530 avg=1.7551 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10860 batch=3210/7650 loss=1.5437 avg=1.7548 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10870 batch=3220/7650 loss=1.5932 avg=1.7546 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10880 batch=3230/7650 loss=1.6188 avg=1.7543 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10890 batch=3240/7650 loss=1.7293 avg=1.7544 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10900 batch=3250/7650 loss=1.6769 avg=1.7541 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10910 batch=3260/7650 loss=1.7965 avg=1.7540 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10920 batch=3270/7650 loss=1.5996 avg=1.7538 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10930 batch=3280/7650 loss=1.8718 avg=1.7537 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10940 batch=3290/7650 loss=1.6925 avg=1.7537 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10950 batch=3300/7650 loss=1.9175 avg=1.7537 ppl=5.78 lr=3.00e-04\n",
      "[epoch 2/3] step=10960 batch=3310/7650 loss=1.6271 avg=1.7535 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=10970 batch=3320/7650 loss=1.6737 avg=1.7533 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=10980 batch=3330/7650 loss=1.7100 avg=1.7533 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=10990 batch=3340/7650 loss=1.7565 avg=1.7532 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11000 batch=3350/7650 loss=1.6409 avg=1.7531 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11010 batch=3360/7650 loss=1.5175 avg=1.7527 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11020 batch=3370/7650 loss=1.9269 avg=1.7526 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11030 batch=3380/7650 loss=1.5247 avg=1.7525 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11040 batch=3390/7650 loss=1.8041 avg=1.7522 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11050 batch=3400/7650 loss=1.8817 avg=1.7521 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11060 batch=3410/7650 loss=1.5248 avg=1.7519 ppl=5.77 lr=3.00e-04\n",
      "[epoch 2/3] step=11070 batch=3420/7650 loss=1.8658 avg=1.7517 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11080 batch=3430/7650 loss=1.8931 avg=1.7517 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11090 batch=3440/7650 loss=1.6781 avg=1.7516 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11100 batch=3450/7650 loss=1.6188 avg=1.7514 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11110 batch=3460/7650 loss=1.6657 avg=1.7512 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11120 batch=3470/7650 loss=1.7869 avg=1.7511 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11130 batch=3480/7650 loss=1.9702 avg=1.7510 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11140 batch=3490/7650 loss=1.6676 avg=1.7507 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11150 batch=3500/7650 loss=1.8084 avg=1.7507 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11160 batch=3510/7650 loss=1.6821 avg=1.7505 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11170 batch=3520/7650 loss=1.6741 avg=1.7503 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11180 batch=3530/7650 loss=1.7158 avg=1.7504 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11190 batch=3540/7650 loss=1.6223 avg=1.7502 ppl=5.76 lr=3.00e-04\n",
      "[epoch 2/3] step=11200 batch=3550/7650 loss=1.5319 avg=1.7499 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11210 batch=3560/7650 loss=1.7036 avg=1.7498 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11220 batch=3570/7650 loss=1.9910 avg=1.7497 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11230 batch=3580/7650 loss=1.5626 avg=1.7497 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11240 batch=3590/7650 loss=1.5028 avg=1.7496 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11250 batch=3600/7650 loss=1.7320 avg=1.7495 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11260 batch=3610/7650 loss=1.8392 avg=1.7495 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11270 batch=3620/7650 loss=1.7357 avg=1.7494 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11280 batch=3630/7650 loss=1.6621 avg=1.7493 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11290 batch=3640/7650 loss=1.8429 avg=1.7493 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11300 batch=3650/7650 loss=1.6363 avg=1.7492 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11310 batch=3660/7650 loss=1.7562 avg=1.7491 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11320 batch=3670/7650 loss=1.9314 avg=1.7488 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11330 batch=3680/7650 loss=1.6047 avg=1.7487 ppl=5.75 lr=3.00e-04\n",
      "[epoch 2/3] step=11340 batch=3690/7650 loss=1.4535 avg=1.7483 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11350 batch=3700/7650 loss=1.5776 avg=1.7482 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11360 batch=3710/7650 loss=1.5625 avg=1.7480 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11370 batch=3720/7650 loss=1.5964 avg=1.7478 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11380 batch=3730/7650 loss=1.5053 avg=1.7476 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11390 batch=3740/7650 loss=1.7451 avg=1.7477 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11400 batch=3750/7650 loss=1.8283 avg=1.7476 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11410 batch=3760/7650 loss=1.7022 avg=1.7476 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11420 batch=3770/7650 loss=1.6159 avg=1.7475 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11430 batch=3780/7650 loss=1.6708 avg=1.7474 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11440 batch=3790/7650 loss=1.8507 avg=1.7474 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11450 batch=3800/7650 loss=1.5888 avg=1.7473 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11460 batch=3810/7650 loss=1.6029 avg=1.7470 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11470 batch=3820/7650 loss=1.7728 avg=1.7468 ppl=5.74 lr=3.00e-04\n",
      "[epoch 2/3] step=11480 batch=3830/7650 loss=1.5210 avg=1.7464 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11490 batch=3840/7650 loss=1.6054 avg=1.7461 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11500 batch=3850/7650 loss=1.8845 avg=1.7459 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11510 batch=3860/7650 loss=1.5886 avg=1.7457 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11520 batch=3870/7650 loss=1.7235 avg=1.7454 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11530 batch=3880/7650 loss=1.6402 avg=1.7451 ppl=5.73 lr=3.00e-04\n",
      "[epoch 2/3] step=11540 batch=3890/7650 loss=1.6153 avg=1.7448 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11550 batch=3900/7650 loss=1.5411 avg=1.7446 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11560 batch=3910/7650 loss=1.5620 avg=1.7444 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11570 batch=3920/7650 loss=1.8386 avg=1.7442 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11580 batch=3930/7650 loss=1.6308 avg=1.7441 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11590 batch=3940/7650 loss=1.7177 avg=1.7441 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11600 batch=3950/7650 loss=1.5707 avg=1.7437 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11610 batch=3960/7650 loss=1.6390 avg=1.7436 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11620 batch=3970/7650 loss=1.6915 avg=1.7435 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11630 batch=3980/7650 loss=1.9012 avg=1.7434 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11640 batch=3990/7650 loss=1.7031 avg=1.7433 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11650 batch=4000/7650 loss=1.6985 avg=1.7432 ppl=5.72 lr=3.00e-04\n",
      "[epoch 2/3] step=11660 batch=4010/7650 loss=1.5817 avg=1.7430 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11670 batch=4020/7650 loss=1.5736 avg=1.7427 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11680 batch=4030/7650 loss=1.6996 avg=1.7425 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11690 batch=4040/7650 loss=1.6656 avg=1.7424 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11700 batch=4050/7650 loss=1.7675 avg=1.7423 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11710 batch=4060/7650 loss=1.6279 avg=1.7422 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11720 batch=4070/7650 loss=1.7709 avg=1.7423 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11730 batch=4080/7650 loss=1.5942 avg=1.7422 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11740 batch=4090/7650 loss=1.7587 avg=1.7420 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11750 batch=4100/7650 loss=1.6418 avg=1.7417 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11760 batch=4110/7650 loss=1.7715 avg=1.7416 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11770 batch=4120/7650 loss=1.7562 avg=1.7416 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11780 batch=4130/7650 loss=1.7518 avg=1.7416 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11790 batch=4140/7650 loss=1.8749 avg=1.7415 ppl=5.71 lr=3.00e-04\n",
      "[epoch 2/3] step=11800 batch=4150/7650 loss=1.5142 avg=1.7412 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11810 batch=4160/7650 loss=1.4829 avg=1.7410 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11820 batch=4170/7650 loss=1.7482 avg=1.7410 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11830 batch=4180/7650 loss=1.8361 avg=1.7407 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11840 batch=4190/7650 loss=1.6835 avg=1.7407 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11850 batch=4200/7650 loss=1.7210 avg=1.7407 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11860 batch=4210/7650 loss=1.8274 avg=1.7407 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11870 batch=4220/7650 loss=1.6454 avg=1.7406 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11880 batch=4230/7650 loss=1.6483 avg=1.7404 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11890 batch=4240/7650 loss=1.7993 avg=1.7402 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11900 batch=4250/7650 loss=1.6792 avg=1.7401 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11910 batch=4260/7650 loss=2.0049 avg=1.7400 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11920 batch=4270/7650 loss=1.7998 avg=1.7399 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11930 batch=4280/7650 loss=1.4400 avg=1.7397 ppl=5.70 lr=3.00e-04\n",
      "[epoch 2/3] step=11940 batch=4290/7650 loss=1.6121 avg=1.7395 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=11950 batch=4300/7650 loss=1.4661 avg=1.7394 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=11960 batch=4310/7650 loss=1.8602 avg=1.7394 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=11970 batch=4320/7650 loss=1.7583 avg=1.7392 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=11980 batch=4330/7650 loss=1.7862 avg=1.7392 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=11990 batch=4340/7650 loss=1.4582 avg=1.7390 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12000 batch=4350/7650 loss=1.7674 avg=1.7390 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12010 batch=4360/7650 loss=1.6712 avg=1.7389 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12020 batch=4370/7650 loss=1.7574 avg=1.7390 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12030 batch=4380/7650 loss=1.7461 avg=1.7387 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12040 batch=4390/7650 loss=1.6950 avg=1.7386 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12050 batch=4400/7650 loss=1.8205 avg=1.7386 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12060 batch=4410/7650 loss=1.8214 avg=1.7385 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12070 batch=4420/7650 loss=1.5951 avg=1.7384 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12080 batch=4430/7650 loss=1.7859 avg=1.7382 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12090 batch=4440/7650 loss=1.7647 avg=1.7381 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12100 batch=4450/7650 loss=1.8211 avg=1.7380 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12110 batch=4460/7650 loss=1.5292 avg=1.7379 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12120 batch=4470/7650 loss=1.7241 avg=1.7379 ppl=5.69 lr=3.00e-04\n",
      "[epoch 2/3] step=12130 batch=4480/7650 loss=1.4992 avg=1.7377 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12140 batch=4490/7650 loss=1.6986 avg=1.7376 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12150 batch=4500/7650 loss=1.5779 avg=1.7375 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12160 batch=4510/7650 loss=1.7014 avg=1.7373 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12170 batch=4520/7650 loss=1.6660 avg=1.7372 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12180 batch=4530/7650 loss=1.6882 avg=1.7371 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12190 batch=4540/7650 loss=1.5993 avg=1.7370 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12200 batch=4550/7650 loss=1.5361 avg=1.7370 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12210 batch=4560/7650 loss=1.6645 avg=1.7369 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12220 batch=4570/7650 loss=1.5914 avg=1.7367 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12230 batch=4580/7650 loss=1.4881 avg=1.7365 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12240 batch=4590/7650 loss=1.6525 avg=1.7364 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12250 batch=4600/7650 loss=1.3997 avg=1.7362 ppl=5.68 lr=3.00e-04\n",
      "[epoch 2/3] step=12260 batch=4610/7650 loss=1.5118 avg=1.7360 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12270 batch=4620/7650 loss=1.8109 avg=1.7359 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12280 batch=4630/7650 loss=1.7046 avg=1.7358 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12290 batch=4640/7650 loss=1.6148 avg=1.7356 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12300 batch=4650/7650 loss=1.6429 avg=1.7355 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12310 batch=4660/7650 loss=1.7014 avg=1.7354 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12320 batch=4670/7650 loss=1.9019 avg=1.7354 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12330 batch=4680/7650 loss=1.5155 avg=1.7353 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12340 batch=4690/7650 loss=1.6468 avg=1.7351 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12350 batch=4700/7650 loss=1.7976 avg=1.7350 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12360 batch=4710/7650 loss=1.7022 avg=1.7350 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12370 batch=4720/7650 loss=1.6949 avg=1.7349 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12380 batch=4730/7650 loss=1.4156 avg=1.7347 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12390 batch=4740/7650 loss=1.6446 avg=1.7347 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12400 batch=4750/7650 loss=1.7537 avg=1.7345 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12410 batch=4760/7650 loss=1.7865 avg=1.7344 ppl=5.67 lr=3.00e-04\n",
      "[epoch 2/3] step=12420 batch=4770/7650 loss=1.4760 avg=1.7342 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12430 batch=4780/7650 loss=1.7325 avg=1.7339 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12440 batch=4790/7650 loss=1.7582 avg=1.7339 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12450 batch=4800/7650 loss=1.7847 avg=1.7339 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12460 batch=4810/7650 loss=1.6896 avg=1.7336 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12470 batch=4820/7650 loss=1.7889 avg=1.7335 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12480 batch=4830/7650 loss=1.5918 avg=1.7333 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12490 batch=4840/7650 loss=1.6828 avg=1.7332 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12500 batch=4850/7650 loss=1.4674 avg=1.7331 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12510 batch=4860/7650 loss=1.6840 avg=1.7330 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12520 batch=4870/7650 loss=1.5128 avg=1.7327 ppl=5.66 lr=3.00e-04\n",
      "[epoch 2/3] step=12530 batch=4880/7650 loss=1.5376 avg=1.7325 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12540 batch=4890/7650 loss=1.6355 avg=1.7325 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12550 batch=4900/7650 loss=1.5368 avg=1.7324 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12560 batch=4910/7650 loss=1.7230 avg=1.7323 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12570 batch=4920/7650 loss=1.6679 avg=1.7322 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12580 batch=4930/7650 loss=1.6086 avg=1.7320 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12590 batch=4940/7650 loss=1.4963 avg=1.7317 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12600 batch=4950/7650 loss=1.6745 avg=1.7316 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12610 batch=4960/7650 loss=1.6662 avg=1.7316 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12620 batch=4970/7650 loss=1.5798 avg=1.7314 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12630 batch=4980/7650 loss=1.7561 avg=1.7313 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12640 batch=4990/7650 loss=1.6539 avg=1.7311 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12650 batch=5000/7650 loss=1.6519 avg=1.7310 ppl=5.65 lr=3.00e-04\n",
      "[epoch 2/3] step=12660 batch=5010/7650 loss=1.6700 avg=1.7308 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12670 batch=5020/7650 loss=1.4973 avg=1.7307 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12680 batch=5030/7650 loss=1.5395 avg=1.7305 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12690 batch=5040/7650 loss=1.8005 avg=1.7304 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12700 batch=5050/7650 loss=1.8670 avg=1.7303 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12710 batch=5060/7650 loss=1.6113 avg=1.7301 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12720 batch=5070/7650 loss=1.6448 avg=1.7300 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12730 batch=5080/7650 loss=1.6528 avg=1.7299 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12740 batch=5090/7650 loss=1.5687 avg=1.7296 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12750 batch=5100/7650 loss=1.7173 avg=1.7294 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12760 batch=5110/7650 loss=1.7377 avg=1.7292 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12770 batch=5120/7650 loss=1.6199 avg=1.7292 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12780 batch=5130/7650 loss=1.8420 avg=1.7292 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12790 batch=5140/7650 loss=1.8152 avg=1.7291 ppl=5.64 lr=3.00e-04\n",
      "[epoch 2/3] step=12800 batch=5150/7650 loss=1.6171 avg=1.7289 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12810 batch=5160/7650 loss=1.5060 avg=1.7288 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12820 batch=5170/7650 loss=1.5930 avg=1.7287 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12830 batch=5180/7650 loss=1.6678 avg=1.7286 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12840 batch=5190/7650 loss=1.4730 avg=1.7285 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12850 batch=5200/7650 loss=1.8272 avg=1.7285 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12860 batch=5210/7650 loss=1.4900 avg=1.7283 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12870 batch=5220/7650 loss=1.7987 avg=1.7282 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12880 batch=5230/7650 loss=1.6455 avg=1.7280 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12890 batch=5240/7650 loss=1.7271 avg=1.7279 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12900 batch=5250/7650 loss=1.8557 avg=1.7278 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12910 batch=5260/7650 loss=1.5860 avg=1.7277 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12920 batch=5270/7650 loss=1.4862 avg=1.7276 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12930 batch=5280/7650 loss=1.7678 avg=1.7275 ppl=5.63 lr=3.00e-04\n",
      "[epoch 2/3] step=12940 batch=5290/7650 loss=1.6121 avg=1.7272 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=12950 batch=5300/7650 loss=1.6949 avg=1.7271 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=12960 batch=5310/7650 loss=1.4910 avg=1.7270 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=12970 batch=5320/7650 loss=1.8200 avg=1.7269 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=12980 batch=5330/7650 loss=1.5515 avg=1.7266 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=12990 batch=5340/7650 loss=1.5857 avg=1.7265 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13000 batch=5350/7650 loss=1.5866 avg=1.7264 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13010 batch=5360/7650 loss=1.6819 avg=1.7262 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13020 batch=5370/7650 loss=1.7577 avg=1.7259 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13030 batch=5380/7650 loss=1.6221 avg=1.7258 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13040 batch=5390/7650 loss=1.6828 avg=1.7258 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13050 batch=5400/7650 loss=1.7242 avg=1.7257 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13060 batch=5410/7650 loss=1.7534 avg=1.7257 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13070 batch=5420/7650 loss=1.7357 avg=1.7256 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13080 batch=5430/7650 loss=1.5659 avg=1.7255 ppl=5.62 lr=3.00e-04\n",
      "[epoch 2/3] step=13090 batch=5440/7650 loss=1.6880 avg=1.7253 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13100 batch=5450/7650 loss=1.6664 avg=1.7251 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13110 batch=5460/7650 loss=1.5416 avg=1.7250 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13120 batch=5470/7650 loss=1.5139 avg=1.7249 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13130 batch=5480/7650 loss=1.5822 avg=1.7249 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13140 batch=5490/7650 loss=1.6215 avg=1.7248 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13150 batch=5500/7650 loss=1.6786 avg=1.7247 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13160 batch=5510/7650 loss=1.4952 avg=1.7246 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13170 batch=5520/7650 loss=1.5876 avg=1.7245 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13180 batch=5530/7650 loss=1.6483 avg=1.7244 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13190 batch=5540/7650 loss=1.6562 avg=1.7242 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13200 batch=5550/7650 loss=1.8348 avg=1.7240 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13210 batch=5560/7650 loss=1.7000 avg=1.7239 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13220 batch=5570/7650 loss=1.7069 avg=1.7238 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13230 batch=5580/7650 loss=1.7097 avg=1.7237 ppl=5.61 lr=3.00e-04\n",
      "[epoch 2/3] step=13240 batch=5590/7650 loss=1.6217 avg=1.7236 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13250 batch=5600/7650 loss=1.7841 avg=1.7235 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13260 batch=5610/7650 loss=1.6374 avg=1.7234 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13270 batch=5620/7650 loss=1.8700 avg=1.7233 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13280 batch=5630/7650 loss=1.7274 avg=1.7232 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13290 batch=5640/7650 loss=1.6921 avg=1.7231 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13300 batch=5650/7650 loss=1.5061 avg=1.7230 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13310 batch=5660/7650 loss=1.5861 avg=1.7229 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13320 batch=5670/7650 loss=1.4434 avg=1.7227 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13330 batch=5680/7650 loss=1.5677 avg=1.7225 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13340 batch=5690/7650 loss=1.5702 avg=1.7224 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13350 batch=5700/7650 loss=1.5166 avg=1.7223 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13360 batch=5710/7650 loss=1.7364 avg=1.7222 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13370 batch=5720/7650 loss=1.5069 avg=1.7221 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13380 batch=5730/7650 loss=1.7604 avg=1.7220 ppl=5.60 lr=3.00e-04\n",
      "[epoch 2/3] step=13390 batch=5740/7650 loss=1.5419 avg=1.7218 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13400 batch=5750/7650 loss=1.4050 avg=1.7216 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13410 batch=5760/7650 loss=1.6925 avg=1.7215 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13420 batch=5770/7650 loss=1.6652 avg=1.7214 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13430 batch=5780/7650 loss=1.6678 avg=1.7213 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13440 batch=5790/7650 loss=1.6165 avg=1.7212 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13450 batch=5800/7650 loss=1.7185 avg=1.7212 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13460 batch=5810/7650 loss=1.8911 avg=1.7211 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13470 batch=5820/7650 loss=1.7734 avg=1.7210 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13480 batch=5830/7650 loss=1.6042 avg=1.7209 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13490 batch=5840/7650 loss=1.8863 avg=1.7208 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13500 batch=5850/7650 loss=1.7556 avg=1.7207 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13510 batch=5860/7650 loss=1.5538 avg=1.7206 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13520 batch=5870/7650 loss=1.6029 avg=1.7204 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13530 batch=5880/7650 loss=1.6164 avg=1.7203 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13540 batch=5890/7650 loss=1.6053 avg=1.7203 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13550 batch=5900/7650 loss=1.4250 avg=1.7201 ppl=5.59 lr=3.00e-04\n",
      "[epoch 2/3] step=13560 batch=5910/7650 loss=1.5119 avg=1.7199 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13570 batch=5920/7650 loss=1.7758 avg=1.7197 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13580 batch=5930/7650 loss=1.5247 avg=1.7196 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13590 batch=5940/7650 loss=1.6761 avg=1.7195 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13600 batch=5950/7650 loss=1.6791 avg=1.7193 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13610 batch=5960/7650 loss=1.7111 avg=1.7191 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13620 batch=5970/7650 loss=1.5174 avg=1.7190 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13630 batch=5980/7650 loss=1.5464 avg=1.7188 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13640 batch=5990/7650 loss=1.7018 avg=1.7187 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13650 batch=6000/7650 loss=1.5983 avg=1.7186 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13660 batch=6010/7650 loss=1.6679 avg=1.7185 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13670 batch=6020/7650 loss=1.6513 avg=1.7183 ppl=5.58 lr=3.00e-04\n",
      "[epoch 2/3] step=13680 batch=6030/7650 loss=1.5627 avg=1.7181 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13690 batch=6040/7650 loss=1.5658 avg=1.7179 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13700 batch=6050/7650 loss=1.6104 avg=1.7177 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13710 batch=6060/7650 loss=1.6769 avg=1.7176 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13720 batch=6070/7650 loss=1.8347 avg=1.7175 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13730 batch=6080/7650 loss=1.6781 avg=1.7174 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13740 batch=6090/7650 loss=1.6883 avg=1.7173 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13750 batch=6100/7650 loss=1.8086 avg=1.7172 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13760 batch=6110/7650 loss=1.6217 avg=1.7170 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13770 batch=6120/7650 loss=1.8262 avg=1.7170 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13780 batch=6130/7650 loss=1.6143 avg=1.7169 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13790 batch=6140/7650 loss=1.8247 avg=1.7169 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13800 batch=6150/7650 loss=1.4288 avg=1.7167 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13810 batch=6160/7650 loss=1.5136 avg=1.7165 ppl=5.57 lr=3.00e-04\n",
      "[epoch 2/3] step=13820 batch=6170/7650 loss=1.7259 avg=1.7164 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13830 batch=6180/7650 loss=1.7083 avg=1.7163 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13840 batch=6190/7650 loss=1.6552 avg=1.7163 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13850 batch=6200/7650 loss=1.4483 avg=1.7163 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13860 batch=6210/7650 loss=1.5917 avg=1.7162 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13870 batch=6220/7650 loss=1.5355 avg=1.7161 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13880 batch=6230/7650 loss=1.6486 avg=1.7159 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13890 batch=6240/7650 loss=1.6096 avg=1.7157 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13900 batch=6250/7650 loss=1.7775 avg=1.7155 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13910 batch=6260/7650 loss=1.6308 avg=1.7155 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13920 batch=6270/7650 loss=1.4585 avg=1.7153 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13930 batch=6280/7650 loss=1.8023 avg=1.7152 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13940 batch=6290/7650 loss=1.8320 avg=1.7151 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13950 batch=6300/7650 loss=1.6067 avg=1.7149 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13960 batch=6310/7650 loss=1.5487 avg=1.7148 ppl=5.56 lr=3.00e-04\n",
      "[epoch 2/3] step=13970 batch=6320/7650 loss=1.5798 avg=1.7147 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=13980 batch=6330/7650 loss=1.6365 avg=1.7146 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=13990 batch=6340/7650 loss=1.8159 avg=1.7146 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14000 batch=6350/7650 loss=1.6314 avg=1.7146 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14010 batch=6360/7650 loss=1.5865 avg=1.7145 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14020 batch=6370/7650 loss=1.8726 avg=1.7144 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14030 batch=6380/7650 loss=1.8749 avg=1.7143 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14040 batch=6390/7650 loss=1.4558 avg=1.7142 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14050 batch=6400/7650 loss=1.5881 avg=1.7140 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14060 batch=6410/7650 loss=1.4832 avg=1.7138 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14070 batch=6420/7650 loss=1.5774 avg=1.7137 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14080 batch=6430/7650 loss=1.4987 avg=1.7135 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14090 batch=6440/7650 loss=1.6774 avg=1.7134 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14100 batch=6450/7650 loss=1.5225 avg=1.7133 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14110 batch=6460/7650 loss=1.6697 avg=1.7132 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14120 batch=6470/7650 loss=1.5242 avg=1.7130 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14130 batch=6480/7650 loss=1.6567 avg=1.7129 ppl=5.55 lr=3.00e-04\n",
      "[epoch 2/3] step=14140 batch=6490/7650 loss=1.4554 avg=1.7127 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14150 batch=6500/7650 loss=1.5974 avg=1.7125 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14160 batch=6510/7650 loss=1.6900 avg=1.7124 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14170 batch=6520/7650 loss=1.7233 avg=1.7124 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14180 batch=6530/7650 loss=1.8969 avg=1.7123 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14190 batch=6540/7650 loss=1.6201 avg=1.7122 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14200 batch=6550/7650 loss=1.4167 avg=1.7121 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14210 batch=6560/7650 loss=1.5355 avg=1.7119 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14220 batch=6570/7650 loss=1.6432 avg=1.7118 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14230 batch=6580/7650 loss=1.8042 avg=1.7117 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14240 batch=6590/7650 loss=1.5271 avg=1.7117 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14250 batch=6600/7650 loss=1.5946 avg=1.7116 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14260 batch=6610/7650 loss=1.7155 avg=1.7115 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14270 batch=6620/7650 loss=1.9340 avg=1.7115 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14280 batch=6630/7650 loss=1.5994 avg=1.7114 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14290 batch=6640/7650 loss=1.5853 avg=1.7113 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14300 batch=6650/7650 loss=1.5745 avg=1.7113 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14310 batch=6660/7650 loss=1.8232 avg=1.7112 ppl=5.54 lr=3.00e-04\n",
      "[epoch 2/3] step=14320 batch=6670/7650 loss=1.6956 avg=1.7110 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14330 batch=6680/7650 loss=1.9330 avg=1.7109 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14340 batch=6690/7650 loss=1.5666 avg=1.7108 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14350 batch=6700/7650 loss=1.6494 avg=1.7107 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14360 batch=6710/7650 loss=1.4792 avg=1.7105 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14370 batch=6720/7650 loss=1.4453 avg=1.7104 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14380 batch=6730/7650 loss=1.6606 avg=1.7103 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14390 batch=6740/7650 loss=1.7054 avg=1.7103 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14400 batch=6750/7650 loss=1.4967 avg=1.7102 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14410 batch=6760/7650 loss=1.6024 avg=1.7101 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14420 batch=6770/7650 loss=1.5422 avg=1.7100 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14430 batch=6780/7650 loss=1.7057 avg=1.7100 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14440 batch=6790/7650 loss=1.6107 avg=1.7098 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14450 batch=6800/7650 loss=1.4684 avg=1.7096 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14460 batch=6810/7650 loss=1.5892 avg=1.7095 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14470 batch=6820/7650 loss=1.5876 avg=1.7094 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14480 batch=6830/7650 loss=1.8440 avg=1.7093 ppl=5.53 lr=3.00e-04\n",
      "[epoch 2/3] step=14490 batch=6840/7650 loss=1.5843 avg=1.7091 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14500 batch=6850/7650 loss=1.5694 avg=1.7090 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14510 batch=6860/7650 loss=1.6063 avg=1.7089 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14520 batch=6870/7650 loss=1.6530 avg=1.7088 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14530 batch=6880/7650 loss=1.6383 avg=1.7087 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14540 batch=6890/7650 loss=1.5340 avg=1.7086 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14550 batch=6900/7650 loss=1.3421 avg=1.7085 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14560 batch=6910/7650 loss=1.4045 avg=1.7083 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14570 batch=6920/7650 loss=1.6583 avg=1.7082 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14580 batch=6930/7650 loss=1.7048 avg=1.7081 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14590 batch=6940/7650 loss=1.7508 avg=1.7080 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14600 batch=6950/7650 loss=1.8674 avg=1.7079 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14610 batch=6960/7650 loss=1.5772 avg=1.7078 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14620 batch=6970/7650 loss=1.4823 avg=1.7076 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14630 batch=6980/7650 loss=1.7172 avg=1.7076 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14640 batch=6990/7650 loss=1.4875 avg=1.7075 ppl=5.52 lr=3.00e-04\n",
      "[epoch 2/3] step=14650 batch=7000/7650 loss=1.5747 avg=1.7074 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14660 batch=7010/7650 loss=1.8128 avg=1.7074 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14670 batch=7020/7650 loss=1.5121 avg=1.7073 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14680 batch=7030/7650 loss=1.5867 avg=1.7072 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14690 batch=7040/7650 loss=1.5966 avg=1.7071 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14700 batch=7050/7650 loss=1.5972 avg=1.7069 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14710 batch=7060/7650 loss=1.5420 avg=1.7068 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14720 batch=7070/7650 loss=1.6255 avg=1.7068 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14730 batch=7080/7650 loss=1.6845 avg=1.7068 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14740 batch=7090/7650 loss=1.6782 avg=1.7066 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14750 batch=7100/7650 loss=1.8201 avg=1.7065 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14760 batch=7110/7650 loss=1.6497 avg=1.7064 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14770 batch=7120/7650 loss=1.6229 avg=1.7064 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14780 batch=7130/7650 loss=1.5282 avg=1.7063 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14790 batch=7140/7650 loss=1.5069 avg=1.7061 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14800 batch=7150/7650 loss=1.7087 avg=1.7060 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14810 batch=7160/7650 loss=1.7598 avg=1.7059 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14820 batch=7170/7650 loss=1.5956 avg=1.7057 ppl=5.51 lr=3.00e-04\n",
      "[epoch 2/3] step=14830 batch=7180/7650 loss=1.7819 avg=1.7057 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14840 batch=7190/7650 loss=1.6712 avg=1.7056 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14850 batch=7200/7650 loss=1.4897 avg=1.7055 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14860 batch=7210/7650 loss=1.6331 avg=1.7054 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14870 batch=7220/7650 loss=1.6075 avg=1.7053 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14880 batch=7230/7650 loss=1.8743 avg=1.7051 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14890 batch=7240/7650 loss=1.6526 avg=1.7050 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14900 batch=7250/7650 loss=1.6473 avg=1.7049 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14910 batch=7260/7650 loss=1.5172 avg=1.7048 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14920 batch=7270/7650 loss=1.5259 avg=1.7046 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14930 batch=7280/7650 loss=1.5480 avg=1.7045 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14940 batch=7290/7650 loss=1.6662 avg=1.7045 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14950 batch=7300/7650 loss=1.5662 avg=1.7043 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14960 batch=7310/7650 loss=1.6044 avg=1.7042 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14970 batch=7320/7650 loss=1.5194 avg=1.7041 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14980 batch=7330/7650 loss=1.5790 avg=1.7039 ppl=5.50 lr=3.00e-04\n",
      "[epoch 2/3] step=14990 batch=7340/7650 loss=1.4739 avg=1.7038 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15000 batch=7350/7650 loss=1.5455 avg=1.7038 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15010 batch=7360/7650 loss=1.6584 avg=1.7037 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15020 batch=7370/7650 loss=1.5261 avg=1.7035 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15030 batch=7380/7650 loss=1.6301 avg=1.7034 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15040 batch=7390/7650 loss=1.5947 avg=1.7033 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15050 batch=7400/7650 loss=1.5707 avg=1.7032 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15060 batch=7410/7650 loss=1.5537 avg=1.7030 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15070 batch=7420/7650 loss=1.6809 avg=1.7029 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15080 batch=7430/7650 loss=1.5972 avg=1.7028 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15090 batch=7440/7650 loss=1.6996 avg=1.7027 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15100 batch=7450/7650 loss=1.4916 avg=1.7026 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15110 batch=7460/7650 loss=1.5962 avg=1.7024 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15120 batch=7470/7650 loss=1.6634 avg=1.7023 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15130 batch=7480/7650 loss=1.8548 avg=1.7022 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15140 batch=7490/7650 loss=1.4862 avg=1.7021 ppl=5.49 lr=3.00e-04\n",
      "[epoch 2/3] step=15150 batch=7500/7650 loss=1.7356 avg=1.7020 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15160 batch=7510/7650 loss=1.6310 avg=1.7019 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15170 batch=7520/7650 loss=1.6715 avg=1.7018 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15180 batch=7530/7650 loss=1.5783 avg=1.7017 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15190 batch=7540/7650 loss=1.4824 avg=1.7014 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15200 batch=7550/7650 loss=1.4603 avg=1.7013 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15210 batch=7560/7650 loss=1.5940 avg=1.7012 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15220 batch=7570/7650 loss=1.5864 avg=1.7011 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15230 batch=7580/7650 loss=1.7025 avg=1.7009 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15240 batch=7590/7650 loss=1.4645 avg=1.7008 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15250 batch=7600/7650 loss=1.4152 avg=1.7006 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15260 batch=7610/7650 loss=1.7248 avg=1.7006 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15270 batch=7620/7650 loss=1.6856 avg=1.7004 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15280 batch=7630/7650 loss=1.8118 avg=1.7003 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15290 batch=7640/7650 loss=1.6697 avg=1.7002 ppl=5.48 lr=3.00e-04\n",
      "[epoch 2/3] step=15300 batch=7650/7650 loss=1.6332 avg=1.7001 ppl=5.47 lr=3.00e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba3fdc5d5d9444cdb184112327c1dd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev 2/3:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dev epoch 2: first batch ok\n",
      "\n",
      "Epoch 2 DONE | train_loss=1.7001 ppl=5.47 | dev_loss=1.4769 ppl=4.38\n",
      "\n",
      "✅ saved best: /kaggle/working/vlsp_finetune/best_finetune.pt\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0031601cce5470ea9a1f6c10bfa20b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 3/3:   0%|          | 0/7650 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ train epoch 3: first batch ok\n",
      "[epoch 3/3] step=15310 batch=10/7650 loss=1.5533 avg=1.5292 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=15320 batch=20/7650 loss=1.5537 avg=1.5291 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=15330 batch=30/7650 loss=1.4956 avg=1.5505 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=15340 batch=40/7650 loss=1.4473 avg=1.5393 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15350 batch=50/7650 loss=1.4484 avg=1.5363 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15360 batch=60/7650 loss=1.4628 avg=1.5322 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=15370 batch=70/7650 loss=1.6011 avg=1.5375 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15380 batch=80/7650 loss=1.3176 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15390 batch=90/7650 loss=1.3634 avg=1.5351 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=15400 batch=100/7650 loss=1.6328 avg=1.5365 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15410 batch=110/7650 loss=1.3519 avg=1.5358 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=15420 batch=120/7650 loss=1.5937 avg=1.5369 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15430 batch=130/7650 loss=1.4905 avg=1.5360 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=15440 batch=140/7650 loss=1.4370 avg=1.5344 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=15450 batch=150/7650 loss=1.6291 avg=1.5382 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15460 batch=160/7650 loss=1.6505 avg=1.5402 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15470 batch=170/7650 loss=1.3774 avg=1.5408 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15480 batch=180/7650 loss=1.4688 avg=1.5395 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15490 batch=190/7650 loss=1.4303 avg=1.5392 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15500 batch=200/7650 loss=1.6842 avg=1.5397 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15510 batch=210/7650 loss=1.5537 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15520 batch=220/7650 loss=1.3920 avg=1.5422 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15530 batch=230/7650 loss=1.5668 avg=1.5436 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=15540 batch=240/7650 loss=1.5970 avg=1.5448 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15550 batch=250/7650 loss=1.5016 avg=1.5445 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15560 batch=260/7650 loss=1.4503 avg=1.5421 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15570 batch=270/7650 loss=1.6727 avg=1.5420 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15580 batch=280/7650 loss=1.6235 avg=1.5402 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15590 batch=290/7650 loss=1.4838 avg=1.5397 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=15600 batch=300/7650 loss=1.5459 avg=1.5410 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=15610 batch=310/7650 loss=1.6352 avg=1.5423 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=15620 batch=320/7650 loss=1.6052 avg=1.5441 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=15630 batch=330/7650 loss=1.3180 avg=1.5440 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=15640 batch=340/7650 loss=1.7087 avg=1.5447 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15650 batch=350/7650 loss=1.5294 avg=1.5445 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15660 batch=360/7650 loss=1.7337 avg=1.5459 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15670 batch=370/7650 loss=1.4433 avg=1.5462 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15680 batch=380/7650 loss=1.5604 avg=1.5468 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=15690 batch=390/7650 loss=1.4350 avg=1.5461 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15700 batch=400/7650 loss=1.3646 avg=1.5456 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=15710 batch=410/7650 loss=1.6314 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=15720 batch=420/7650 loss=1.4667 avg=1.5471 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=15730 batch=430/7650 loss=1.5017 avg=1.5493 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=15740 batch=440/7650 loss=1.6279 avg=1.5502 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=15750 batch=450/7650 loss=1.6016 avg=1.5509 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15760 batch=460/7650 loss=1.5793 avg=1.5505 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=15770 batch=470/7650 loss=1.7058 avg=1.5504 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=15780 batch=480/7650 loss=1.6090 avg=1.5515 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15790 batch=490/7650 loss=1.4568 avg=1.5520 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15800 batch=500/7650 loss=1.5483 avg=1.5529 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15810 batch=510/7650 loss=1.4777 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15820 batch=520/7650 loss=1.5791 avg=1.5528 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15830 batch=530/7650 loss=1.5407 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15840 batch=540/7650 loss=1.5292 avg=1.5520 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15850 batch=550/7650 loss=1.6040 avg=1.5537 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15860 batch=560/7650 loss=1.4283 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15870 batch=570/7650 loss=1.6162 avg=1.5533 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15880 batch=580/7650 loss=1.5448 avg=1.5543 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15890 batch=590/7650 loss=1.6451 avg=1.5541 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15900 batch=600/7650 loss=1.4437 avg=1.5535 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15910 batch=610/7650 loss=1.4774 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15920 batch=620/7650 loss=1.4892 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15930 batch=630/7650 loss=1.5164 avg=1.5530 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15940 batch=640/7650 loss=1.6268 avg=1.5530 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15950 batch=650/7650 loss=1.6420 avg=1.5530 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15960 batch=660/7650 loss=1.6158 avg=1.5527 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=15970 batch=670/7650 loss=1.7837 avg=1.5537 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15980 batch=680/7650 loss=1.6519 avg=1.5533 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=15990 batch=690/7650 loss=1.3817 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16000 batch=700/7650 loss=1.4668 avg=1.5543 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16010 batch=710/7650 loss=1.5457 avg=1.5537 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16020 batch=720/7650 loss=1.5137 avg=1.5542 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16030 batch=730/7650 loss=1.4110 avg=1.5543 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16040 batch=740/7650 loss=1.3399 avg=1.5550 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16050 batch=750/7650 loss=1.3454 avg=1.5547 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16060 batch=760/7650 loss=1.6100 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16070 batch=770/7650 loss=1.5566 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16080 batch=780/7650 loss=1.4902 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16090 batch=790/7650 loss=1.5721 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16100 batch=800/7650 loss=1.6011 avg=1.5547 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16110 batch=810/7650 loss=1.5483 avg=1.5543 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16120 batch=820/7650 loss=1.5643 avg=1.5545 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=16130 batch=830/7650 loss=1.4818 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16140 batch=840/7650 loss=1.5847 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16150 batch=850/7650 loss=1.6124 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16160 batch=860/7650 loss=1.3015 avg=1.5555 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16170 batch=870/7650 loss=1.5548 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16180 batch=880/7650 loss=1.6367 avg=1.5556 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16190 batch=890/7650 loss=1.4852 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16200 batch=900/7650 loss=1.6351 avg=1.5555 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16210 batch=910/7650 loss=1.4977 avg=1.5554 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16220 batch=920/7650 loss=1.6991 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16230 batch=930/7650 loss=1.4577 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16240 batch=940/7650 loss=1.6162 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16250 batch=950/7650 loss=1.3935 avg=1.5562 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16260 batch=960/7650 loss=1.5526 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16270 batch=970/7650 loss=1.7232 avg=1.5556 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16280 batch=980/7650 loss=1.4748 avg=1.5555 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16290 batch=990/7650 loss=1.4865 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16300 batch=1000/7650 loss=1.5372 avg=1.5556 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16310 batch=1010/7650 loss=1.6638 avg=1.5563 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16320 batch=1020/7650 loss=1.4676 avg=1.5558 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16330 batch=1030/7650 loss=1.5497 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16340 batch=1040/7650 loss=1.3938 avg=1.5556 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16350 batch=1050/7650 loss=1.4207 avg=1.5559 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16360 batch=1060/7650 loss=1.7525 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16370 batch=1070/7650 loss=1.4201 avg=1.5562 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16380 batch=1080/7650 loss=1.4690 avg=1.5567 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16390 batch=1090/7650 loss=1.6297 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16400 batch=1100/7650 loss=1.6398 avg=1.5568 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16410 batch=1110/7650 loss=1.6473 avg=1.5570 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16420 batch=1120/7650 loss=1.5500 avg=1.5572 ppl=4.75 lr=3.00e-04\n",
      "[epoch 3/3] step=16430 batch=1130/7650 loss=1.9054 avg=1.5574 ppl=4.75 lr=3.00e-04\n",
      "[epoch 3/3] step=16440 batch=1140/7650 loss=1.6171 avg=1.5573 ppl=4.75 lr=3.00e-04\n",
      "[epoch 3/3] step=16450 batch=1150/7650 loss=1.4865 avg=1.5570 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16460 batch=1160/7650 loss=1.6625 avg=1.5571 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16470 batch=1170/7650 loss=1.5843 avg=1.5566 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16480 batch=1180/7650 loss=1.7131 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16490 batch=1190/7650 loss=1.5800 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16500 batch=1200/7650 loss=1.5325 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16510 batch=1210/7650 loss=1.4963 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16520 batch=1220/7650 loss=1.6478 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16530 batch=1230/7650 loss=1.6440 avg=1.5565 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16540 batch=1240/7650 loss=1.4332 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16550 batch=1250/7650 loss=1.6681 avg=1.5565 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16560 batch=1260/7650 loss=1.7041 avg=1.5565 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16570 batch=1270/7650 loss=1.4914 avg=1.5559 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16580 batch=1280/7650 loss=1.5932 avg=1.5558 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16590 batch=1290/7650 loss=1.4355 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16600 batch=1300/7650 loss=1.7041 avg=1.5562 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16610 batch=1310/7650 loss=1.5177 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16620 batch=1320/7650 loss=1.4715 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16630 batch=1330/7650 loss=1.7243 avg=1.5563 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16640 batch=1340/7650 loss=1.5212 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16650 batch=1350/7650 loss=1.4019 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16660 batch=1360/7650 loss=1.6346 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16670 batch=1370/7650 loss=1.4311 avg=1.5555 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16680 batch=1380/7650 loss=1.5743 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16690 batch=1390/7650 loss=1.3083 avg=1.5558 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16700 batch=1400/7650 loss=1.6718 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16710 batch=1410/7650 loss=1.7714 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16720 batch=1420/7650 loss=1.8134 avg=1.5559 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16730 batch=1430/7650 loss=1.6495 avg=1.5559 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16740 batch=1440/7650 loss=1.6292 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16750 batch=1450/7650 loss=1.4005 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16760 batch=1460/7650 loss=1.3569 avg=1.5558 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16770 batch=1470/7650 loss=1.4955 avg=1.5559 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16780 batch=1480/7650 loss=1.7976 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16790 batch=1490/7650 loss=1.5018 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16800 batch=1500/7650 loss=1.4262 avg=1.5560 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16810 batch=1510/7650 loss=1.4725 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16820 batch=1520/7650 loss=1.6169 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16830 batch=1530/7650 loss=1.6313 avg=1.5565 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16840 batch=1540/7650 loss=1.4462 avg=1.5565 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16850 batch=1550/7650 loss=1.6371 avg=1.5567 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16860 batch=1560/7650 loss=1.6447 avg=1.5567 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16870 batch=1570/7650 loss=1.5196 avg=1.5564 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16880 batch=1580/7650 loss=1.5801 avg=1.5563 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16890 batch=1590/7650 loss=1.6069 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16900 batch=1600/7650 loss=1.5938 avg=1.5566 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16910 batch=1610/7650 loss=1.5125 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16920 batch=1620/7650 loss=1.6290 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16930 batch=1630/7650 loss=1.6280 avg=1.5561 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16940 batch=1640/7650 loss=1.3606 avg=1.5557 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16950 batch=1650/7650 loss=1.4572 avg=1.5555 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16960 batch=1660/7650 loss=1.5800 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16970 batch=1670/7650 loss=1.4831 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16980 batch=1680/7650 loss=1.5053 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=16990 batch=1690/7650 loss=1.5816 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17000 batch=1700/7650 loss=1.5752 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17010 batch=1710/7650 loss=1.6798 avg=1.5550 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17020 batch=1720/7650 loss=1.4322 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17030 batch=1730/7650 loss=1.3734 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17040 batch=1740/7650 loss=1.5925 avg=1.5550 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17050 batch=1750/7650 loss=1.6652 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17060 batch=1760/7650 loss=1.4726 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17070 batch=1770/7650 loss=1.6819 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17080 batch=1780/7650 loss=1.4952 avg=1.5554 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17090 batch=1790/7650 loss=1.6338 avg=1.5550 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17100 batch=1800/7650 loss=1.4669 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17110 batch=1810/7650 loss=1.4422 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17120 batch=1820/7650 loss=1.3424 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17130 batch=1830/7650 loss=1.5346 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17140 batch=1840/7650 loss=1.4865 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17150 batch=1850/7650 loss=1.4477 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17160 batch=1860/7650 loss=1.3203 avg=1.5551 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17170 batch=1870/7650 loss=1.5339 avg=1.5550 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17180 batch=1880/7650 loss=1.5171 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17190 batch=1890/7650 loss=1.6625 avg=1.5552 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17200 batch=1900/7650 loss=1.5329 avg=1.5553 ppl=4.74 lr=3.00e-04\n",
      "[epoch 3/3] step=17210 batch=1910/7650 loss=1.5986 avg=1.5549 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17220 batch=1920/7650 loss=1.7476 avg=1.5549 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17230 batch=1930/7650 loss=1.3385 avg=1.5548 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17240 batch=1940/7650 loss=1.5385 avg=1.5547 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17250 batch=1950/7650 loss=1.6105 avg=1.5545 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17260 batch=1960/7650 loss=1.5700 avg=1.5543 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17270 batch=1970/7650 loss=1.4105 avg=1.5542 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17280 batch=1980/7650 loss=1.3734 avg=1.5540 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17290 batch=1990/7650 loss=1.4822 avg=1.5537 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17300 batch=2000/7650 loss=1.6098 avg=1.5536 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17310 batch=2010/7650 loss=1.3580 avg=1.5533 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17320 batch=2020/7650 loss=1.6001 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17330 batch=2030/7650 loss=1.5648 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17340 batch=2040/7650 loss=1.5584 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17350 batch=2050/7650 loss=1.4389 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17360 batch=2060/7650 loss=1.5936 avg=1.5531 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17370 batch=2070/7650 loss=1.5983 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17380 batch=2080/7650 loss=1.5431 avg=1.5535 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17390 batch=2090/7650 loss=1.5108 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17400 batch=2100/7650 loss=1.5418 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17410 batch=2110/7650 loss=1.5415 avg=1.5534 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17420 batch=2120/7650 loss=1.5961 avg=1.5535 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17430 batch=2130/7650 loss=1.5675 avg=1.5533 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17440 batch=2140/7650 loss=1.6220 avg=1.5532 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17450 batch=2150/7650 loss=1.6314 avg=1.5531 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17460 batch=2160/7650 loss=1.2830 avg=1.5528 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17470 batch=2170/7650 loss=1.5818 avg=1.5529 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17480 batch=2180/7650 loss=1.5091 avg=1.5526 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17490 batch=2190/7650 loss=1.3950 avg=1.5527 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17500 batch=2200/7650 loss=1.6859 avg=1.5529 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17510 batch=2210/7650 loss=1.5472 avg=1.5531 ppl=4.73 lr=3.00e-04\n",
      "[epoch 3/3] step=17520 batch=2220/7650 loss=1.5656 avg=1.5528 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17530 batch=2230/7650 loss=1.4920 avg=1.5528 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17540 batch=2240/7650 loss=1.8316 avg=1.5527 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17550 batch=2250/7650 loss=1.5963 avg=1.5526 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17560 batch=2260/7650 loss=1.3676 avg=1.5527 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17570 batch=2270/7650 loss=1.3227 avg=1.5527 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17580 batch=2280/7650 loss=1.5435 avg=1.5524 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17590 batch=2290/7650 loss=1.4384 avg=1.5521 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17600 batch=2300/7650 loss=1.7412 avg=1.5523 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17610 batch=2310/7650 loss=1.4804 avg=1.5521 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17620 batch=2320/7650 loss=1.3672 avg=1.5520 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17630 batch=2330/7650 loss=1.5269 avg=1.5518 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17640 batch=2340/7650 loss=1.4601 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17650 batch=2350/7650 loss=1.7168 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17660 batch=2360/7650 loss=1.4131 avg=1.5515 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17670 batch=2370/7650 loss=1.6443 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17680 batch=2380/7650 loss=1.5600 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17690 batch=2390/7650 loss=1.5113 avg=1.5515 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17700 batch=2400/7650 loss=1.4607 avg=1.5514 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17710 batch=2410/7650 loss=1.5430 avg=1.5516 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17720 batch=2420/7650 loss=1.6074 avg=1.5515 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17730 batch=2430/7650 loss=1.4636 avg=1.5512 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17740 batch=2440/7650 loss=1.5380 avg=1.5513 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17750 batch=2450/7650 loss=1.5736 avg=1.5512 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17760 batch=2460/7650 loss=1.4181 avg=1.5511 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17770 batch=2470/7650 loss=1.4574 avg=1.5510 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17780 batch=2480/7650 loss=1.5297 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17790 batch=2490/7650 loss=1.6099 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17800 batch=2500/7650 loss=1.6163 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17810 batch=2510/7650 loss=1.6258 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17820 batch=2520/7650 loss=1.6840 avg=1.5509 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17830 batch=2530/7650 loss=1.4569 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17840 batch=2540/7650 loss=1.4601 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17850 batch=2550/7650 loss=1.6896 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17860 batch=2560/7650 loss=1.5757 avg=1.5505 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17870 batch=2570/7650 loss=1.4817 avg=1.5504 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17880 batch=2580/7650 loss=1.6706 avg=1.5505 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17890 batch=2590/7650 loss=1.4996 avg=1.5504 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17900 batch=2600/7650 loss=1.6125 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17910 batch=2610/7650 loss=1.6866 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17920 batch=2620/7650 loss=1.3170 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17930 batch=2630/7650 loss=1.8163 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17940 batch=2640/7650 loss=1.7338 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=17950 batch=2650/7650 loss=1.7425 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17960 batch=2660/7650 loss=1.5754 avg=1.5507 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17970 batch=2670/7650 loss=1.7005 avg=1.5510 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17980 batch=2680/7650 loss=1.4648 avg=1.5510 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=17990 batch=2690/7650 loss=1.4916 avg=1.5508 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=18000 batch=2700/7650 loss=1.6435 avg=1.5509 ppl=4.72 lr=3.00e-04\n",
      "[epoch 3/3] step=18010 batch=2710/7650 loss=1.4622 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18020 batch=2720/7650 loss=1.5063 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18030 batch=2730/7650 loss=1.4631 avg=1.5504 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18040 batch=2740/7650 loss=1.6470 avg=1.5503 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18050 batch=2750/7650 loss=1.7022 avg=1.5505 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18060 batch=2760/7650 loss=1.8540 avg=1.5506 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18070 batch=2770/7650 loss=1.4694 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18080 batch=2780/7650 loss=1.6304 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18090 batch=2790/7650 loss=1.7010 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18100 batch=2800/7650 loss=1.5087 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18110 batch=2810/7650 loss=1.6150 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18120 batch=2820/7650 loss=1.6169 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18130 batch=2830/7650 loss=1.3693 avg=1.5507 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18140 batch=2840/7650 loss=1.5308 avg=1.5504 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18150 batch=2850/7650 loss=1.5319 avg=1.5503 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18160 batch=2860/7650 loss=1.3603 avg=1.5502 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18170 batch=2870/7650 loss=1.5772 avg=1.5501 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18180 batch=2880/7650 loss=1.6580 avg=1.5501 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18190 batch=2890/7650 loss=1.3222 avg=1.5500 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18200 batch=2900/7650 loss=1.5747 avg=1.5500 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18210 batch=2910/7650 loss=1.6145 avg=1.5499 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18220 batch=2920/7650 loss=1.5292 avg=1.5499 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18230 batch=2930/7650 loss=1.3817 avg=1.5500 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18240 batch=2940/7650 loss=1.3789 avg=1.5501 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18250 batch=2950/7650 loss=1.3509 avg=1.5500 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18260 batch=2960/7650 loss=1.6435 avg=1.5501 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18270 batch=2970/7650 loss=1.4075 avg=1.5500 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18280 batch=2980/7650 loss=1.5577 avg=1.5499 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18290 batch=2990/7650 loss=1.2819 avg=1.5498 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18300 batch=3000/7650 loss=1.4413 avg=1.5495 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18310 batch=3010/7650 loss=1.5193 avg=1.5496 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18320 batch=3020/7650 loss=1.5439 avg=1.5497 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18330 batch=3030/7650 loss=1.6088 avg=1.5497 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18340 batch=3040/7650 loss=1.3044 avg=1.5495 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18350 batch=3050/7650 loss=1.4521 avg=1.5495 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18360 batch=3060/7650 loss=1.4384 avg=1.5493 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18370 batch=3070/7650 loss=1.3116 avg=1.5491 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18380 batch=3080/7650 loss=1.5095 avg=1.5490 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18390 batch=3090/7650 loss=1.5075 avg=1.5489 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18400 batch=3100/7650 loss=1.5906 avg=1.5490 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18410 batch=3110/7650 loss=1.6212 avg=1.5491 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18420 batch=3120/7650 loss=1.4167 avg=1.5491 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18430 batch=3130/7650 loss=1.4163 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18440 batch=3140/7650 loss=1.5648 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18450 batch=3150/7650 loss=1.4274 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18460 batch=3160/7650 loss=1.3250 avg=1.5487 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18470 batch=3170/7650 loss=1.4957 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18480 batch=3180/7650 loss=1.4672 avg=1.5487 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18490 batch=3190/7650 loss=1.7614 avg=1.5489 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18500 batch=3200/7650 loss=1.7156 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18510 batch=3210/7650 loss=1.5475 avg=1.5489 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18520 batch=3220/7650 loss=1.4155 avg=1.5488 ppl=4.71 lr=3.00e-04\n",
      "[epoch 3/3] step=18530 batch=3230/7650 loss=1.5968 avg=1.5484 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18540 batch=3240/7650 loss=1.4355 avg=1.5483 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18550 batch=3250/7650 loss=1.4249 avg=1.5483 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18560 batch=3260/7650 loss=1.4488 avg=1.5483 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18570 batch=3270/7650 loss=1.4851 avg=1.5482 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18580 batch=3280/7650 loss=1.5358 avg=1.5482 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18590 batch=3290/7650 loss=1.5740 avg=1.5482 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18600 batch=3300/7650 loss=1.7356 avg=1.5481 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18610 batch=3310/7650 loss=1.5190 avg=1.5480 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18620 batch=3320/7650 loss=1.5731 avg=1.5481 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18630 batch=3330/7650 loss=1.3958 avg=1.5479 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18640 batch=3340/7650 loss=1.6580 avg=1.5479 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18650 batch=3350/7650 loss=1.4393 avg=1.5480 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18660 batch=3360/7650 loss=1.6224 avg=1.5479 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18670 batch=3370/7650 loss=1.6218 avg=1.5477 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18680 batch=3380/7650 loss=1.5579 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18690 batch=3390/7650 loss=1.4662 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18700 batch=3400/7650 loss=1.5895 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18710 batch=3410/7650 loss=1.3912 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18720 batch=3420/7650 loss=1.5083 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18730 batch=3430/7650 loss=1.4378 avg=1.5476 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18740 batch=3440/7650 loss=1.3715 avg=1.5475 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18750 batch=3450/7650 loss=1.3170 avg=1.5474 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18760 batch=3460/7650 loss=1.5692 avg=1.5474 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18770 batch=3470/7650 loss=1.4925 avg=1.5475 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18780 batch=3480/7650 loss=1.5245 avg=1.5475 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18790 batch=3490/7650 loss=1.5983 avg=1.5475 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18800 batch=3500/7650 loss=1.4396 avg=1.5474 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18810 batch=3510/7650 loss=1.7021 avg=1.5473 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18820 batch=3520/7650 loss=1.5433 avg=1.5472 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18830 batch=3530/7650 loss=1.6585 avg=1.5469 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18840 batch=3540/7650 loss=1.4014 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18850 batch=3550/7650 loss=1.4802 avg=1.5468 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18860 batch=3560/7650 loss=1.6305 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18870 batch=3570/7650 loss=1.5142 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18880 batch=3580/7650 loss=1.3528 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18890 batch=3590/7650 loss=1.4829 avg=1.5466 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18900 batch=3600/7650 loss=1.6209 avg=1.5468 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18910 batch=3610/7650 loss=1.4509 avg=1.5469 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18920 batch=3620/7650 loss=1.7090 avg=1.5467 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18930 batch=3630/7650 loss=1.5125 avg=1.5465 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=18940 batch=3640/7650 loss=1.3921 avg=1.5465 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18950 batch=3650/7650 loss=1.4761 avg=1.5465 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=18960 batch=3660/7650 loss=1.4905 avg=1.5465 ppl=4.70 lr=3.00e-04\n",
      "[epoch 3/3] step=18970 batch=3670/7650 loss=1.5518 avg=1.5465 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=18980 batch=3680/7650 loss=1.4669 avg=1.5464 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=18990 batch=3690/7650 loss=1.6118 avg=1.5463 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19000 batch=3700/7650 loss=1.3605 avg=1.5463 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19010 batch=3710/7650 loss=1.7039 avg=1.5464 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19020 batch=3720/7650 loss=1.6364 avg=1.5464 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19030 batch=3730/7650 loss=1.6315 avg=1.5464 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19040 batch=3740/7650 loss=1.5176 avg=1.5463 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19050 batch=3750/7650 loss=1.7329 avg=1.5464 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19060 batch=3760/7650 loss=1.4757 avg=1.5462 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19070 batch=3770/7650 loss=1.6868 avg=1.5462 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19080 batch=3780/7650 loss=1.5500 avg=1.5461 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19090 batch=3790/7650 loss=1.4146 avg=1.5461 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19100 batch=3800/7650 loss=1.4290 avg=1.5459 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19110 batch=3810/7650 loss=1.4111 avg=1.5458 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19120 batch=3820/7650 loss=1.6652 avg=1.5457 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19130 batch=3830/7650 loss=1.6108 avg=1.5457 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19140 batch=3840/7650 loss=1.4667 avg=1.5457 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19150 batch=3850/7650 loss=1.6099 avg=1.5456 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19160 batch=3860/7650 loss=1.6398 avg=1.5455 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19170 batch=3870/7650 loss=1.3980 avg=1.5454 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19180 batch=3880/7650 loss=1.4408 avg=1.5454 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19190 batch=3890/7650 loss=1.4560 avg=1.5453 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19200 batch=3900/7650 loss=1.6268 avg=1.5453 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19210 batch=3910/7650 loss=1.4564 avg=1.5452 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19220 batch=3920/7650 loss=1.3285 avg=1.5452 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19230 batch=3930/7650 loss=1.5578 avg=1.5452 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19240 batch=3940/7650 loss=1.4758 avg=1.5453 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19250 batch=3950/7650 loss=1.5350 avg=1.5454 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19260 batch=3960/7650 loss=1.4869 avg=1.5453 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19270 batch=3970/7650 loss=1.4111 avg=1.5453 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19280 batch=3980/7650 loss=1.5353 avg=1.5454 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19290 batch=3990/7650 loss=1.4198 avg=1.5452 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19300 batch=4000/7650 loss=1.5943 avg=1.5451 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19310 batch=4010/7650 loss=1.5419 avg=1.5452 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19320 batch=4020/7650 loss=1.6988 avg=1.5451 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19330 batch=4030/7650 loss=1.6398 avg=1.5449 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19340 batch=4040/7650 loss=1.5998 avg=1.5448 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19350 batch=4050/7650 loss=1.4997 avg=1.5447 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19360 batch=4060/7650 loss=1.5299 avg=1.5445 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19370 batch=4070/7650 loss=1.6152 avg=1.5444 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19380 batch=4080/7650 loss=1.5171 avg=1.5444 ppl=4.69 lr=3.00e-04\n",
      "[epoch 3/3] step=19390 batch=4090/7650 loss=1.5817 avg=1.5443 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19400 batch=4100/7650 loss=1.5637 avg=1.5442 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19410 batch=4110/7650 loss=1.5365 avg=1.5441 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19420 batch=4120/7650 loss=1.4332 avg=1.5438 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19430 batch=4130/7650 loss=1.4225 avg=1.5437 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19440 batch=4140/7650 loss=1.7036 avg=1.5437 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19450 batch=4150/7650 loss=1.4583 avg=1.5436 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19460 batch=4160/7650 loss=1.4569 avg=1.5435 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19470 batch=4170/7650 loss=1.5160 avg=1.5435 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19480 batch=4180/7650 loss=1.7783 avg=1.5435 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19490 batch=4190/7650 loss=1.5242 avg=1.5433 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19500 batch=4200/7650 loss=1.5712 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19510 batch=4210/7650 loss=1.4328 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19520 batch=4220/7650 loss=1.5593 avg=1.5433 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19530 batch=4230/7650 loss=1.7520 avg=1.5433 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19540 batch=4240/7650 loss=1.6841 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19550 batch=4250/7650 loss=1.5013 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19560 batch=4260/7650 loss=1.7034 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19570 batch=4270/7650 loss=1.4422 avg=1.5431 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19580 batch=4280/7650 loss=1.3874 avg=1.5431 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19590 batch=4290/7650 loss=1.5420 avg=1.5431 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19600 batch=4300/7650 loss=1.3095 avg=1.5432 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19610 batch=4310/7650 loss=1.4464 avg=1.5430 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19620 batch=4320/7650 loss=1.2693 avg=1.5429 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19630 batch=4330/7650 loss=1.5358 avg=1.5428 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19640 batch=4340/7650 loss=1.4930 avg=1.5426 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19650 batch=4350/7650 loss=1.3364 avg=1.5425 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19660 batch=4360/7650 loss=1.4754 avg=1.5423 ppl=4.68 lr=3.00e-04\n",
      "[epoch 3/3] step=19670 batch=4370/7650 loss=1.4354 avg=1.5422 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19680 batch=4380/7650 loss=1.4057 avg=1.5421 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19690 batch=4390/7650 loss=1.4160 avg=1.5420 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19700 batch=4400/7650 loss=1.4746 avg=1.5418 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19710 batch=4410/7650 loss=1.5438 avg=1.5418 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19720 batch=4420/7650 loss=1.5068 avg=1.5417 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19730 batch=4430/7650 loss=1.6459 avg=1.5416 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19740 batch=4440/7650 loss=1.5619 avg=1.5417 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19750 batch=4450/7650 loss=1.4565 avg=1.5416 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19760 batch=4460/7650 loss=1.6691 avg=1.5417 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19770 batch=4470/7650 loss=1.5033 avg=1.5415 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19780 batch=4480/7650 loss=1.6203 avg=1.5414 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19790 batch=4490/7650 loss=1.3910 avg=1.5414 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19800 batch=4500/7650 loss=1.4621 avg=1.5414 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19810 batch=4510/7650 loss=1.5345 avg=1.5413 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19820 batch=4520/7650 loss=1.6315 avg=1.5412 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19830 batch=4530/7650 loss=1.5957 avg=1.5412 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19840 batch=4540/7650 loss=1.4574 avg=1.5410 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19850 batch=4550/7650 loss=1.5863 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19860 batch=4560/7650 loss=1.4009 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19870 batch=4570/7650 loss=1.5723 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19880 batch=4580/7650 loss=1.4834 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19890 batch=4590/7650 loss=1.6508 avg=1.5409 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19900 batch=4600/7650 loss=1.4909 avg=1.5408 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19910 batch=4610/7650 loss=1.5810 avg=1.5407 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19920 batch=4620/7650 loss=1.4386 avg=1.5406 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19930 batch=4630/7650 loss=1.8103 avg=1.5406 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19940 batch=4640/7650 loss=1.5606 avg=1.5406 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19950 batch=4650/7650 loss=1.4128 avg=1.5405 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19960 batch=4660/7650 loss=1.5708 avg=1.5405 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19970 batch=4670/7650 loss=1.6115 avg=1.5404 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19980 batch=4680/7650 loss=1.6127 avg=1.5405 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=19990 batch=4690/7650 loss=1.5467 avg=1.5404 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20000 batch=4700/7650 loss=1.5247 avg=1.5404 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20010 batch=4710/7650 loss=1.5085 avg=1.5404 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20020 batch=4720/7650 loss=1.6195 avg=1.5405 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20030 batch=4730/7650 loss=1.6559 avg=1.5404 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20040 batch=4740/7650 loss=1.5106 avg=1.5403 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20050 batch=4750/7650 loss=1.6346 avg=1.5403 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20060 batch=4760/7650 loss=1.6194 avg=1.5403 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20070 batch=4770/7650 loss=1.7136 avg=1.5403 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20080 batch=4780/7650 loss=1.5147 avg=1.5401 ppl=4.67 lr=3.00e-04\n",
      "[epoch 3/3] step=20090 batch=4790/7650 loss=1.4129 avg=1.5400 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20100 batch=4800/7650 loss=1.3864 avg=1.5399 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20110 batch=4810/7650 loss=1.6790 avg=1.5398 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20120 batch=4820/7650 loss=1.4309 avg=1.5398 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20130 batch=4830/7650 loss=1.5199 avg=1.5399 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20140 batch=4840/7650 loss=1.4225 avg=1.5397 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20150 batch=4850/7650 loss=1.6382 avg=1.5397 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20160 batch=4860/7650 loss=1.4983 avg=1.5396 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20170 batch=4870/7650 loss=1.5918 avg=1.5396 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20180 batch=4880/7650 loss=1.5262 avg=1.5394 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20190 batch=4890/7650 loss=1.6918 avg=1.5395 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20200 batch=4900/7650 loss=1.5065 avg=1.5395 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20210 batch=4910/7650 loss=1.5989 avg=1.5394 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20220 batch=4920/7650 loss=1.3664 avg=1.5393 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20230 batch=4930/7650 loss=1.4333 avg=1.5392 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20240 batch=4940/7650 loss=1.2881 avg=1.5391 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20250 batch=4950/7650 loss=1.3426 avg=1.5390 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20260 batch=4960/7650 loss=1.5874 avg=1.5390 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20270 batch=4970/7650 loss=1.4332 avg=1.5390 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20280 batch=4980/7650 loss=1.5796 avg=1.5389 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20290 batch=4990/7650 loss=1.6300 avg=1.5390 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20300 batch=5000/7650 loss=1.6610 avg=1.5391 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20310 batch=5010/7650 loss=1.4162 avg=1.5390 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20320 batch=5020/7650 loss=1.5051 avg=1.5389 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20330 batch=5030/7650 loss=1.5367 avg=1.5389 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20340 batch=5040/7650 loss=1.5012 avg=1.5388 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20350 batch=5050/7650 loss=1.5680 avg=1.5387 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20360 batch=5060/7650 loss=1.4304 avg=1.5387 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20370 batch=5070/7650 loss=1.4792 avg=1.5386 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20380 batch=5080/7650 loss=1.6589 avg=1.5386 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20390 batch=5090/7650 loss=1.3903 avg=1.5385 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20400 batch=5100/7650 loss=1.5299 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20410 batch=5110/7650 loss=1.4145 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20420 batch=5120/7650 loss=1.6054 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20430 batch=5130/7650 loss=1.5927 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20440 batch=5140/7650 loss=1.6961 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20450 batch=5150/7650 loss=1.3470 avg=1.5384 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20460 batch=5160/7650 loss=1.4121 avg=1.5383 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20470 batch=5170/7650 loss=1.4250 avg=1.5383 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20480 batch=5180/7650 loss=1.6747 avg=1.5382 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20490 batch=5190/7650 loss=1.5085 avg=1.5382 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20500 batch=5200/7650 loss=1.5955 avg=1.5381 ppl=4.66 lr=3.00e-04\n",
      "[epoch 3/3] step=20510 batch=5210/7650 loss=1.6365 avg=1.5379 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20520 batch=5220/7650 loss=1.4554 avg=1.5379 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20530 batch=5230/7650 loss=1.4569 avg=1.5378 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20540 batch=5240/7650 loss=1.5475 avg=1.5377 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20550 batch=5250/7650 loss=1.3707 avg=1.5375 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20560 batch=5260/7650 loss=1.5413 avg=1.5376 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20570 batch=5270/7650 loss=1.4960 avg=1.5376 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20580 batch=5280/7650 loss=1.4190 avg=1.5375 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20590 batch=5290/7650 loss=1.4983 avg=1.5374 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20600 batch=5300/7650 loss=1.3978 avg=1.5373 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20610 batch=5310/7650 loss=1.4780 avg=1.5372 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20620 batch=5320/7650 loss=1.5639 avg=1.5372 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20630 batch=5330/7650 loss=1.5001 avg=1.5372 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20640 batch=5340/7650 loss=1.2947 avg=1.5370 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20650 batch=5350/7650 loss=1.6171 avg=1.5371 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20660 batch=5360/7650 loss=1.6555 avg=1.5370 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20670 batch=5370/7650 loss=1.3700 avg=1.5369 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20680 batch=5380/7650 loss=1.6098 avg=1.5370 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20690 batch=5390/7650 loss=1.4356 avg=1.5369 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20700 batch=5400/7650 loss=1.5766 avg=1.5368 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20710 batch=5410/7650 loss=1.6242 avg=1.5368 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20720 batch=5420/7650 loss=1.4393 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20730 batch=5430/7650 loss=1.4316 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20740 batch=5440/7650 loss=1.5986 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20750 batch=5450/7650 loss=1.5154 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20760 batch=5460/7650 loss=1.3073 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20770 batch=5470/7650 loss=1.3807 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20780 batch=5480/7650 loss=1.5013 avg=1.5365 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20790 batch=5490/7650 loss=1.5563 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20800 batch=5500/7650 loss=1.5379 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20810 batch=5510/7650 loss=1.5668 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20820 batch=5520/7650 loss=1.4784 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20830 batch=5530/7650 loss=1.5884 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20840 batch=5540/7650 loss=1.6176 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20850 batch=5550/7650 loss=1.6388 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20860 batch=5560/7650 loss=1.5740 avg=1.5367 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20870 batch=5570/7650 loss=1.4183 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20880 batch=5580/7650 loss=1.3943 avg=1.5366 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20890 batch=5590/7650 loss=1.3888 avg=1.5365 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20900 batch=5600/7650 loss=1.5268 avg=1.5365 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20910 batch=5610/7650 loss=1.2394 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20920 batch=5620/7650 loss=1.4996 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20930 batch=5630/7650 loss=1.4640 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20940 batch=5640/7650 loss=1.6210 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20950 batch=5650/7650 loss=1.7531 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20960 batch=5660/7650 loss=1.5259 avg=1.5364 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20970 batch=5670/7650 loss=1.5823 avg=1.5363 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20980 batch=5680/7650 loss=1.5509 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=20990 batch=5690/7650 loss=1.5497 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21000 batch=5700/7650 loss=1.4791 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21010 batch=5710/7650 loss=1.5025 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21020 batch=5720/7650 loss=1.5738 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21030 batch=5730/7650 loss=1.6465 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21040 batch=5740/7650 loss=1.5676 avg=1.5362 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21050 batch=5750/7650 loss=1.6073 avg=1.5361 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21060 batch=5760/7650 loss=1.3920 avg=1.5360 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21070 batch=5770/7650 loss=1.4728 avg=1.5359 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21080 batch=5780/7650 loss=1.6438 avg=1.5359 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21090 batch=5790/7650 loss=1.5624 avg=1.5359 ppl=4.65 lr=3.00e-04\n",
      "[epoch 3/3] step=21100 batch=5800/7650 loss=1.2872 avg=1.5357 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21110 batch=5810/7650 loss=1.5475 avg=1.5357 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21120 batch=5820/7650 loss=1.4049 avg=1.5356 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21130 batch=5830/7650 loss=1.7489 avg=1.5355 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21140 batch=5840/7650 loss=1.4580 avg=1.5355 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21150 batch=5850/7650 loss=1.5785 avg=1.5354 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21160 batch=5860/7650 loss=1.5281 avg=1.5354 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21170 batch=5870/7650 loss=1.2432 avg=1.5353 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21180 batch=5880/7650 loss=1.3917 avg=1.5354 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21190 batch=5890/7650 loss=1.4030 avg=1.5352 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21200 batch=5900/7650 loss=1.6110 avg=1.5352 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21210 batch=5910/7650 loss=1.4656 avg=1.5352 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21220 batch=5920/7650 loss=1.5561 avg=1.5352 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21230 batch=5930/7650 loss=1.5607 avg=1.5351 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21240 batch=5940/7650 loss=1.4983 avg=1.5351 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21250 batch=5950/7650 loss=1.4596 avg=1.5351 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21260 batch=5960/7650 loss=1.5581 avg=1.5349 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21270 batch=5970/7650 loss=1.4250 avg=1.5348 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21280 batch=5980/7650 loss=1.5491 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21290 batch=5990/7650 loss=1.3724 avg=1.5348 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21300 batch=6000/7650 loss=1.4820 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21310 batch=6010/7650 loss=1.4374 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21320 batch=6020/7650 loss=1.6722 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21330 batch=6030/7650 loss=1.4069 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21340 batch=6040/7650 loss=1.5261 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21350 batch=6050/7650 loss=1.7466 avg=1.5347 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21360 batch=6060/7650 loss=1.5808 avg=1.5346 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21370 batch=6070/7650 loss=1.5532 avg=1.5346 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21380 batch=6080/7650 loss=1.4145 avg=1.5346 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21390 batch=6090/7650 loss=1.5035 avg=1.5345 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21400 batch=6100/7650 loss=1.8036 avg=1.5345 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21410 batch=6110/7650 loss=1.6949 avg=1.5344 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21420 batch=6120/7650 loss=1.5509 avg=1.5344 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21430 batch=6130/7650 loss=1.5611 avg=1.5346 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21440 batch=6140/7650 loss=1.4529 avg=1.5345 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21450 batch=6150/7650 loss=1.4594 avg=1.5344 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21460 batch=6160/7650 loss=1.5906 avg=1.5343 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21470 batch=6170/7650 loss=1.5660 avg=1.5343 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21480 batch=6180/7650 loss=1.4695 avg=1.5343 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21490 batch=6190/7650 loss=1.3863 avg=1.5341 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21500 batch=6200/7650 loss=1.6312 avg=1.5341 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21510 batch=6210/7650 loss=1.5342 avg=1.5341 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21520 batch=6220/7650 loss=1.6183 avg=1.5340 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21530 batch=6230/7650 loss=1.5561 avg=1.5340 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21540 batch=6240/7650 loss=1.6244 avg=1.5340 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21550 batch=6250/7650 loss=1.4998 avg=1.5339 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21560 batch=6260/7650 loss=1.5688 avg=1.5338 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21570 batch=6270/7650 loss=1.4183 avg=1.5339 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21580 batch=6280/7650 loss=1.3237 avg=1.5338 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21590 batch=6290/7650 loss=1.6102 avg=1.5338 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21600 batch=6300/7650 loss=1.4443 avg=1.5338 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21610 batch=6310/7650 loss=1.4963 avg=1.5337 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21620 batch=6320/7650 loss=1.4988 avg=1.5337 ppl=4.64 lr=3.00e-04\n",
      "[epoch 3/3] step=21630 batch=6330/7650 loss=1.4556 avg=1.5336 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21640 batch=6340/7650 loss=1.5423 avg=1.5336 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21650 batch=6350/7650 loss=1.4829 avg=1.5335 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21660 batch=6360/7650 loss=1.4891 avg=1.5335 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21670 batch=6370/7650 loss=1.4250 avg=1.5334 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21680 batch=6380/7650 loss=1.4383 avg=1.5334 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21690 batch=6390/7650 loss=1.4393 avg=1.5333 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21700 batch=6400/7650 loss=1.3757 avg=1.5331 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21710 batch=6410/7650 loss=1.4617 avg=1.5331 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21720 batch=6420/7650 loss=1.2630 avg=1.5330 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21730 batch=6430/7650 loss=1.4851 avg=1.5330 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21740 batch=6440/7650 loss=1.6288 avg=1.5330 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21750 batch=6450/7650 loss=1.5778 avg=1.5330 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21760 batch=6460/7650 loss=1.5286 avg=1.5329 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21770 batch=6470/7650 loss=1.4740 avg=1.5329 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21780 batch=6480/7650 loss=1.4235 avg=1.5327 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21790 batch=6490/7650 loss=1.4251 avg=1.5327 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21800 batch=6500/7650 loss=1.5538 avg=1.5325 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21810 batch=6510/7650 loss=1.6202 avg=1.5324 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21820 batch=6520/7650 loss=1.3598 avg=1.5324 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21830 batch=6530/7650 loss=1.4338 avg=1.5323 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21840 batch=6540/7650 loss=1.3848 avg=1.5323 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21850 batch=6550/7650 loss=1.3406 avg=1.5321 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21860 batch=6560/7650 loss=1.3622 avg=1.5321 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21870 batch=6570/7650 loss=1.6901 avg=1.5321 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21880 batch=6580/7650 loss=1.4143 avg=1.5320 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21890 batch=6590/7650 loss=1.4965 avg=1.5319 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21900 batch=6600/7650 loss=1.5059 avg=1.5319 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21910 batch=6610/7650 loss=1.5875 avg=1.5318 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21920 batch=6620/7650 loss=1.4385 avg=1.5318 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21930 batch=6630/7650 loss=1.5411 avg=1.5317 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21940 batch=6640/7650 loss=1.4994 avg=1.5315 ppl=4.63 lr=3.00e-04\n",
      "[epoch 3/3] step=21950 batch=6650/7650 loss=1.4829 avg=1.5314 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=21960 batch=6660/7650 loss=1.5107 avg=1.5314 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=21970 batch=6670/7650 loss=1.5236 avg=1.5314 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=21980 batch=6680/7650 loss=1.4763 avg=1.5314 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=21990 batch=6690/7650 loss=1.5212 avg=1.5313 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22000 batch=6700/7650 loss=1.4940 avg=1.5313 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22010 batch=6710/7650 loss=1.5122 avg=1.5313 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22020 batch=6720/7650 loss=1.7294 avg=1.5313 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22030 batch=6730/7650 loss=1.4574 avg=1.5312 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22040 batch=6740/7650 loss=1.6040 avg=1.5312 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22050 batch=6750/7650 loss=1.3466 avg=1.5311 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22060 batch=6760/7650 loss=1.3901 avg=1.5312 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22070 batch=6770/7650 loss=1.5951 avg=1.5311 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22080 batch=6780/7650 loss=1.4662 avg=1.5311 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22090 batch=6790/7650 loss=1.5569 avg=1.5310 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22100 batch=6800/7650 loss=1.4984 avg=1.5310 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22110 batch=6810/7650 loss=1.5199 avg=1.5309 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22120 batch=6820/7650 loss=1.3843 avg=1.5308 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22130 batch=6830/7650 loss=1.2841 avg=1.5307 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22140 batch=6840/7650 loss=1.5666 avg=1.5306 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22150 batch=6850/7650 loss=1.5658 avg=1.5306 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22160 batch=6860/7650 loss=1.4676 avg=1.5306 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22170 batch=6870/7650 loss=1.2805 avg=1.5305 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22180 batch=6880/7650 loss=1.4352 avg=1.5305 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22190 batch=6890/7650 loss=1.3699 avg=1.5304 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22200 batch=6900/7650 loss=1.3843 avg=1.5303 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22210 batch=6910/7650 loss=1.3978 avg=1.5302 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22220 batch=6920/7650 loss=1.4932 avg=1.5302 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22230 batch=6930/7650 loss=1.6644 avg=1.5302 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22240 batch=6940/7650 loss=1.4688 avg=1.5302 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22250 batch=6950/7650 loss=1.6342 avg=1.5301 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22260 batch=6960/7650 loss=1.5647 avg=1.5301 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22270 batch=6970/7650 loss=1.5476 avg=1.5300 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22280 batch=6980/7650 loss=1.3527 avg=1.5299 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22290 batch=6990/7650 loss=1.5522 avg=1.5299 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22300 batch=7000/7650 loss=1.2757 avg=1.5298 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22310 batch=7010/7650 loss=1.3197 avg=1.5298 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22320 batch=7020/7650 loss=1.3828 avg=1.5297 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22330 batch=7030/7650 loss=1.5979 avg=1.5297 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22340 batch=7040/7650 loss=1.4933 avg=1.5297 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22350 batch=7050/7650 loss=1.6438 avg=1.5296 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22360 batch=7060/7650 loss=1.4545 avg=1.5296 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22370 batch=7070/7650 loss=1.5246 avg=1.5296 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22380 batch=7080/7650 loss=1.8248 avg=1.5296 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22390 batch=7090/7650 loss=1.3556 avg=1.5295 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22400 batch=7100/7650 loss=1.3222 avg=1.5295 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22410 batch=7110/7650 loss=1.4301 avg=1.5294 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22420 batch=7120/7650 loss=1.5083 avg=1.5293 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22430 batch=7130/7650 loss=1.4972 avg=1.5293 ppl=4.62 lr=3.00e-04\n",
      "[epoch 3/3] step=22440 batch=7140/7650 loss=1.4434 avg=1.5293 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22450 batch=7150/7650 loss=1.5367 avg=1.5293 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22460 batch=7160/7650 loss=1.3738 avg=1.5292 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22470 batch=7170/7650 loss=1.6050 avg=1.5291 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22480 batch=7180/7650 loss=1.6117 avg=1.5290 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22490 batch=7190/7650 loss=1.4300 avg=1.5289 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22500 batch=7200/7650 loss=1.4715 avg=1.5289 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22510 batch=7210/7650 loss=1.5646 avg=1.5289 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22520 batch=7220/7650 loss=1.4436 avg=1.5288 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22530 batch=7230/7650 loss=1.4166 avg=1.5288 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22540 batch=7240/7650 loss=1.4635 avg=1.5288 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22550 batch=7250/7650 loss=1.5194 avg=1.5287 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22560 batch=7260/7650 loss=1.3792 avg=1.5286 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22570 batch=7270/7650 loss=1.3655 avg=1.5286 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22580 batch=7280/7650 loss=1.5595 avg=1.5286 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22590 batch=7290/7650 loss=1.5774 avg=1.5286 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22600 batch=7300/7650 loss=1.4459 avg=1.5286 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22610 batch=7310/7650 loss=1.4336 avg=1.5285 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22620 batch=7320/7650 loss=1.3541 avg=1.5285 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22630 batch=7330/7650 loss=1.2371 avg=1.5283 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22640 batch=7340/7650 loss=1.7156 avg=1.5283 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22650 batch=7350/7650 loss=1.2573 avg=1.5282 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22660 batch=7360/7650 loss=1.5213 avg=1.5282 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22670 batch=7370/7650 loss=1.5861 avg=1.5281 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22680 batch=7380/7650 loss=1.4317 avg=1.5280 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22690 batch=7390/7650 loss=1.4445 avg=1.5279 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22700 batch=7400/7650 loss=1.4436 avg=1.5279 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22710 batch=7410/7650 loss=1.5480 avg=1.5278 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22720 batch=7420/7650 loss=1.5409 avg=1.5277 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22730 batch=7430/7650 loss=1.4134 avg=1.5277 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22740 batch=7440/7650 loss=1.5478 avg=1.5277 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22750 batch=7450/7650 loss=1.4375 avg=1.5277 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22760 batch=7460/7650 loss=1.4424 avg=1.5277 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22770 batch=7470/7650 loss=1.4579 avg=1.5275 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22780 batch=7480/7650 loss=1.5039 avg=1.5275 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22790 batch=7490/7650 loss=1.5387 avg=1.5275 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22800 batch=7500/7650 loss=1.6175 avg=1.5275 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22810 batch=7510/7650 loss=1.5731 avg=1.5275 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22820 batch=7520/7650 loss=1.3490 avg=1.5274 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22830 batch=7530/7650 loss=1.6263 avg=1.5274 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22840 batch=7540/7650 loss=1.4869 avg=1.5274 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22850 batch=7550/7650 loss=1.5538 avg=1.5274 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22860 batch=7560/7650 loss=1.4433 avg=1.5273 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22870 batch=7570/7650 loss=1.3440 avg=1.5272 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22880 batch=7580/7650 loss=1.6644 avg=1.5272 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22890 batch=7590/7650 loss=1.5066 avg=1.5272 ppl=4.61 lr=3.00e-04\n",
      "[epoch 3/3] step=22900 batch=7600/7650 loss=1.4017 avg=1.5271 ppl=4.60 lr=3.00e-04\n",
      "[epoch 3/3] step=22910 batch=7610/7650 loss=1.5081 avg=1.5270 ppl=4.60 lr=3.00e-04\n",
      "[epoch 3/3] step=22920 batch=7620/7650 loss=1.3939 avg=1.5270 ppl=4.60 lr=3.00e-04\n",
      "[epoch 3/3] step=22930 batch=7630/7650 loss=1.7141 avg=1.5269 ppl=4.60 lr=3.00e-04\n",
      "[epoch 3/3] step=22940 batch=7640/7650 loss=1.3915 avg=1.5269 ppl=4.60 lr=3.00e-04\n",
      "[epoch 3/3] step=22950 batch=7650/7650 loss=1.6488 avg=1.5268 ppl=4.60 lr=3.00e-04\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6947e96bb54636b68c0a509977bfc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dev 3/3:   0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ dev epoch 3: first batch ok\n",
      "\n",
      "Epoch 3 DONE | train_loss=1.5268 ppl=4.60 | dev_loss=1.3663 ppl=3.92\n",
      "\n",
      "✅ saved best: /kaggle/working/vlsp_finetune/best_finetune.pt\n"
     ]
    }
   ],
   "source": [
    "import math, time\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torch.optim import AdamW\n",
    "from tqdm.notebook import tqdm   # \n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-4, betas=(0.9,0.98), eps=1e-9, weight_decay=0.01)\n",
    "\n",
    "global_step = 0\n",
    "PRINT_EVERY = 10   # \n",
    "\n",
    "# \n",
    "MAX_TGT_LEN = 80\n",
    "causal_cache = build_causal_cache(MAX_TGT_LEN, device)\n",
    "\n",
    "def run_one_epoch(model, loader, train: bool, epoch: int, epochs: int):\n",
    "    global global_step\n",
    "\n",
    "    model.train() if train else model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "    start_time = time.time()\n",
    "\n",
    "    mode = \"train\" if train else \"dev\"\n",
    "    pbar = tqdm(enumerate(loader, 1), total=len(loader), leave=False, mininterval=0.5,\n",
    "                desc=f\"{mode} {epoch}/{epochs}\")\n",
    "\n",
    "    for bi, batch in pbar:\n",
    "        if bi == 1:\n",
    "            print(f\"✅ {mode} epoch {epoch}: first batch ok\")\n",
    "\n",
    "        src     = batch[\"src_ids\"].to(device)\n",
    "        tgt_in  = batch[\"tgt_in_ids\"].to(device)\n",
    "        tgt_out = batch[\"tgt_out_ids\"].to(device)\n",
    "\n",
    "        src_pad = batch[\"src_padding_mask\"].to(device).bool()\n",
    "        tgt_pad = batch[\"tgt_padding_mask\"].to(device).bool()\n",
    "\n",
    "        src_mask = make_src_mask(src_pad)                 # (B,1,1,S)\n",
    "        tgt_mask = make_tgt_mask(tgt_pad, causal_cache)   # ✅ sửa dòng này\n",
    "\n",
    "        if train:\n",
    "            logits = model(src, tgt_in, src_mask, tgt_mask)\n",
    "            loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            global_step += 1\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                logits = model(src, tgt_in, src_mask, tgt_mask)\n",
    "                loss = criterion(logits.reshape(-1, logits.size(-1)), tgt_out.reshape(-1))\n",
    "\n",
    "        n_tokens = (tgt_out != pad_id).sum().item()\n",
    "        total_loss   += loss.item() * n_tokens\n",
    "        total_tokens += n_tokens\n",
    "\n",
    "        avg_loss_so_far = total_loss / max(1, total_tokens)\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        elapsed = time.time() - start_time\n",
    "        it_s = bi / max(1e-9, elapsed)\n",
    "        eta_s = (len(loader) - bi) / max(1e-9, it_s)\n",
    "\n",
    "        pbar.set_postfix({\n",
    "            \"step\": global_step if train else \"-\",\n",
    "            \"loss\": f\"{loss.item():.4f}\",\n",
    "            \"avg\":  f\"{avg_loss_so_far:.4f}\",\n",
    "            \"ppl\":  f\"{math.exp(min(20, avg_loss_so_far)):.2f}\",\n",
    "            \"lr\":   f\"{lr:.2e}\",\n",
    "            \"eta\":  f\"{eta_s/60:.1f}m\"\n",
    "        })\n",
    "\n",
    "        if train and (global_step % PRINT_EVERY == 0):\n",
    "            print(f\"[epoch {epoch}/{epochs}] step={global_step} batch={bi}/{len(loader)} \"\n",
    "                  f\"loss={loss.item():.4f} avg={avg_loss_so_far:.4f} \"\n",
    "                  f\"ppl={math.exp(min(20, avg_loss_so_far)):.2f} lr={lr:.2e}\")\n",
    "\n",
    "    avg_loss = total_loss / max(1, total_tokens)\n",
    "    ppl = math.exp(min(20, avg_loss))\n",
    "    return avg_loss, ppl\n",
    "\n",
    "\n",
    "best_dev = 1e9\n",
    "OUT_DIR = Path(\"/kaggle/working/vlsp_finetune\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EPOCHS = 3\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_loss, train_ppl = run_one_epoch(model, train_loader, train=True,  epoch=epoch, epochs=EPOCHS)\n",
    "    dev_loss, dev_ppl     = run_one_epoch(model, dev_loader,   train=False, epoch=epoch, epochs=EPOCHS)\n",
    "\n",
    "    print(f\"\\nEpoch {epoch} DONE | train_loss={train_loss:.4f} ppl={train_ppl:.2f} | dev_loss={dev_loss:.4f} ppl={dev_ppl:.2f}\\n\")\n",
    "\n",
    "    if dev_loss < best_dev:\n",
    "        best_dev = dev_loss\n",
    "        torch.save(model.state_dict(), OUT_DIR / \"best_finetune.pt\")\n",
    "        print(\"✅ saved best:\", OUT_DIR / \"best_finetune.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af32c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T12:32:00.957395Z",
     "iopub.status.busy": "2025-12-16T12:32:00.956876Z",
     "iopub.status.idle": "2025-12-16T15:46:13.110569Z",
     "shell.execute_reply": "2025-12-16T15:46:13.109747Z"
    },
    "papermill": {
     "duration": 11652.331015,
     "end_time": "2025-12-16T15:46:13.198592",
     "exception": false,
     "start_time": "2025-12-16T12:32:00.867577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ed052cafce41ada6b3790142fcd7fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Translating dev (greedy+beam):   0%|          | 0/9992 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEV BLEU (greedy): 41.08170874469103\n",
      "DEV BLEU (beam4) : 42.26405444624932\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "import sacrebleu\n",
    "\n",
    "# =========================\n",
    "# Utils\n",
    "# =========================\n",
    "def load_lines(p):\n",
    "    with open(p, encoding=\"utf-8\") as f:\n",
    "        return [line.strip() for line in f if line.strip()]\n",
    "\n",
    "# =========================\n",
    "# Causal cache for decoding\n",
    "# =========================\n",
    "DECODE_MAX_LEN = 120\n",
    "causal_cache = build_causal_cache(max_tgt_len=DECODE_MAX_LEN + 5, device=device)\n",
    "\n",
    "# =========================\n",
    "# Greedy decoding\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def greedy_translate_one(src_ids: torch.Tensor, max_len=DECODE_MAX_LEN) -> str:\n",
    "    model.eval()\n",
    "    src_ids = src_ids.unsqueeze(0).to(device)  # (1,S)\n",
    "\n",
    "    # 1 câu => không PAD\n",
    "    src_pad_mask = torch.zeros_like(src_ids, dtype=torch.bool, device=device)  # (1,S)\n",
    "    src_mask = make_src_mask(src_pad_mask)  # (1,1,1,S)\n",
    "\n",
    "    enc = model.encode(src_ids, src_mask)\n",
    "\n",
    "    ys = torch.tensor([[tok.bos_id]], device=device, dtype=torch.long)  # (1,1)\n",
    "    for _ in range(max_len):\n",
    "        tgt_pad = (ys == pad_id)  # (1,T)\n",
    "        tgt_mask = make_tgt_mask(tgt_pad, causal_cache)  # (1,1,T,T)\n",
    "\n",
    "        dec = model.decode(ys, enc, src_mask, tgt_mask)\n",
    "        logits = model.projection(dec)  # (1,T,V)\n",
    "\n",
    "        next_id = torch.argmax(logits[:, -1, :], dim=-1).item()\n",
    "        ys = torch.cat([ys, torch.tensor([[next_id]], device=device)], dim=1)\n",
    "\n",
    "        if next_id == tok.eos_id:\n",
    "            break\n",
    "\n",
    "    return tok.decode(ys.squeeze(0).tolist())\n",
    "\n",
    "# =========================\n",
    "# Beam search decoding\n",
    "# =========================\n",
    "@torch.no_grad()\n",
    "def beam_translate_one(\n",
    "    src_ids: torch.Tensor,\n",
    "    beam_size: int = 4,\n",
    "    max_len: int = DECODE_MAX_LEN,\n",
    "    len_norm_alpha: float = 0.6,\n",
    ") -> str:\n",
    "    model.eval()\n",
    "    src_ids = src_ids.unsqueeze(0).to(device)  # (1,S)\n",
    "\n",
    "    src_pad_mask = torch.zeros_like(src_ids, dtype=torch.bool, device=device)\n",
    "    src_mask = make_src_mask(src_pad_mask)\n",
    "\n",
    "    enc = model.encode(src_ids, src_mask)\n",
    "\n",
    "    # beam item: (tokens_list, sum_logprob, ended_bool)\n",
    "    beams = [([tok.bos_id], 0.0, False)]\n",
    "\n",
    "    def score(sum_logprob: float, length: int) -> float:\n",
    "        # GNMT length normalization\n",
    "        if len_norm_alpha <= 0:\n",
    "            return sum_logprob\n",
    "        denom = ((5.0 + length) / 6.0) ** len_norm_alpha\n",
    "        return sum_logprob / denom\n",
    "\n",
    "    for _ in range(max_len):\n",
    "        candidates = []\n",
    "\n",
    "        for tokens, sum_lp, ended in beams:\n",
    "            if ended:\n",
    "                candidates.append((tokens, sum_lp, True))\n",
    "                continue\n",
    "\n",
    "            ys = torch.tensor(tokens, device=device, dtype=torch.long).unsqueeze(0)  # (1,T)\n",
    "            tgt_pad = (ys == pad_id)\n",
    "            tgt_mask = make_tgt_mask(tgt_pad, causal_cache)\n",
    "\n",
    "            dec = model.decode(ys, enc, src_mask, tgt_mask)\n",
    "            logits = model.projection(dec)  # (1,T,V)\n",
    "            log_probs = torch.log_softmax(logits[:, -1, :], dim=-1).squeeze(0)  # (V,)\n",
    "\n",
    "            topk_lp, topk_id = torch.topk(log_probs, k=beam_size)\n",
    "            topk_lp = topk_lp.tolist()\n",
    "            topk_id = topk_id.tolist()\n",
    "\n",
    "            for lp, tid in zip(topk_lp, topk_id):\n",
    "                new_tokens = tokens + [tid]\n",
    "                new_sum_lp = sum_lp + lp\n",
    "                new_ended = (tid == tok.eos_id)\n",
    "                candidates.append((new_tokens, new_sum_lp, new_ended))\n",
    "\n",
    "        candidates.sort(key=lambda x: score(x[1], len(x[0])), reverse=True)\n",
    "        beams = candidates[:beam_size]\n",
    "\n",
    "        if all(b[2] for b in beams):  # all ended\n",
    "            break\n",
    "\n",
    "    best_tokens, _, _ = max(beams, key=lambda x: score(x[1], len(x[0])))\n",
    "    return tok.decode(best_tokens)\n",
    "\n",
    "# =========================\n",
    "# Evaluate BLEU on dev\n",
    "# =========================\n",
    "dev_src = load_lines(PROCESSED_DIR / \"dev.en\")\n",
    "dev_ref = load_lines(PROCESSED_DIR / \"dev.vi\")\n",
    "\n",
    "N = None  \n",
    "M = min(N, len(dev_src)) if N is not None else len(dev_src)\n",
    "\n",
    "preds_greedy = []\n",
    "preds_beam   = []\n",
    "\n",
    "for i in tqdm(range(M), desc=\"Translating dev (greedy+beam)\"):\n",
    "    src_ids = torch.tensor(\n",
    "        tok.encode_src(dev_src[i], add_bos=False, add_eos=True),\n",
    "        dtype=torch.long\n",
    "    )\n",
    "    preds_greedy.append(greedy_translate_one(src_ids))\n",
    "    preds_beam.append(beam_translate_one(src_ids, beam_size=6, len_norm_alpha=1.0))\n",
    "\n",
    "bleu_g = sacrebleu.corpus_bleu(preds_greedy, [dev_ref[:M]])\n",
    "bleu_b = sacrebleu.corpus_bleu(preds_beam,   [dev_ref[:M]])\n",
    "\n",
    "print(\"DEV BLEU (greedy):\", bleu_g.score)\n",
    "print(\"DEV BLEU (beam4) :\", bleu_b.score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a48c0f0",
   "metadata": {
    "papermill": {
     "duration": 0.085581,
     "end_time": "2025-12-16T15:46:13.370258",
     "exception": false,
     "start_time": "2025-12-16T15:46:13.284677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8997669,
     "sourceId": 14122672,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 9033082,
     "sourceId": 14171467,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 15014.152758,
   "end_time": "2025-12-16T15:46:15.280072",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-16T11:36:01.127314",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "00da549622cc413a9e90e70c90e74509": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "060f53626c0e460281a11f3c75d5c1a7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "090d3df75c7e4cc0941c9c46eea7dbd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e68aefed9ba4cbe98e8da94068c78da": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "10ae6e8df1bb4f3d990cd0e2e54a64c0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_d1a4ae1fb8614eb19a8d35b172cb8594",
        "IPY_MODEL_5b2f109c18da413e88b5e1ccc4a14e1c",
        "IPY_MODEL_5bc0a1f8eb5041ea9137c5e5eae8a844"
       ],
       "layout": "IPY_MODEL_0e68aefed9ba4cbe98e8da94068c78da",
       "tabbable": null,
       "tooltip": null
      }
     },
     "128a239a0bf14d05b0f7f2fb0d986217": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "167134fb41b2445ba9e10b6529d18fc7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1705bc7886d04fca9696d7df8238ad75": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_c2c355b2d54d4f1bb4f940ab37f12fd4",
       "max": 7650.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_22944a3968d64ba2acdaeb0ab99648dd",
       "tabbable": null,
       "tooltip": null,
       "value": 7650.0
      }
     },
     "1795121211a74bfbb65a7431360a26fa": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19ed052cafce41ada6b3790142fcd7fd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_4131525cac4442f5b40813a3e8a2737c",
        "IPY_MODEL_a329319add804a1c92c5f26139042473",
        "IPY_MODEL_67bd65f6d9ed4bc09daf0ce006b5cdaa"
       ],
       "layout": "IPY_MODEL_e0b9c6617c6a408d9c52e5203caf90bb",
       "tabbable": null,
       "tooltip": null
      }
     },
     "1bd90a39279e4f98883b237e3c032974": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2012524aeb2043cfab252cd34916f736": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "20e432b3ed9249f5bf8265eb273c7039": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "22944a3968d64ba2acdaeb0ab99648dd": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "248f4906a1d44cdaa2ee0505120e3eda": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "2e086c619b784635943f30649d9ddb08": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b9e00ed8b6ae41f0bbca2fed6ec5e97a",
       "max": 7650.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_928b77f391024c98a34cd081c8cf3478",
       "tabbable": null,
       "tooltip": null,
       "value": 7650.0
      }
     },
     "35f7927d1f434f3c87b47a4fe73b059a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a4f928b5f35449cfb9aaea60573438de",
       "placeholder": "​",
       "style": "IPY_MODEL_6b667fe6a6a64ea6889e466f59e3991c",
       "tabbable": null,
       "tooltip": null,
       "value": "dev 2/3:  99%"
      }
     },
     "365547c9fed74f5bb38a7dd4f91734bd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "37663d02376b4183877ccedfc25ba641": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "3ba75784d1d3483284cf8e5ce8fcdaa0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "3d3af4c20b254a26ba77d176395ec8e0": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8e3d4044721e44fc8cef7af48f98d0b1",
       "placeholder": "​",
       "style": "IPY_MODEL_4a92421f24e642e4a92db3ffc4864693",
       "tabbable": null,
       "tooltip": null,
       "value": " 156/157 [00:07&lt;00:00, 21.55it/s, step=-, loss=2.1726, avg=1.6929, ppl=5.44, lr=3.00e-04, eta=0.0m]"
      }
     },
     "4131525cac4442f5b40813a3e8a2737c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_167134fb41b2445ba9e10b6529d18fc7",
       "placeholder": "​",
       "style": "IPY_MODEL_00da549622cc413a9e90e70c90e74509",
       "tabbable": null,
       "tooltip": null,
       "value": "Translating dev (greedy+beam): 100%"
      }
     },
     "463831d2c7ad4258bb561e1350bf346d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_a59cdc1a03b043da9772748910c640d7",
       "placeholder": "​",
       "style": "IPY_MODEL_98001065d9914f2abec3ffaf08ab2287",
       "tabbable": null,
       "tooltip": null,
       "value": " 7648/7650 [18:25&lt;00:00,  6.75it/s, step=22950, loss=1.6488, avg=1.5268, ppl=4.60, lr=3.00e-04, eta=0.0m]"
      }
     },
     "4948c04d87104425b88a524eb61067ad": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4a92421f24e642e4a92db3ffc4864693": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "5086eab6421c4d088eb99fdffe112add": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_eeca8dda6e234a09a3d93f5e3e56ff17",
       "placeholder": "​",
       "style": "IPY_MODEL_d48f8dae1d8743e1b2a2d6303ca86be1",
       "tabbable": null,
       "tooltip": null,
       "value": " 7649/7650 [18:25&lt;00:00,  6.82it/s, step=7650, loss=1.7936, avg=2.4884, ppl=12.04, lr=3.00e-04, eta=0.0m]"
      }
     },
     "5920a87eaafd4bf7a84cd1d08e3b7a97": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_db97841691384f5fabf16f0d576f0c4e",
       "max": 157.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_a022855fca26499c87137a3a2e8852d6",
       "tabbable": null,
       "tooltip": null,
       "value": 157.0
      }
     },
     "5b2f109c18da413e88b5e1ccc4a14e1c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7219c9a2245643a7a3a17ec77ded795b",
       "max": 7650.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6cee254665f24a5aaa014f995211a51f",
       "tabbable": null,
       "tooltip": null,
       "value": 7650.0
      }
     },
     "5bc0a1f8eb5041ea9137c5e5eae8a844": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95f2ab15e9f6436fa1cd0ea56255895a",
       "placeholder": "​",
       "style": "IPY_MODEL_3ba75784d1d3483284cf8e5ce8fcdaa0",
       "tabbable": null,
       "tooltip": null,
       "value": " 7648/7650 [18:24&lt;00:00,  6.76it/s, step=15300, loss=1.6332, avg=1.7001, ppl=5.47, lr=3.00e-04, eta=0.0m]"
      }
     },
     "60efd15052a94ad39b62cc59219c7cd8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_4948c04d87104425b88a524eb61067ad",
       "placeholder": "​",
       "style": "IPY_MODEL_bb3469dee6444acfa7fae640d274acee",
       "tabbable": null,
       "tooltip": null,
       "value": " 155/157 [00:07&lt;00:00, 21.39it/s, step=-, loss=1.8385, avg=1.3663, ppl=3.92, lr=3.00e-04, eta=0.0m]"
      }
     },
     "67bd65f6d9ed4bc09daf0ce006b5cdaa": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1bd90a39279e4f98883b237e3c032974",
       "placeholder": "​",
       "style": "IPY_MODEL_2012524aeb2043cfab252cd34916f736",
       "tabbable": null,
       "tooltip": null,
       "value": " 9992/9992 [3:14:06&lt;00:00,  1.67s/it]"
      }
     },
     "6b667fe6a6a64ea6889e466f59e3991c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6cee254665f24a5aaa014f995211a51f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6dc25d6d544b4847abaadcfde7b3db1f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e03db8ae20434051a5cf397bcaa514dd",
       "max": 157.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_e127994a1955403381663e85d186b2ec",
       "tabbable": null,
       "tooltip": null,
       "value": 157.0
      }
     },
     "70f63eb0c1f848069e48ec99005aeb24": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7219c9a2245643a7a3a17ec77ded795b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "8469d298194846f1880aacc5fcb653fd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "864fc286c483497293d19062b486bcef": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b1b4e28ca54c4edf9cc8611b672381df",
        "IPY_MODEL_b3250f9c5a8b45a088071646b91620ac",
        "IPY_MODEL_3d3af4c20b254a26ba77d176395ec8e0"
       ],
       "layout": "IPY_MODEL_37663d02376b4183877ccedfc25ba641",
       "tabbable": null,
       "tooltip": null
      }
     },
     "8e3d4044721e44fc8cef7af48f98d0b1": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "928b77f391024c98a34cd081c8cf3478": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "95f2ab15e9f6436fa1cd0ea56255895a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "98001065d9914f2abec3ffaf08ab2287": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "993c00cffe0e4f27b4160c9e87ee3290": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9a81e7aaa81541f782c60ae02c06b9e9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9bed71ab24c44071b0fe728fad32b632": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "9e4d0a9ef70448a4a23c259b8360a6ea": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_9a81e7aaa81541f782c60ae02c06b9e9",
       "placeholder": "​",
       "style": "IPY_MODEL_a0adaab481124777a6ae33098632bd37",
       "tabbable": null,
       "tooltip": null,
       "value": "train 3/3: 100%"
      }
     },
     "9ff4b07aaddd4799a6152e5168baf39d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e24c6b27d850492e84765c8f93ad6c8b",
       "placeholder": "​",
       "style": "IPY_MODEL_9bed71ab24c44071b0fe728fad32b632",
       "tabbable": null,
       "tooltip": null,
       "value": "train 1/3: 100%"
      }
     },
     "a0031601cce5470ea9a1f6c10bfa20b1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9e4d0a9ef70448a4a23c259b8360a6ea",
        "IPY_MODEL_1705bc7886d04fca9696d7df8238ad75",
        "IPY_MODEL_463831d2c7ad4258bb561e1350bf346d"
       ],
       "layout": "IPY_MODEL_365547c9fed74f5bb38a7dd4f91734bd",
       "tabbable": null,
       "tooltip": null
      }
     },
     "a022855fca26499c87137a3a2e8852d6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "a0adaab481124777a6ae33098632bd37": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "a329319add804a1c92c5f26139042473": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_8469d298194846f1880aacc5fcb653fd",
       "max": 9992.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_993c00cffe0e4f27b4160c9e87ee3290",
       "tabbable": null,
       "tooltip": null,
       "value": 9992.0
      }
     },
     "a4f928b5f35449cfb9aaea60573438de": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a59cdc1a03b043da9772748910c640d7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ad9ffcd209a346c292e7fd28a66a9f9e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e5912898b10546bfacf17009d388555b",
       "placeholder": "​",
       "style": "IPY_MODEL_b59d65d5a425425cb019ab965df59f4a",
       "tabbable": null,
       "tooltip": null,
       "value": " 155/157 [00:07&lt;00:00, 21.40it/s, step=-, loss=1.9676, avg=1.4769, ppl=4.38, lr=3.00e-04, eta=0.0m]"
      }
     },
     "b0c7ac7c416e46de9b2e803c6e7198fb": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_090d3df75c7e4cc0941c9c46eea7dbd6",
       "placeholder": "​",
       "style": "IPY_MODEL_f5eb2753db55492da544dff454a40700",
       "tabbable": null,
       "tooltip": null,
       "value": "dev 3/3:  99%"
      }
     },
     "b1b4e28ca54c4edf9cc8611b672381df": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_248f4906a1d44cdaa2ee0505120e3eda",
       "placeholder": "​",
       "style": "IPY_MODEL_060f53626c0e460281a11f3c75d5c1a7",
       "tabbable": null,
       "tooltip": null,
       "value": "dev 1/3:  99%"
      }
     },
     "b3250f9c5a8b45a088071646b91620ac": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_1795121211a74bfbb65a7431360a26fa",
       "max": 157.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_128a239a0bf14d05b0f7f2fb0d986217",
       "tabbable": null,
       "tooltip": null,
       "value": 157.0
      }
     },
     "b4379ad5443545cf888163643c47abd6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "b59d65d5a425425cb019ab965df59f4a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "b9e00ed8b6ae41f0bbca2fed6ec5e97a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ba3fdc5d5d9444cdb184112327c1dd89": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_35f7927d1f434f3c87b47a4fe73b059a",
        "IPY_MODEL_5920a87eaafd4bf7a84cd1d08e3b7a97",
        "IPY_MODEL_ad9ffcd209a346c292e7fd28a66a9f9e"
       ],
       "layout": "IPY_MODEL_b4379ad5443545cf888163643c47abd6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "bb3469dee6444acfa7fae640d274acee": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "c2c355b2d54d4f1bb4f940ab37f12fd4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "d1a4ae1fb8614eb19a8d35b172cb8594": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_e20ccab2f94546f5b36af0d468b711a4",
       "placeholder": "​",
       "style": "IPY_MODEL_70f63eb0c1f848069e48ec99005aeb24",
       "tabbable": null,
       "tooltip": null,
       "value": "train 2/3: 100%"
      }
     },
     "d48f8dae1d8743e1b2a2d6303ca86be1": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "db97841691384f5fabf16f0d576f0c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e03db8ae20434051a5cf397bcaa514dd": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e0b9c6617c6a408d9c52e5203caf90bb": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e127994a1955403381663e85d186b2ec": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "e1dd3bca30504c62aaf4132eade59531": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": "hidden",
       "width": null
      }
     },
     "e20ccab2f94546f5b36af0d468b711a4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e24c6b27d850492e84765c8f93ad6c8b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e5912898b10546bfacf17009d388555b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea6947e96bb54636b68c0a509977bfc3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b0c7ac7c416e46de9b2e803c6e7198fb",
        "IPY_MODEL_6dc25d6d544b4847abaadcfde7b3db1f",
        "IPY_MODEL_60efd15052a94ad39b62cc59219c7cd8"
       ],
       "layout": "IPY_MODEL_e1dd3bca30504c62aaf4132eade59531",
       "tabbable": null,
       "tooltip": null
      }
     },
     "eeca8dda6e234a09a3d93f5e3e56ff17": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f491890304e94799b21e03e36aa52f26": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_9ff4b07aaddd4799a6152e5168baf39d",
        "IPY_MODEL_2e086c619b784635943f30649d9ddb08",
        "IPY_MODEL_5086eab6421c4d088eb99fdffe112add"
       ],
       "layout": "IPY_MODEL_20e432b3ed9249f5bf8265eb273c7039",
       "tabbable": null,
       "tooltip": null
      }
     },
     "f5eb2753db55492da544dff454a40700": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
