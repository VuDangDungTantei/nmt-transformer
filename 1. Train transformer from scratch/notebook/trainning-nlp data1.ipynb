{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c202ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T07:05:18.808933Z",
     "iopub.status.busy": "2025-12-13T07:05:18.808641Z",
     "iopub.status.idle": "2025-12-13T07:05:22.313102Z",
     "shell.execute_reply": "2025-12-13T07:05:22.312232Z"
    },
    "papermill": {
     "duration": 3.510965,
     "end_time": "2025-12-13T07:05:22.314441",
     "exception": false,
     "start_time": "2025-12-13T07:05:18.803476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CODE_DIR: /kaggle/input/envi-nmt-code/src\n",
      "PROCESSED_DIR: /kaggle/input/envi-nmt-data/data/processed\n",
      "SPM_MODEL: /kaggle/input/envi-nmt-data/data/spm/spm_unigram.model\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import sys, math, time\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1) ƒê∆∞·ªùng d·∫´n code & d·ªØ li·ªáu tr√™n Kaggle\n",
    "CODE_DIR = Path(\"/kaggle/input/envi-nmt-code/src\")\n",
    "DATA_ROOT = Path(\"/kaggle/input/envi-nmt-data/data\")\n",
    "PROCESSED_DIR = DATA_ROOT / \"processed\"\n",
    "SPM_MODEL = DATA_ROOT / \"spm\" / \"spm_unigram.model\"\n",
    "\n",
    "print(\"CODE_DIR:\", CODE_DIR)\n",
    "print(\"PROCESSED_DIR:\", PROCESSED_DIR)\n",
    "print(\"SPM_MODEL:\", SPM_MODEL)\n",
    "\n",
    "# 2) Th√™m src v√†o sys.path ƒë·ªÉ import ƒë∆∞·ª£c tokenizer, dataset, model\n",
    "sys.path.append(str(CODE_DIR))\n",
    "\n",
    "# 3) Import c√°c class ƒë√£ c√≥ s·∫µn\n",
    "from tokenizer import SubwordTokenizer\n",
    "from dataset import NMTDataset, collate_fn\n",
    "from model import Transformer\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93bab6d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T07:05:22.321766Z",
     "iopub.status.busy": "2025-12-13T07:05:22.321460Z",
     "iopub.status.idle": "2025-12-13T07:05:23.061543Z",
     "shell.execute_reply": "2025-12-13T07:05:23.060657Z"
    },
    "papermill": {
     "duration": 0.74491,
     "end_time": "2025-12-13T07:05:23.062724",
     "exception": false,
     "start_time": "2025-12-13T07:05:22.317814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 8000\n",
      "pad/bos/eos: 0 2 3\n",
      "Train size: 132406\n",
      "Valid size: 1550\n",
      "src_ids torch.Size([64, 70]) torch.int64\n",
      "tgt_in_ids torch.Size([64, 70]) torch.int64\n",
      "tgt_out_ids torch.Size([64, 70]) torch.int64\n",
      "src_padding_mask torch.Size([64, 70]) torch.bool\n",
      "tgt_padding_mask torch.Size([64, 70]) torch.bool\n"
     ]
    }
   ],
   "source": [
    "# Kh·ªüi t·∫°o tokenizer t·ª´ SentencePiece model\n",
    "tokenizer = SubwordTokenizer(str(SPM_MODEL))\n",
    "\n",
    "# L∆ØU √ù: version sentencepiece tr√™n Kaggle d√πng vocab_size(), kh√¥ng c√≥ get_vocab_size()\n",
    "vocab_size = tokenizer.sp.vocab_size()\n",
    "pad_id = tokenizer.pad_id\n",
    "bos_id = tokenizer.bos_id\n",
    "eos_id = tokenizer.eos_id\n",
    "\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "print(\"pad/bos/eos:\", pad_id, bos_id, eos_id)\n",
    "\n",
    "# C√°c hyperparameter c∆° b·∫£n\n",
    "MAX_SRC_LEN = 70\n",
    "MAX_TGT_LEN = 70\n",
    "BATCH_SIZE = 64  # n·∫øu b·ªã OOM th√¨ gi·∫£m xu·ªëng 32 ho·∫∑c 16\n",
    "\n",
    "# T·∫°o Dataset\n",
    "train_dataset = NMTDataset(\n",
    "    data_dir=str(PROCESSED_DIR),\n",
    "    split=\"train\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_src_len=MAX_SRC_LEN,\n",
    "    max_tgt_len=MAX_TGT_LEN,\n",
    ")\n",
    "\n",
    "valid_dataset = NMTDataset(\n",
    "    data_dir=str(PROCESSED_DIR),\n",
    "    split=\"valid\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_src_len=MAX_SRC_LEN,\n",
    "    max_tgt_len=MAX_TGT_LEN,\n",
    ")\n",
    "\n",
    "print(\"Train size:\", len(train_dataset))\n",
    "print(\"Valid size:\", len(valid_dataset))\n",
    "\n",
    "# T·∫°o DataLoader\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id),\n",
    ")\n",
    "\n",
    "valid_loader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id),\n",
    ")\n",
    "\n",
    "# Test 1 batch cho ch·∫Øc\n",
    "batch = next(iter(train_loader))\n",
    "for k, v in batch.items():\n",
    "    print(k, v.shape, v.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ca64e31",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T07:05:23.070576Z",
     "iopub.status.busy": "2025-12-13T07:05:23.069950Z",
     "iopub.status.idle": "2025-12-13T07:05:28.095858Z",
     "shell.execute_reply": "2025-12-13T07:05:28.095015Z"
    },
    "papermill": {
     "duration": 5.03107,
     "end_time": "2025-12-13T07:05:28.097186",
     "exception": false,
     "start_time": "2025-12-13T07:05:23.066116",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model params: 41.723712 M\n"
     ]
    }
   ],
   "source": [
    "# H√†m t·∫°o mask cho Encoder\n",
    "def make_src_mask(src_ids: torch.Tensor, pad_id: int):\n",
    "    \"\"\"\n",
    "    src_ids: (B, S)\n",
    "    Tr·∫£ v·ªÅ mask shape (B, 1, 1, S), 1 = kh√¥ng b·ªã mask, 0 = b·ªã che\n",
    "    \"\"\"\n",
    "    # True ·ªü v·ªã tr√≠ NOT PAD\n",
    "    mask = (src_ids != pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,S), bool\n",
    "    return mask  # ƒë·ªÉ bool c≈©ng ƒë∆∞·ª£c, v√¨ attention d√πng mask == 0\n",
    "\n",
    "\n",
    "# H√†m t·∫°o mask cho Decoder (pad + look-ahead)\n",
    "def make_tgt_mask(tgt_ids: torch.Tensor, pad_id: int):\n",
    "    \"\"\"\n",
    "    tgt_ids: (B, T) = input cho decoder (BOS, w1, w2, ...)\n",
    "    Tr·∫£ v·ªÅ mask shape (B, 1, T, T)\n",
    "    \"\"\"\n",
    "    B, T = tgt_ids.shape\n",
    "\n",
    "    # Pad mask: True ·ªü v·ªã tr√≠ NOT PAD\n",
    "    pad_mask = (tgt_ids != pad_id).unsqueeze(1).unsqueeze(2)  # (B,1,1,T)\n",
    "\n",
    "    # Look-ahead mask: tam gi√°c d∆∞·ªõi (ch·ªâ ƒë∆∞·ª£c nh√¨n qu√° kh·ª© & hi·ªán t·∫°i)\n",
    "    nopeak = torch.tril(torch.ones((T, T), device=tgt_ids.device)).bool()  # (T,T)\n",
    "    nopeak = nopeak.unsqueeze(0).unsqueeze(1)  # (1,1,T,T)\n",
    "\n",
    "    # K·∫øt h·ª£p: ch·ªâ cho ph√©p n·∫øu c·∫£ 2 ƒë·ªÅu True\n",
    "    combined = pad_mask & nopeak  # (B,1,T,T) bool\n",
    "    return combined\n",
    "\n",
    "\n",
    "# Kh·ªüi t·∫°o model\n",
    "d_model = 512\n",
    "n_layers = 4        # gi·∫£m 4 layer cho nh·∫π, kh√¥ng nh·∫•t thi·∫øt ph·∫£i 6 nh∆∞ paper\n",
    "n_heads = 8\n",
    "d_ff = 2048\n",
    "dropout = 0.1\n",
    "max_len = max(MAX_SRC_LEN, MAX_TGT_LEN)\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_layers=n_layers,\n",
    "    n_heads=n_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    max_len=max_len,\n",
    ").to(device)\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=pad_id)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=3e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()) / 1e6, \"M\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca43f1c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T07:05:28.104943Z",
     "iopub.status.busy": "2025-12-13T07:05:28.104372Z",
     "iopub.status.idle": "2025-12-13T09:15:34.006595Z",
     "shell.execute_reply": "2025-12-13T09:15:34.005650Z"
    },
    "papermill": {
     "duration": 7805.911605,
     "end_time": "2025-12-13T09:15:34.012060",
     "exception": false,
     "start_time": "2025-12-13T07:05:28.100455",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Device: cuda\n",
      "V3 config: d_model=384, n_layers=4, n_heads=8, d_ff=1536, dropout=0.1\n",
      "\n",
      "üöÄ START TRAINING V3\n",
      "Epoch 01 | Time: 4.35 min | lr: 0.000417354\n",
      "  Train Loss: 5.7000 | PPL: 298.86\n",
      "  Valid Loss: 4.5653 | PPL: 96.09\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 02 | Time: 4.34 min | lr: 0.000793303\n",
      "  Train Loss: 4.1893 | PPL: 65.98\n",
      "  Valid Loss: 3.8176 | PPL: 45.50\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 03 | Time: 4.33 min | lr: 0.000647729\n",
      "  Train Loss: 3.6328 | PPL: 37.82\n",
      "  Valid Loss: 3.4817 | PPL: 32.52\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 04 | Time: 4.34 min | lr: 0.00056095\n",
      "  Train Loss: 3.3496 | PPL: 28.49\n",
      "  Valid Loss: 3.3379 | PPL: 28.16\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 05 | Time: 4.34 min | lr: 0.000501729\n",
      "  Train Loss: 3.1889 | PPL: 24.26\n",
      "  Valid Loss: 3.2584 | PPL: 26.01\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 06 | Time: 4.33 min | lr: 0.000458014\n",
      "  Train Loss: 3.0811 | PPL: 21.78\n",
      "  Valid Loss: 3.2049 | PPL: 24.65\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 07 | Time: 4.33 min | lr: 0.000424038\n",
      "  Train Loss: 2.9996 | PPL: 20.08\n",
      "  Valid Loss: 3.1569 | PPL: 23.50\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 08 | Time: 4.33 min | lr: 0.000396652\n",
      "  Train Loss: 2.9342 | PPL: 18.81\n",
      "  Valid Loss: 3.1392 | PPL: 23.09\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 09 | Time: 4.34 min | lr: 0.000373967\n",
      "  Train Loss: 2.8812 | PPL: 17.84\n",
      "  Valid Loss: 3.1057 | PPL: 22.33\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 10 | Time: 4.33 min | lr: 0.000354776\n",
      "  Train Loss: 2.8360 | PPL: 17.05\n",
      "  Valid Loss: 3.0880 | PPL: 21.93\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 11 | Time: 4.32 min | lr: 0.000338266\n",
      "  Train Loss: 2.7958 | PPL: 16.38\n",
      "  Valid Loss: 3.0874 | PPL: 21.92\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 12 | Time: 4.32 min | lr: 0.000323865\n",
      "  Train Loss: 2.7606 | PPL: 15.81\n",
      "  Valid Loss: 3.0716 | PPL: 21.58\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 13 | Time: 4.33 min | lr: 0.000311159\n",
      "  Train Loss: 2.7302 | PPL: 15.34\n",
      "  Valid Loss: 3.0570 | PPL: 21.26\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 14 | Time: 4.32 min | lr: 0.00029984\n",
      "  Train Loss: 2.7011 | PPL: 14.90\n",
      "  Valid Loss: 3.0534 | PPL: 21.19\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 15 | Time: 4.32 min | lr: 0.000289673\n",
      "  Train Loss: 2.6755 | PPL: 14.52\n",
      "  Valid Loss: 3.0513 | PPL: 21.14\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 16 | Time: 4.32 min | lr: 0.000280475\n",
      "  Train Loss: 2.6514 | PPL: 14.17\n",
      "  Valid Loss: 3.0486 | PPL: 21.09\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 17 | Time: 4.33 min | lr: 0.000272101\n",
      "  Train Loss: 2.6292 | PPL: 13.86\n",
      "  Valid Loss: 3.0506 | PPL: 21.13\n",
      "Epoch 18 | Time: 4.33 min | lr: 0.000264434\n",
      "  Train Loss: 2.6079 | PPL: 13.57\n",
      "  Valid Loss: 3.0468 | PPL: 21.05\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 19 | Time: 4.32 min | lr: 0.000257382\n",
      "  Train Loss: 2.5893 | PPL: 13.32\n",
      "  Valid Loss: 3.0450 | PPL: 21.01\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 20 | Time: 4.33 min | lr: 0.000250864\n",
      "  Train Loss: 2.5709 | PPL: 13.08\n",
      "  Valid Loss: 3.0527 | PPL: 21.17\n",
      "Epoch 21 | Time: 4.33 min | lr: 0.000244819\n",
      "  Train Loss: 2.5530 | PPL: 12.85\n",
      "  Valid Loss: 3.0445 | PPL: 21.00\n",
      "  ‚úÖ Saved new best V3 to /kaggle/working/best_transformer_v3.pt\n",
      "Epoch 22 | Time: 4.34 min | lr: 0.00023919\n",
      "  Train Loss: 2.5369 | PPL: 12.64\n",
      "  Valid Loss: 3.0499 | PPL: 21.11\n",
      "Epoch 23 | Time: 4.33 min | lr: 0.000233932\n",
      "  Train Loss: 2.5212 | PPL: 12.44\n",
      "  Valid Loss: 3.0487 | PPL: 21.09\n",
      "Epoch 24 | Time: 4.33 min | lr: 0.000229007\n",
      "  Train Loss: 2.5061 | PPL: 12.26\n",
      "  Valid Loss: 3.0497 | PPL: 21.11\n",
      "Epoch 25 | Time: 4.34 min | lr: 0.00022438\n",
      "  Train Loss: 2.4921 | PPL: 12.09\n",
      "  Valid Loss: 3.0516 | PPL: 21.15\n",
      "Epoch 26 | Time: 4.35 min | lr: 0.000220023\n",
      "  Train Loss: 2.4785 | PPL: 11.92\n",
      "  Valid Loss: 3.0571 | PPL: 21.27\n",
      "Epoch 27 | Time: 4.35 min | lr: 0.00021591\n",
      "  Train Loss: 2.4667 | PPL: 11.78\n",
      "  Valid Loss: 3.0563 | PPL: 21.25\n",
      "Epoch 28 | Time: 4.36 min | lr: 0.000212019\n",
      "  Train Loss: 2.4540 | PPL: 11.64\n",
      "  Valid Loss: 3.0594 | PPL: 21.31\n",
      "Epoch 29 | Time: 4.35 min | lr: 0.000208332\n",
      "  Train Loss: 2.4424 | PPL: 11.50\n",
      "  Valid Loss: 3.0644 | PPL: 21.42\n",
      "Epoch 30 | Time: 4.35 min | lr: 0.00020483\n",
      "  Train Loss: 2.4301 | PPL: 11.36\n",
      "  Valid Loss: 3.0675 | PPL: 21.49\n"
     ]
    }
   ],
   "source": [
    "import os, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"üöÄ Device:\", DEVICE)\n",
    "\n",
    "# ========= CONFIG V3 =========\n",
    "D_MODEL = 384\n",
    "N_LAYERS = 4\n",
    "N_HEADS  = 8\n",
    "D_FF     = 1536\n",
    "DROPOUT  = 0.1\n",
    "MAX_LEN  = 5000\n",
    "\n",
    "N_EPOCHS = 30\n",
    "WARMUP_STEPS = 4000\n",
    "LABEL_SMOOTHING = 0.1\n",
    "\n",
    "best_v3_path = \"/kaggle/working/best_transformer_v3.pt\"\n",
    "\n",
    "print(\"V3 config:\",\n",
    "      f\"d_model={D_MODEL}, n_layers={N_LAYERS}, n_heads={N_HEADS}, d_ff={D_FF}, dropout={DROPOUT}\")\n",
    "\n",
    "# ========= MASKS =========\n",
    "def make_masks(src, tgt_in, pad_id):\n",
    "    # src_mask: [B, 1, 1, S]\n",
    "    src_mask = (src != pad_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    # tgt_mask: [B, 1, T, T]\n",
    "    tgt_pad_mask = (tgt_in != pad_id).unsqueeze(1).unsqueeze(2)\n",
    "    T = tgt_in.size(1)\n",
    "    nopeak_mask = torch.tril(torch.ones((1, 1, T, T), device=src.device)).bool()\n",
    "\n",
    "    tgt_mask = tgt_pad_mask & nopeak_mask\n",
    "    return src_mask, tgt_mask\n",
    "\n",
    "# ========= NOAM SCHEDULER (simple + robust) =========\n",
    "class NoamLR:\n",
    "    \"\"\"\n",
    "    lr = factor * d_model^{-0.5} * min(step^{-0.5}, step * warmup^{-1.5})\n",
    "    \"\"\"\n",
    "    def __init__(self, optimizer, d_model, warmup_steps=4000, factor=1.0):\n",
    "        self.optimizer = optimizer\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.factor = factor\n",
    "        self.step_num = 0\n",
    "\n",
    "    def rate(self):\n",
    "        step = max(self.step_num, 1)\n",
    "        return self.factor * (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "\n",
    "    def step(self):\n",
    "        self.step_num += 1\n",
    "        lr = self.rate()\n",
    "        for g in self.optimizer.param_groups:\n",
    "            g[\"lr\"] = lr\n",
    "\n",
    "# ========= MODEL =========\n",
    "model_v3 = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(DEVICE)\n",
    "\n",
    "criterion_v3 = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# NOTE: optimizer lr ƒë·ªÉ 0, scheduler s·∫Ω set lr theo Noam\n",
    "optimizer_v3 = optim.Adam(model_v3.parameters(), lr=0.0, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler_v3 = NoamLR(optimizer_v3, d_model=D_MODEL, warmup_steps=WARMUP_STEPS, factor=1.0)\n",
    "\n",
    "def run_epoch(model, dataloader, optimizer, criterion, device, pad_id, is_train=True, scheduler=None):\n",
    "    model.train() if is_train else model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    for batch in dataloader:\n",
    "        src = batch[\"src_ids\"].to(device)\n",
    "        tgt_in = batch[\"tgt_in_ids\"].to(device)\n",
    "        tgt_out = batch[\"tgt_out_ids\"].to(device)\n",
    "\n",
    "        src_mask, tgt_mask = make_masks(src, tgt_in, pad_id)\n",
    "\n",
    "        if is_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        with torch.set_grad_enabled(is_train):\n",
    "            logits = model(src, tgt_in, src_mask, tgt_mask)  # [B, T, V]\n",
    "            V = logits.size(-1)\n",
    "\n",
    "            loss = criterion(logits.view(-1, V), tgt_out.view(-1))\n",
    "\n",
    "            if is_train:\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                if scheduler is not None:\n",
    "                    scheduler.step()\n",
    "\n",
    "        # t√≠nh theo token ƒë·ªÉ loss/ppl ·ªïn ƒë·ªãnh h∆°n\n",
    "        non_pad = (tgt_out != pad_id).sum().item()\n",
    "        total_loss += loss.item() * non_pad\n",
    "        total_tokens += non_pad\n",
    "\n",
    "    avg_loss = total_loss / max(total_tokens, 1)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
    "    return avg_loss, ppl\n",
    "\n",
    "print(\"\\nüöÄ START TRAINING V3\")\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss, train_ppl = run_epoch(\n",
    "        model_v3, train_loader, optimizer_v3, criterion_v3, DEVICE, pad_id,\n",
    "        is_train=True, scheduler=scheduler_v3\n",
    "    )\n",
    "    valid_loss, valid_ppl = run_epoch(\n",
    "        model_v3, valid_loader, optimizer_v3, criterion_v3, DEVICE, pad_id,\n",
    "        is_train=False, scheduler=None   # ‚ùó validation KH√îNG step scheduler\n",
    "    )\n",
    "\n",
    "    mins = (time.time() - start) / 60\n",
    "    lr_now = optimizer_v3.param_groups[0][\"lr\"]\n",
    "    print(f\"Epoch {epoch:02d} | Time: {mins:.2f} min | lr: {lr_now:.6g}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | PPL: {train_ppl:.2f}\")\n",
    "    print(f\"  Valid Loss: {valid_loss:.4f} | PPL: {valid_ppl:.2f}\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_v3.state_dict(), best_v3_path)\n",
    "        print(f\"  ‚úÖ Saved new best V3 to {best_v3_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92753d9e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:15:34.023153Z",
     "iopub.status.busy": "2025-12-13T09:15:34.022869Z",
     "iopub.status.idle": "2025-12-13T09:28:36.677708Z",
     "shell.execute_reply": "2025-12-13T09:28:36.676917Z"
    },
    "papermill": {
     "duration": 782.667016,
     "end_time": "2025-12-13T09:28:36.684147",
     "exception": false,
     "start_time": "2025-12-13T09:15:34.017131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tune from: /kaggle/working/best_transformer_v3.pt\n",
      "\n",
      "üèÅ START FINE-TUNE V3\n",
      "[V3_FT] Epoch 01 | Time: 4.35 min\n",
      "  Train Loss: 2.4535 | PPL: 11.63\n",
      "  Valid Loss: 3.0263 | PPL: 20.62\n",
      "  ‚úÖ Saved new best V3_FT to /kaggle/working/best_transformer_v3_ft.pt\n",
      "[V3_FT] Epoch 02 | Time: 4.34 min\n",
      "  Train Loss: 2.4258 | PPL: 11.31\n",
      "  Valid Loss: 3.0273 | PPL: 20.64\n",
      "[V3_FT] Epoch 03 | Time: 4.34 min\n",
      "  Train Loss: 2.4124 | PPL: 11.16\n",
      "  Valid Loss: 3.0246 | PPL: 20.59\n",
      "  ‚úÖ Saved new best V3_FT to /kaggle/working/best_transformer_v3_ft.pt\n"
     ]
    }
   ],
   "source": [
    "import os, math, time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "ckpt_v3 = \"/kaggle/working/best_transformer_v3.pt\"\n",
    "assert os.path.exists(ckpt_v3), f\"Kh√¥ng th·∫•y checkpoint: {ckpt_v3}\"\n",
    "print(\"Fine-tune from:\", ckpt_v3)\n",
    "\n",
    "# ph·∫£i kh·ªõp config v·ªõi V3 ƒë√£ train\n",
    "D_MODEL = 384\n",
    "N_LAYERS = 4\n",
    "N_HEADS  = 8\n",
    "D_FF     = 1536\n",
    "DROPOUT  = 0.1\n",
    "MAX_LEN  = 5000\n",
    "\n",
    "ft_v3 = Transformer(\n",
    "    src_vocab_size=vocab_size,\n",
    "    tgt_vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    n_layers=N_LAYERS,\n",
    "    n_heads=N_HEADS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT,\n",
    "    max_len=MAX_LEN,\n",
    ").to(DEVICE)\n",
    "\n",
    "ft_v3.load_state_dict(torch.load(ckpt_v3, map_location=DEVICE))\n",
    "\n",
    "# Fine-tune nh·∫π: th∆∞·ªùng LR nh·ªè + c√≥ th·ªÉ gi·∫£m/gi·ªØ label_smoothing tu·ª≥ b·∫°n\n",
    "criterion_ft = nn.CrossEntropyLoss(ignore_index=pad_id, label_smoothing=0.1)\n",
    "optimizer_ft = optim.Adam(ft_v3.parameters(), lr=5e-5, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "N_EPOCHS_FT = 3\n",
    "best_ft_path = \"/kaggle/working/best_transformer_v3_ft.pt\"\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "print(\"\\nüèÅ START FINE-TUNE V3\")\n",
    "for epoch in range(1, N_EPOCHS_FT + 1):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss, train_ppl = run_epoch(\n",
    "        ft_v3, train_loader, optimizer_ft, criterion_ft, DEVICE, pad_id,\n",
    "        is_train=True, scheduler=None\n",
    "    )\n",
    "    valid_loss, valid_ppl = run_epoch(\n",
    "        ft_v3, valid_loader, optimizer_ft, criterion_ft, DEVICE, pad_id,\n",
    "        is_train=False, scheduler=None\n",
    "    )\n",
    "\n",
    "    mins = (time.time() - start) / 60\n",
    "    print(f\"[V3_FT] Epoch {epoch:02d} | Time: {mins:.2f} min\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f} | PPL: {train_ppl:.2f}\")\n",
    "    print(f\"  Valid Loss: {valid_loss:.4f} | PPL: {valid_ppl:.2f}\")\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(ft_v3.state_dict(), best_ft_path)\n",
    "        print(f\"  ‚úÖ Saved new best V3_FT to {best_ft_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b73d0c1d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:36.695494Z",
     "iopub.status.busy": "2025-12-13T09:28:36.695256Z",
     "iopub.status.idle": "2025-12-13T09:28:37.201563Z",
     "shell.execute_reply": "2025-12-13T09:28:37.200834Z"
    },
    "papermill": {
     "duration": 0.513413,
     "end_time": "2025-12-13T09:28:37.202712",
     "exception": false,
     "start_time": "2025-12-13T09:28:36.689299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded v3_ft model.\n"
     ]
    }
   ],
   "source": [
    "def load_version(version=\"v2_ft\"):\n",
    "    global eval_model\n",
    "\n",
    "    if version == \"v1\":\n",
    "        ckpt = torch.load(\"/kaggle/working/best_transformer_v1.pt\", map_location=device)\n",
    "        d_model  = ckpt.get(\"d_model\", 256)\n",
    "        n_layers = ckpt.get(\"n_layers\", 3)\n",
    "        n_heads  = ckpt.get(\"n_heads\", 4)\n",
    "        d_ff     = ckpt.get(\"d_ff\", 1024)\n",
    "        dropout  = ckpt.get(\"dropout\", 0.1)\n",
    "        max_len  = ckpt.get(\"max_len\", 5000)\n",
    "\n",
    "        eval_model = Transformer(\n",
    "            vocab_size, vocab_size,\n",
    "            d_model, n_layers, n_heads, d_ff, dropout, max_len\n",
    "        ).to(device)\n",
    "        eval_model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "\n",
    "    elif version == \"v2\":\n",
    "        path = \"/kaggle/working/best_transformer_v2.pt\"\n",
    "        D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN = 256, 3, 4, 1024, 0.1, 5000\n",
    "        eval_model = Transformer(\n",
    "            vocab_size, vocab_size,\n",
    "            D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN\n",
    "        ).to(device)\n",
    "        eval_model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    elif version == \"v2_ft\":\n",
    "        path = \"/kaggle/working/best_transformer_v2_ft.pt\"\n",
    "        D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN = 256, 3, 4, 1024, 0.1, 5000\n",
    "        eval_model = Transformer(\n",
    "            vocab_size, vocab_size,\n",
    "            D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN\n",
    "        ).to(device)\n",
    "        eval_model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    elif version == \"v3\":\n",
    "        path = \"/kaggle/working/best_transformer_v3.pt\"\n",
    "        D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN = 384, 4, 8, 1536, 0.1, 5000\n",
    "        eval_model = Transformer(\n",
    "            vocab_size, vocab_size,\n",
    "            D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN\n",
    "        ).to(device)\n",
    "        eval_model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    elif version == \"v3_ft\":\n",
    "        path = \"/kaggle/working/best_transformer_v3_ft.pt\"\n",
    "        D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN = 384, 4, 8, 1536, 0.1, 5000\n",
    "        eval_model = Transformer(\n",
    "            vocab_size, vocab_size,\n",
    "            D_MODEL, N_LAYERS, N_HEADS, D_FF, DROPOUT, MAX_LEN\n",
    "        ).to(device)\n",
    "        eval_model.load_state_dict(torch.load(path, map_location=device))\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"version ph·∫£i l√† 'v1', 'v2', 'v2_ft', 'v3' ho·∫∑c 'v3_ft'.\")\n",
    "\n",
    "    eval_model.eval()\n",
    "    print(f\"‚úÖ Loaded {version} model.\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# V√≠ d·ª•:\n",
    "# load_version(\"v2_ft\")   # d√πng V2 fine-tune\n",
    "# load_version(\"v2\")   # d√πng V2\n",
    "# load_version(\"v1\")    # d√πng V1\n",
    "# load_version(\"v3\")    # d√πng V3\n",
    "load_version(\"v3_ft\")   # d√πng V3 ƒë√£ fine-tune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "306e01fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:37.214960Z",
     "iopub.status.busy": "2025-12-13T09:28:37.214349Z",
     "iopub.status.idle": "2025-12-13T09:28:37.221623Z",
     "shell.execute_reply": "2025-12-13T09:28:37.220926Z"
    },
    "papermill": {
     "duration": 0.014564,
     "end_time": "2025-12-13T09:28:37.222808",
     "exception": false,
     "start_time": "2025-12-13T09:28:37.208244",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import re\n",
    "\n",
    "@torch.no_grad()\n",
    "def greedy_translate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    src_sentence: str,\n",
    "    max_len: int = 70,\n",
    "    max_src_len: int = MAX_SRC_LEN,\n",
    ") -> None: \n",
    "    \"\"\"\n",
    "    D·ªãch 1 c√¢u EN -> VI b·∫±ng greedy search.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 0) Chu·∫©n ho√° c√¢u ti·∫øng Anh gi·ªëng l√∫c train (lower-case)\n",
    "    src_clean_str = src_sentence.strip().lower()\n",
    "\n",
    "    # 1) Encode c√¢u ngu·ªìn\n",
    "    src_ids = tokenizer.encode_src(src_clean_str, add_bos=False, add_eos=True)\n",
    "    if len(src_ids) > max_src_len:\n",
    "        src_ids = src_ids[:max_src_len]\n",
    "\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "    src_mask = make_src_mask(src, pad_id)\n",
    "    enc_output = model.encode(src, src_mask)\n",
    "\n",
    "    # 2) Decode greedy\n",
    "    tgt_ids = [bos_id]\n",
    "    for _ in range(max_len):\n",
    "        tgt = torch.tensor(tgt_ids, dtype=torch.long, device=device).unsqueeze(0)\n",
    "        tgt_mask = make_tgt_mask(tgt, pad_id)\n",
    "\n",
    "        dec_output = model.decode(tgt, enc_output, src_mask, tgt_mask)\n",
    "        logits = model.projection(dec_output)\n",
    "        next_token = logits[:, -1, :].argmax(dim=-1).item()\n",
    "        tgt_ids.append(next_token)\n",
    "        if next_token == eos_id:\n",
    "            break\n",
    "\n",
    "    # 3) B·ªè BOS/EOS, decode sang text\n",
    "    out_ids = tgt_ids[1:]\n",
    "    if out_ids and out_ids[-1] == eos_id:\n",
    "        out_ids = out_ids[:-1]\n",
    "\n",
    "    translation = tokenizer.decode(out_ids)\n",
    "    translation = translation.replace(\"ÔøΩ\", \"\").replace(\"‚Åá\", \"\").strip()\n",
    "    translation = \" \".join(translation.split())\n",
    "\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7346a57b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:37.234401Z",
     "iopub.status.busy": "2025-12-13T09:28:37.234187Z",
     "iopub.status.idle": "2025-12-13T09:28:37.243641Z",
     "shell.execute_reply": "2025-12-13T09:28:37.243078Z"
    },
    "papermill": {
     "duration": 0.016392,
     "end_time": "2025-12-13T09:28:37.244612",
     "exception": false,
     "start_time": "2025-12-13T09:28:37.228220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import random\n",
    "\n",
    "@torch.no_grad()\n",
    "def beam_translate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    src_sentence: str,\n",
    "    max_len: int = 70,\n",
    "    max_src_len: int = MAX_SRC_LEN,\n",
    "    beam_size: int = 4,\n",
    "    length_penalty: float = 1.0,  # ƒë·ªÅ xu·∫•t 1.0 cho c√¢u ƒë·ª° b·ªã ng·∫Øn qu√°\n",
    "):\n",
    "    \"\"\"\n",
    "    Beam Search decode cho 1 c√¢u EN -> VI.\n",
    "    \"\"\"\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # 0) Chu·∫©n ho√° c√¢u ti·∫øng Anh gi·ªëng l√∫c train (lower-case)\n",
    "    src_clean_str = src_sentence.strip().lower()\n",
    "\n",
    "    # 1) Encode c√¢u ngu·ªìn\n",
    "    src_ids = tokenizer.encode_src(src_clean_str, add_bos=False, add_eos=True)\n",
    "    if len(src_ids) > max_src_len:\n",
    "        src_ids = src_ids[:max_src_len]\n",
    "\n",
    "    src = torch.tensor(src_ids, dtype=torch.long, device=device).unsqueeze(0)  # (1, S)\n",
    "    src_mask = make_src_mask(src, pad_id)\n",
    "    enc_output = model.encode(src, src_mask)\n",
    "\n",
    "    # 2) Kh·ªüi t·∫°o beam\n",
    "    # M·ªói beam: (tokens, log_prob, finished)\n",
    "    beams = [([bos_id], 0.0, False)]\n",
    "\n",
    "    for step in range(max_len):\n",
    "        new_beams = []\n",
    "\n",
    "        for tokens, log_prob, finished in beams:\n",
    "            # N·∫øu beam ƒë√£ k·∫øt th√∫c (ra EOS), gi·ªØ nguy√™n\n",
    "            if finished:\n",
    "                new_beams.append((tokens, log_prob, True))\n",
    "                continue\n",
    "\n",
    "            # Chu·∫©n b·ªã input cho decoder\n",
    "            tgt = torch.tensor(tokens, dtype=torch.long, device=device).unsqueeze(0)  # (1, T)\n",
    "            tgt_mask = make_tgt_mask(tgt, pad_id)\n",
    "\n",
    "            dec_out = model.decode(tgt, enc_output, src_mask, tgt_mask)  # (1, T, d_model)\n",
    "            logits = model.projection(dec_out)                           # (1, T, V)\n",
    "            next_log_probs = F.log_softmax(logits[:, -1, :], dim=-1)     # (1, V)\n",
    "\n",
    "            # L·∫•y top-k ti·∫øp theo cho beam n√†y\n",
    "            topk_log_probs, topk_ids = torch.topk(next_log_probs, beam_size, dim=-1)\n",
    "\n",
    "            for k in range(beam_size):\n",
    "                token_id = topk_ids[0, k].item()\n",
    "                token_lp = topk_log_probs[0, k].item()\n",
    "                new_tokens = tokens + [token_id]\n",
    "                new_log_prob = log_prob + token_lp\n",
    "                new_finished = (token_id == eos_id)\n",
    "                new_beams.append((new_tokens, new_log_prob, new_finished))\n",
    "\n",
    "        # 3) Ch·ªçn l·∫°i top beam_size theo score ƒë√£ chu·∫©n h√≥a ƒë·ªô d√†i\n",
    "        # score = log_prob / (len(tokens) ** length_penalty)\n",
    "        beams = sorted(\n",
    "            new_beams,\n",
    "            key=lambda x: x[1] / (len(x[0]) ** length_penalty),\n",
    "            reverse=True\n",
    "        )[:beam_size]\n",
    "\n",
    "        # N·∫øu t·∫•t c·∫£ beam ƒë·ªÅu finished th√¨ d·ª´ng s·ªõm\n",
    "        if all(f for _, _, f in beams):\n",
    "            break\n",
    "\n",
    "    # 4) Ch·ªçn beam t·ªët nh·∫•t\n",
    "    best_tokens, best_log_prob, finished = max(\n",
    "        beams,\n",
    "        key=lambda x: x[1] / (len(x[0]) ** length_penalty)\n",
    "    )\n",
    "\n",
    "    # B·ªè BOS v√† EOS\n",
    "    out_ids = best_tokens[1:]\n",
    "    if out_ids and out_ids[-1] == eos_id:\n",
    "        out_ids = out_ids[:-1]\n",
    "\n",
    "    translation = tokenizer.decode(out_ids)\n",
    "\n",
    "    # L√†m s·∫°ch m·ªôt ch√∫t cho d·ªÖ ƒë·ªçc\n",
    "    translation = translation.replace(\"ÔøΩ\", \"\").replace(\"‚Åá\", \"\").strip()\n",
    "    translation = \" \".join(translation.split())  # g·ªôp b·ªõt kho·∫£ng tr·∫Øng th·ª´a\n",
    "\n",
    "    return translation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc9927cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:37.256136Z",
     "iopub.status.busy": "2025-12-13T09:28:37.255631Z",
     "iopub.status.idle": "2025-12-13T09:28:37.259559Z",
     "shell.execute_reply": "2025-12-13T09:28:37.258867Z"
    },
    "papermill": {
     "duration": 0.010955,
     "end_time": "2025-12-13T09:28:37.260718",
     "exception": false,
     "start_time": "2025-12-13T09:28:37.249763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    # 3 c√¢u ban ƒë·∫ßu\n",
    "    \"I really like natural language processing.\",\n",
    "    \"This is a small machine translation model.\",\n",
    "    \"Thank you for your help.\",\n",
    "\n",
    "    # Th√™m nhi·ªÅu c√¢u ch·ªß ng·ªØ \"I\"\n",
    "    \"I am studying machine learning at the university.\",\n",
    "    \"I don't understand this sentence very well.\",\n",
    "    \"I will try to improve the translation quality.\",\n",
    "\n",
    "    # \"We\"\n",
    "    \"We are working on a neural machine translation project.\",\n",
    "    \"We need more training data to get better results.\",\n",
    "    \"We will present our model in the final report.\",\n",
    "\n",
    "    # \"You\"\n",
    "    \"You can run the code on Kaggle with a GPU.\",\n",
    "    \"You should compare the greedy and beam search outputs.\",\n",
    "\n",
    "    # \"He / She / They\"\n",
    "    \"He likes to read research papers about deep learning.\",\n",
    "    \"She is preparing a presentation about transformers.\",\n",
    "    \"They want to build a better translation system.\",\n",
    "\n",
    "    # C√¢u d√†i h∆°n, c√≥ m·ªánh ƒë·ªÅ\n",
    "    \"When I first learned about attention, I was very confused.\",\n",
    "    \"Even if the BLEU score is not very high, the model can still be useful.\",\n",
    "    \"If we have more time, we will try a larger transformer model.\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f64a9d98",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:37.272093Z",
     "iopub.status.busy": "2025-12-13T09:28:37.271573Z",
     "iopub.status.idle": "2025-12-13T09:28:43.167742Z",
     "shell.execute_reply": "2025-12-13T09:28:43.166931Z"
    },
    "papermill": {
     "duration": 5.903002,
     "end_time": "2025-12-13T09:28:43.168989",
     "exception": false,
     "start_time": "2025-12-13T09:28:37.265987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN     : I really like natural language processing.\n",
      "Greedy : t√¥i th·ª±c s·ª± th√≠ch qu√° tr√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n .\n",
      "Beam   : t√¥i r·∫•t th√≠ch qu√° tr√¨nh x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n .\n",
      "------------------------------------------------------------\n",
      "EN     : This is a small machine translation model.\n",
      "Greedy : ƒë√¢y l√† m·ªôt m√¥ h√¨nh d·ªãch thu·∫≠t nh·ªè .\n",
      "Beam   : ƒë√¢y l√† m·ªôt m√¥ h√¨nh d·ªãch thu·∫≠t nh·ªè .\n",
      "------------------------------------------------------------\n",
      "EN     : Thank you for your help.\n",
      "Greedy : c·∫£m ∆°n s·ª± gi√∫p ƒë·ª° c·ªßa c√°c b·∫°n .\n",
      "Beam   : c·∫£m ∆°n s·ª± gi√∫p ƒë·ª° c·ªßa c√°c b·∫°n .\n",
      "------------------------------------------------------------\n",
      "EN     : I am studying machine learning at the university.\n",
      "Greedy : t√¥i ƒëang nghi√™n c·ª©u m√°y m√≥c h·ªçc t·∫°i tr∆∞·ªùng ƒë·∫°i h·ªçc .\n",
      "Beam   : t√¥i ƒëang nghi√™n c·ª©u m√°y m√≥c h·ªçc t·∫°i tr∆∞·ªùng ƒë·∫°i h·ªçc .\n",
      "------------------------------------------------------------\n",
      "EN     : I don't understand this sentence very well.\n",
      "Greedy : t√¥i kh√¥ng hi·ªÉu r c√¢u n√†y .\n",
      "Beam   : t√¥i kh√¥ng hi·ªÉu r c√¢u n√†y .\n",
      "------------------------------------------------------------\n",
      "EN     : I will try to improve the translation quality.\n",
      "Greedy : t√¥i s·∫Ω c·ªë g·∫Øng c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng phi√™n d·ªãch .\n",
      "Beam   : t√¥i s·∫Ω c·ªë g·∫Øng c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng phi√™n d·ªãch .\n",
      "------------------------------------------------------------\n",
      "EN     : We are working on a neural machine translation project.\n",
      "Greedy : ch√∫ng t√¥i ƒëang nghi√™n c·ª©u m·ªôt d·ª± √°n chuy·ªÉn ho√° th·∫ßn kinh .\n",
      "Beam   : ch√∫ng t√¥i ƒëang nghi√™n c·ª©u m·ªôt d·ª± √°n chuy·ªÉn ho√° th·∫ßn kinh .\n",
      "------------------------------------------------------------\n",
      "EN     : We need more training data to get better results.\n",
      "Greedy : ch√∫ng ta c·∫ßn nhi·ªÅu d·ªØ li·ªáu hu·∫•n luy·ªán h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët h∆°n .\n",
      "Beam   : ch√∫ng ta c·∫ßn nhi·ªÅu d·ªØ li·ªáu hu·∫•n luy·ªán h∆°n ƒë·ªÉ ƒë·∫°t ƒë∆∞·ª£c k·∫øt qu·∫£ t·ªët h∆°n .\n",
      "------------------------------------------------------------\n",
      "EN     : We will present our model in the final report.\n",
      "Greedy : ch√∫ng t√¥i s·∫Ω tr√¨nh b√†y m√¥ h√¨nh c·ªßa m√¨nh trong b√°o c√°o cu·ªëi c√πng .\n",
      "Beam   : ch√∫ng t√¥i s·∫Ω tr√¨nh b√†y m√¥ h√¨nh c·ªßa ch√∫ng t√¥i trong b√°o c√°o cu·ªëi c√πng .\n",
      "------------------------------------------------------------\n",
      "EN     : You can run the code on Kaggle with a GPU.\n",
      "Greedy : b·∫°n c√≥ th·ªÉ ch·∫°y m√£ tr√™n kaim√©t v·ªõi m·ªôt c√°i gpus .\n",
      "Beam   : b·∫°n c√≥ th·ªÉ ch·∫°y ƒëo·∫°n m√£ tr√™n katha v·ªõi apus .\n",
      "------------------------------------------------------------\n",
      "EN     : You should compare the greedy and beam search outputs.\n",
      "Greedy : b·∫°n n√™n so s√°nh s·ª± tham lam v√† s·ª± ra ƒë·ªùi c·ªßa h√£ng t√¨m ki·∫øm .\n",
      "Beam   : b·∫°n n√™n so s√°nh s·ª± tham lam v√† th√†nh qu·∫£ t√¨m ki·∫øm .\n",
      "------------------------------------------------------------\n",
      "EN     : He likes to read research papers about deep learning.\n",
      "Greedy : √¥ng th√≠ch ƒë·ªçc b√°o nghi√™n c·ª©u v·ªÅ h·ªçc t·∫≠p s√¢u s·∫Øc .\n",
      "Beam   : anh ·∫•y th√≠ch ƒë·ªçc b√°o nghi√™n c·ª©u s√¢u v·ªÅ h·ªçc t·∫≠p .\n",
      "------------------------------------------------------------\n",
      "EN     : She is preparing a presentation about transformers.\n",
      "Greedy : c√¥ ·∫•y ƒëang chu·∫©n b·ªã m·ªôt b√†i thuy·∫øt tr√¨nh v·ªÅ nh·ªØng ng∆∞·ªùi bi·∫øn ƒë·ªïi .\n",
      "Beam   : c√¥ ·∫•y ƒëang chu·∫©n b·ªã m·ªôt b√†i thuy·∫øt tr√¨nh v·ªÅ nh·ªØng ng∆∞·ªùi bi·∫øn ƒë·ªïi .\n",
      "------------------------------------------------------------\n",
      "EN     : They want to build a better translation system.\n",
      "Greedy : h·ªç mu·ªën x√¢y d·ª±ng h·ªá th·ªëng d·ªãch t·ªët h∆°n .\n",
      "Beam   : h·ªç mu·ªën x√¢y d·ª±ng h·ªá th·ªëng phi√™n d·ªãch t·ªët h∆°n .\n",
      "------------------------------------------------------------\n",
      "EN     : When I first learned about attention, I was very confused.\n",
      "Greedy : khi t√¥i m·ªõi h·ªçc v·ªÅ s·ª± ch√∫ √Ω , t√¥i ƒë√£ r·∫•t b·ªëi r·ªëi .\n",
      "Beam   : l·∫ßn ƒë·∫ßu ti√™n t√¥i bi·∫øt v·ªÅ s·ª± ch√∫ √Ω , t√¥i r·∫•t b·ªëi r·ªëi .\n",
      "------------------------------------------------------------\n",
      "EN     : Even if the BLEU score is not very high, the model can still be useful.\n",
      "Greedy : ngay c·∫£ khi ƒëi·ªÉm bleu kh√¥ng cao l·∫Øm m√¥ h√¨nh v·∫´n c√≥ th·ªÉ v·∫´n c√≥ √≠ch .\n",
      "Beam   : ngay c·∫£ khi ƒëi·ªÉm bleuy kh√¥ng cao l·∫Øm , m√¥ h√¨nh v·∫´n c√≥ th·ªÉ v·∫´n c√≥ √≠ch .\n",
      "------------------------------------------------------------\n",
      "EN     : If we have more time, we will try a larger transformer model.\n",
      "Greedy : n·∫øu ch√∫ng ta c√≥ nhi·ªÅu th·ªùi gian h∆°n , ch√∫ng ta s·∫Ω th·ª≠ m·ªôt m√¥ h√¨nh bi·∫øn ƒë·ªïi l·ªõn h∆°n .\n",
      "Beam   : n·∫øu ch√∫ng ta c√≥ nhi·ªÅu th·ªùi gian h∆°n , ch√∫ng ta s·∫Ω th·ª≠ m·ªôt m√¥ h√¨nh bi·∫øn ƒë·ªïi l·ªõn h∆°n .\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for s in sentences:\n",
    "    vi_greedy = greedy_translate(eval_model, tokenizer, s, max_len=MAX_TGT_LEN)\n",
    "    vi_beam   = beam_translate(eval_model, tokenizer, s, max_len=MAX_TGT_LEN, beam_size=4)\n",
    "\n",
    "    print(\"EN     :\", s)\n",
    "    print(\"Greedy :\", vi_greedy)\n",
    "    print(\"Beam   :\", vi_beam)\n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "df2a02d0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:43.182204Z",
     "iopub.status.busy": "2025-12-13T09:28:43.181718Z",
     "iopub.status.idle": "2025-12-13T09:28:43.223860Z",
     "shell.execute_reply": "2025-12-13T09:28:43.223151Z"
    },
    "papermill": {
     "duration": 0.049797,
     "end_time": "2025-12-13T09:28:43.225046",
     "exception": false,
     "start_time": "2025-12-13T09:28:43.175249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test size: 1262\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_dataset = NMTDataset(\n",
    "    data_dir=str(PROCESSED_DIR),\n",
    "    split=\"test\",\n",
    "    tokenizer=tokenizer,\n",
    "    max_src_len=MAX_SRC_LEN,\n",
    "    max_tgt_len=MAX_TGT_LEN,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    collate_fn=lambda batch: collate_fn(batch, pad_id=pad_id),\n",
    ")\n",
    "\n",
    "print(\"Test size:\", len(test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b63712ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:43.238218Z",
     "iopub.status.busy": "2025-12-13T09:28:43.238023Z",
     "iopub.status.idle": "2025-12-13T09:28:47.683172Z",
     "shell.execute_reply": "2025-12-13T09:28:47.682470Z"
    },
    "papermill": {
     "duration": 4.453052,
     "end_time": "2025-12-13T09:28:47.684547",
     "exception": false,
     "start_time": "2025-12-13T09:28:43.231495",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install -q sacrebleu\n",
    "import sacrebleu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d1f96a11",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:47.698492Z",
     "iopub.status.busy": "2025-12-13T09:28:47.698095Z",
     "iopub.status.idle": "2025-12-13T09:28:47.996433Z",
     "shell.execute_reply": "2025-12-13T09:28:47.995614Z"
    },
    "papermill": {
     "duration": 0.30672,
     "end_time": "2025-12-13T09:28:47.997795",
     "exception": false,
     "start_time": "2025-12-13T09:28:47.691075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_bleu(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    loader,\n",
    "    max_len=70,\n",
    "    max_sentences=None,\n",
    "    use_beam=False,\n",
    "    beam_size=4,\n",
    "    length_penalty=1.0,\n",
    "):\n",
    "    model.eval()\n",
    "    all_refs = []\n",
    "    all_hyps = []\n",
    "    count = 0\n",
    "\n",
    "    for batch in tqdm(loader, desc=\"BLEU decoding\"):\n",
    "        src_ids = batch[\"src_ids\"]\n",
    "        tgt_out_ids = batch[\"tgt_out_ids\"]\n",
    "        B = src_ids.size(0)\n",
    "\n",
    "        for i in range(B):\n",
    "            if (max_sentences is not None) and (count >= max_sentences):\n",
    "                break\n",
    "\n",
    "            # --- decode input EN ---\n",
    "            src_seq = src_ids[i].tolist()\n",
    "            src_clean = [tid for tid in src_seq if tid not in (pad_id, eos_id)]\n",
    "            src_text = tokenizer.decode(src_clean)\n",
    "\n",
    "            # --- ch·ªçn greedy ho·∫∑c beam ---\n",
    "            if use_beam:\n",
    "                hyp = beam_translate(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    src_text,\n",
    "                    max_len=max_len,\n",
    "                    beam_size=beam_size,\n",
    "                    length_penalty=length_penalty,\n",
    "                )\n",
    "            else:\n",
    "                hyp = greedy_translate(\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    src_text,\n",
    "                    max_len=max_len,\n",
    "                )\n",
    "\n",
    "            # --- reference VI ---\n",
    "            tgt_seq = tgt_out_ids[i].tolist()\n",
    "            tgt_clean = [tid for tid in tgt_seq if tid not in (pad_id, eos_id)]\n",
    "            ref_text = tokenizer.decode(tgt_clean)\n",
    "\n",
    "            all_hyps.append(hyp)\n",
    "            all_refs.append(ref_text)\n",
    "            count += 1\n",
    "\n",
    "        if (max_sentences is not None) and (count >= max_sentences):\n",
    "            break\n",
    "\n",
    "    bleu = sacrebleu.corpus_bleu(all_hyps, [all_refs])\n",
    "    return bleu.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9a44e2b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:28:48.011710Z",
     "iopub.status.busy": "2025-12-13T09:28:48.011451Z",
     "iopub.status.idle": "2025-12-13T09:31:35.257480Z",
     "shell.execute_reply": "2025-12-13T09:31:35.256700Z"
    },
    "papermill": {
     "duration": 167.254384,
     "end_time": "2025-12-13T09:31:35.258662",
     "exception": false,
     "start_time": "2025-12-13T09:28:48.004278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe4caead5b8495a95370ed59570bc8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BLEU decoding:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (greedy): 29.78482464599802\n"
     ]
    }
   ],
   "source": [
    "bleu_greedy = compute_bleu(\n",
    "    eval_model,\n",
    "    tokenizer,\n",
    "    test_loader,\n",
    "    max_len=MAX_TGT_LEN,\n",
    "    max_sentences=None,\n",
    "    use_beam=False,       # ho·∫∑c b·ªè v√¨ default = False\n",
    ")\n",
    "print(\"BLEU (greedy):\", bleu_greedy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd32b766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-13T09:31:35.272716Z",
     "iopub.status.busy": "2025-12-13T09:31:35.272503Z",
     "iopub.status.idle": "2025-12-13T09:45:58.232772Z",
     "shell.execute_reply": "2025-12-13T09:45:58.232194Z"
    },
    "papermill": {
     "duration": 862.96868,
     "end_time": "2025-12-13T09:45:58.233976",
     "exception": false,
     "start_time": "2025-12-13T09:31:35.265296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a30b7a12be040858b5be3c3ba38163f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BLEU decoding:   0%|          | 0/40 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "That's 100 lines that end in a tokenized period ('.')\n",
      "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
      "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU (beam=5, lp=1.2): 30.59901819145823\n"
     ]
    }
   ],
   "source": [
    "bleu_beam = compute_bleu(\n",
    "    eval_model,\n",
    "    tokenizer,\n",
    "    test_loader,\n",
    "    max_len=MAX_TGT_LEN,\n",
    "    max_sentences=None,\n",
    "    use_beam=True,\n",
    "    beam_size=5,\n",
    "    length_penalty=1.2,\n",
    ")\n",
    "print(\"BLEU (beam=5, lp=1.2):\", bleu_beam)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8976618,
     "sourceId": 14096163,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8976917,
     "sourceId": 14096556,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 9644.437518,
   "end_time": "2025-12-13T09:45:59.762298",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-13T07:05:15.324780",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "052357ea1dfe4ec3b45c27608f0bc9db": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "12a681cc4c0d45f9b1b800ae55df6aae": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_95b2ea8bb9404352bd875b6e095b2a12",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_e538bd04ed2148629649c43a2e6665b5",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá40/40‚Äá[02:46&lt;00:00,‚Äá‚Äá3.85s/it]"
      }
     },
     "141d738910c24fbbb16dad8dbf67ba11": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "15f5aca4a33f47d29824c4ae003da9f7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "19ec14392ce841d49cdb6b566e22a24b": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1a17ab35d1bd48899efa008c7162b015": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "28f90cc1213849a9977708e7b2d49417": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_052357ea1dfe4ec3b45c27608f0bc9db",
       "max": 40.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_141d738910c24fbbb16dad8dbf67ba11",
       "tabbable": null,
       "tooltip": null,
       "value": 40.0
      }
     },
     "3604d709933946dfacea0383ddd1c8e6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_fa8c4f1479d24c85b8a5877c4a012fb7",
       "max": 40.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_1a17ab35d1bd48899efa008c7162b015",
       "tabbable": null,
       "tooltip": null,
       "value": 40.0
      }
     },
     "454ed2c2676e415c8f14e21a55abfc48": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "5a30b7a12be040858b5be3c3ba38163f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_727226dc757e45ce87d51fed10ede6c6",
        "IPY_MODEL_28f90cc1213849a9977708e7b2d49417",
        "IPY_MODEL_c1d132759a714870abe4369521b02cf3"
       ],
       "layout": "IPY_MODEL_ad313df39c19407d90a68bb075755893",
       "tabbable": null,
       "tooltip": null
      }
     },
     "64dfcdb788ca4a84bd88500535e6c235": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "727226dc757e45ce87d51fed10ede6c6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_edf3bab52b87403e9784b0240f7f3547",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_64dfcdb788ca4a84bd88500535e6c235",
       "tabbable": null,
       "tooltip": null,
       "value": "BLEU‚Äádecoding:‚Äá100%"
      }
     },
     "95b2ea8bb9404352bd875b6e095b2a12": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "9fba56bc4e95425a80b311d30b5337e5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad313df39c19407d90a68bb075755893": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "beb1c766ae8e4288b7e947cc7b618d3c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_15f5aca4a33f47d29824c4ae003da9f7",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_df2457c31604411e8e27e60c137f6ac3",
       "tabbable": null,
       "tooltip": null,
       "value": "BLEU‚Äádecoding:‚Äá100%"
      }
     },
     "c1d132759a714870abe4369521b02cf3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_19ec14392ce841d49cdb6b566e22a24b",
       "placeholder": "‚Äã",
       "style": "IPY_MODEL_9fba56bc4e95425a80b311d30b5337e5",
       "tabbable": null,
       "tooltip": null,
       "value": "‚Äá40/40‚Äá[14:22&lt;00:00,‚Äá20.03s/it]"
      }
     },
     "df2457c31604411e8e27e60c137f6ac3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "e538bd04ed2148629649c43a2e6665b5": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ebe4caead5b8495a95370ed59570bc8f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_beb1c766ae8e4288b7e947cc7b618d3c",
        "IPY_MODEL_3604d709933946dfacea0383ddd1c8e6",
        "IPY_MODEL_12a681cc4c0d45f9b1b800ae55df6aae"
       ],
       "layout": "IPY_MODEL_454ed2c2676e415c8f14e21a55abfc48",
       "tabbable": null,
       "tooltip": null
      }
     },
     "edf3bab52b87403e9784b0240f7f3547": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "fa8c4f1479d24c85b8a5877c4a012fb7": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
