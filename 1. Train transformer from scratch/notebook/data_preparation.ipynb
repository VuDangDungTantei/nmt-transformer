{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "073c8624",
   "metadata": {},
   "source": [
    "Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf33d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import sentencepiece as spm\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Cấu hình đường dẫn (Paths)\n",
    "# Lưu ý: Notebook đang nằm trong thư mục 'notebook/' nên root là '..'\n",
    "ROOT = Path(\"..\").resolve()\n",
    "DATA_RAW = ROOT / \"data/raw\"\n",
    "DATA_PROC = ROOT / \"data/processed\"\n",
    "DATA_SPM = ROOT / \"data/spm\"\n",
    "SRC_DIR = ROOT / \"src\"\n",
    "\n",
    "# 2. Tạo thư mục nếu chưa có\n",
    "DATA_PROC.mkdir(parents=True, exist_ok=True)\n",
    "DATA_SPM.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 3. Thêm folder 'src' vào hệ thống để import module\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "print(f\"Project Root: {ROOT}\")\n",
    "print(f\"Data Raw: {DATA_RAW}\")\n",
    "print(f\"Data Processed: {DATA_PROC}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32e46ab",
   "metadata": {},
   "source": [
    "Check Raw Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfceedb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== 1. CHECK RAW DATA ===\")\n",
    "files = [\"train.en\", \"train.vi\", \"valid.en\", \"valid.vi\", \"test.en\", \"test.vi\"]\n",
    "\n",
    "for fn in files:\n",
    "    path = DATA_RAW / fn\n",
    "    if path.exists():\n",
    "        with open(path, encoding=\"utf-8\") as f:\n",
    "            n_lines = sum(1 for _ in f)\n",
    "        print(f\"{fn:<10}: {n_lines} lines\")\n",
    "    else:\n",
    "        print(f\"⚠️ {fn:<10}: NOT FOUND (Hãy kiểm tra lại thư mục data/raw)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acdb496",
   "metadata": {},
   "source": [
    "Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f78f58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 2. STATISTICS (RAW DATA) ===\")\n",
    "\n",
    "def load_lines(path):\n",
    "    if not path.exists(): return []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        return [line.strip().lower() for line in f if line.strip()]\n",
    "\n",
    "# Load tạm tập train để thống kê\n",
    "train_en = load_lines(DATA_RAW / \"train.en\")\n",
    "train_vi = load_lines(DATA_RAW / \"train.vi\")\n",
    "\n",
    "def summarize_lengths(name, sents):\n",
    "    if not sents: return\n",
    "    lengths = [len(s.split()) for s in sents] # Tách từ theo khoảng trắng\n",
    "    arr = np.array(lengths)\n",
    "    print(f\"\\n--- {name} ---\")\n",
    "    print(f\"Min: {arr.min()}, Max: {arr.max()}\")\n",
    "    print(f\"Mean: {arr.mean():.2f}, Median: {np.median(arr)}\")\n",
    "    print(f\"95th percentile: {np.percentile(arr, 95)}\")\n",
    "    print(f\"99th percentile: {np.percentile(arr, 99)}\")\n",
    "\n",
    "summarize_lengths(\"English\", train_en)\n",
    "summarize_lengths(\"Vietnamese\", train_vi)\n",
    "\n",
    "# Xóa biến để giải phóng RAM\n",
    "del train_en, train_vi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6916c78a",
   "metadata": {},
   "source": [
    "Filter & Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2eadcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 3. PREPROCESSING & FILTERING ===\")\n",
    "\n",
    "MAX_LEN = 100  # Ngưỡng cắt câu (dựa trên thống kê percentile 99%)\n",
    "\n",
    "def clean_line(line: str) -> str:\n",
    "    # Chuyển thường và bỏ khoảng trắng thừa\n",
    "    return line.strip().lower()\n",
    "\n",
    "def process_split(split_name):\n",
    "    src_in = DATA_RAW / f\"{split_name}.en\"\n",
    "    tgt_in = DATA_RAW / f\"{split_name}.vi\"\n",
    "    \n",
    "    src_out = DATA_PROC / f\"{split_name}.en\"\n",
    "    tgt_out = DATA_PROC / f\"{split_name}.vi\"\n",
    "    \n",
    "    if not src_in.exists() or not tgt_in.exists():\n",
    "        print(f\"Skipping {split_name} (files not found)\")\n",
    "        return\n",
    "\n",
    "    kept = 0\n",
    "    dropped = 0\n",
    "\n",
    "    with open(src_in, encoding=\"utf-8\") as f_src, \\\n",
    "         open(tgt_in, encoding=\"utf-8\") as f_tgt, \\\n",
    "         open(src_out, \"w\", encoding=\"utf-8\") as f_src_out, \\\n",
    "         open(tgt_out, \"w\", encoding=\"utf-8\") as f_tgt_out:\n",
    "\n",
    "        for s_en, s_vi in zip(f_src, f_tgt):\n",
    "            s_en = clean_line(s_en)\n",
    "            s_vi = clean_line(s_vi)\n",
    "\n",
    "            # 1. Bỏ câu rỗng\n",
    "            if not s_en or not s_vi:\n",
    "                dropped += 1\n",
    "                continue\n",
    "            \n",
    "            # 2. Bỏ câu quá dài (Outliers)\n",
    "            if len(s_en.split()) > MAX_LEN or len(s_vi.split()) > MAX_LEN:\n",
    "                dropped += 1\n",
    "                continue\n",
    "\n",
    "            f_src_out.write(s_en + \"\\n\")\n",
    "            f_tgt_out.write(s_vi + \"\\n\")\n",
    "            kept += 1\n",
    "\n",
    "    print(f\"Processed {split_name.upper()}: Kept {kept}, Dropped {dropped}\")\n",
    "\n",
    "# Chạy cho cả 3 tập\n",
    "for split in [\"train\", \"valid\", \"test\"]:\n",
    "    process_split(split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3888b9ff",
   "metadata": {},
   "source": [
    "Tokenizer (SentencePiece)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34d30ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 4. TRAIN TOKENIZER (SENTENCEPIECE) ===\")\n",
    "\n",
    "# 4.1. Tạo file gộp (Combined text) từ tập TRAIN đã xử lý\n",
    "combined_path = DATA_SPM / \"train_combined.txt\"\n",
    "path_train_en = DATA_PROC / \"train.en\"\n",
    "path_train_vi = DATA_PROC / \"train.vi\"\n",
    "\n",
    "if path_train_en.exists() and path_train_vi.exists():\n",
    "    with open(path_train_en, encoding=\"utf-8\") as f_en, \\\n",
    "         open(path_train_vi, encoding=\"utf-8\") as f_vi, \\\n",
    "         open(combined_path, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for line in f_en: f_out.write(line)\n",
    "        for line in f_vi: f_out.write(line)\n",
    "    \n",
    "    print(f\"Created combined file: {combined_path}\")\n",
    "\n",
    "    # 4.2. Train SentencePiece Model\n",
    "    model_prefix = str(DATA_SPM / \"spm_unigram\")\n",
    "    \n",
    "    # Cấu hình Tokenizer\n",
    "    spm.SentencePieceTrainer.Train(\n",
    "        input=str(combined_path),\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=8000,           # Kích thước từ điển shared\n",
    "        model_type=\"unigram\",      # Thuật toán Unigram tốt cho dịch máy\n",
    "        character_coverage=0.9995, # Độ phủ ký tự\n",
    "        pad_id=0,                  # ID mặc định\n",
    "        unk_id=1,\n",
    "        bos_id=2,\n",
    "        eos_id=3\n",
    "    )\n",
    "    print(f\"✅ Tokenizer trained successfully! Saved to: {model_prefix}.model\")\n",
    "else:\n",
    "    print(\"❌ Error: Processed training files not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3ce93f",
   "metadata": {},
   "source": [
    "Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15bf1dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== 5. VERIFY DATA PIPELINE ===\")\n",
    "\n",
    "try:\n",
    "    # Import các class tự viết trong src/\n",
    "    from tokenizer import SubwordTokenizer\n",
    "    from dataset import NMTDataset, collate_fn\n",
    "\n",
    "    # 1. Load Tokenizer\n",
    "    model_path = DATA_SPM / \"spm_unigram.model\"\n",
    "    if not model_path.exists():\n",
    "        raise FileNotFoundError(\"Tokenizer model not found! Run Cell 5 first.\")\n",
    "        \n",
    "    tok = SubwordTokenizer(model_path)\n",
    "    \n",
    "    # Test mã hóa thử 1 câu\n",
    "    test_str = \"hello world\"\n",
    "    print(f\"Test Tokenizer ('{test_str}'): {tok.encode_src(test_str)}\")\n",
    "\n",
    "    # 2. Khởi tạo Dataset (Load từ folder processed)\n",
    "    train_dataset = NMTDataset(\n",
    "        data_dir=str(DATA_PROC),\n",
    "        split=\"train\",\n",
    "        tokenizer=tok,\n",
    "        max_src_len=70,\n",
    "        max_tgt_len=70,\n",
    "    )\n",
    "    print(f\"Dataset Size: {len(train_dataset)}\")\n",
    "\n",
    "    # 3. Khởi tạo DataLoader\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=4,        # Test batch nhỏ\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda batch: collate_fn(batch, pad_id=tok.pad_id)\n",
    "    )\n",
    "\n",
    "    # 4. Lấy thử 1 batch\n",
    "    batch = next(iter(train_loader))\n",
    "    print(\"\\n✅ Batch Output Shapes:\")\n",
    "    for k, v in batch.items():\n",
    "        print(f\" - {k}: {v.shape} | Type: {v.dtype}\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"❌ ImportError: {e}\")\n",
    "    print(\"Gợi ý: Kiểm tra lại file src/tokenizer.py và src/dataset.py\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
